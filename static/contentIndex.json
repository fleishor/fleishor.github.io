{"Dotnet/Authentication/Cookie-based-Authentication":{"title":"Cookie based Authentication","links":[],"tags":["Dotnet"],"content":"References\n\nGithub\n\nAdd authentication and authorization to program.cs\nAddAuthentication()\nAdd authentication service to DI container. Here we use cookie based authentication, which means for each http request a cookie is used which contains all authentication information. Because cookie-based authentication also redirects to the login page, we must override OnRedirectToAccessDenied and OnRedirectToLogin.\nbuilder.Services.AddAuthentication(AuthenticationScheme)\n    .AddCookie(AuthenticationScheme, options =&gt;\n    {\n        options.Cookie.Name = &quot;MyAuthCookie&quot;;\n        options.Events.OnRedirectToAccessDenied = f =&gt;\n        {\n            f.Response.StatusCode = StatusCodes.Status403Forbidden;\n            return Task.CompletedTask;\n        };\n        options.Events.OnRedirectToLogin = f =&gt;\n        {\n            f.Response.StatusCode = StatusCodes.Status401Unauthorized;\n            return Task.CompletedTask;\n                    };\nAddAuthorization()\nAdd policy to authorization, see also AuthorizationAttribute\nbuilder.Services.AddAuthorization(policyBuilder =&gt;\n    {\n        policyBuilder.AddPolicy(&quot;HelloWorldPolicy&quot;, policy =&gt;\n        {\n            policy.RequireAuthenticatedUser()\n                .AddAuthenticationSchemes(AuthenticationScheme)\n                .RequireRole(&quot;HelloWorldRole&quot;);\n        });\n    });\nAdd authentication and authorization middleware\nAs soon as AuthorizationAttribute is used, authentication and authorization middleware must be added. In order to add this middleware the services must be added to DI container.\napp.UseAuthentication();\napp.UseAuthorization();\nProgram.cs\n\n \nusing Microsoft.AspNetCore.Authentication;\nusing Microsoft.AspNetCore.Authentication.Cookies;\n \nnamespace CookieBased\n{\n    public class Program\n    {\n        public static string AuthenticationScheme = &quot;cookie&quot;;\n \n        public static void Main(string[] args)\n        {\n            var builder = WebApplication.CreateBuilder(args);\n \n            builder.Services.AddHttpContextAccessor();\n \n            // Add services to the container.\n \n            // Add Cookie Authorization\n            builder.Services.AddAuthentication(AuthenticationScheme)\n                .AddCookie(AuthenticationScheme, options =&gt;\n                {\n                    options.Cookie.Name = &quot;MyAuthCookie&quot;;\n                    options.Events.OnRedirectToAccessDenied = f =&gt;\n                    {\n                        f.Response.StatusCode = StatusCodes.Status403Forbidden;\n                        return Task.CompletedTask;\n                    };\n                    options.Events.OnRedirectToLogin = f =&gt;\n                    {\n                        f.Response.StatusCode = StatusCodes.Status401Unauthorized;\n                        return Task.CompletedTask;\n                    };\n                });\n \n            // Add Authorization Policy which can be used in AuthorizeAttribute\n            builder.Services.AddAuthorization(policyBuilder =&gt;\n            {\n                policyBuilder.AddPolicy(&quot;HelloWorldPolicy&quot;, policy =&gt;\n                {\n                    policy.RequireAuthenticatedUser()\n                        .AddAuthenticationSchemes(AuthenticationScheme)\n                        .RequireRole(&quot;HelloWorldRole&quot;);\n                });\n            });\n \n            builder.Services.AddControllers();\n \n            // Learn more about configuring Swagger/OpenAPI at aka.ms/aspnetcore/swashbuckle\n            builder.Services.AddEndpointsApiExplorer();\n            builder.Services.AddSwaggerGen();\n \n            var app = builder.Build();\n \n            // Configure the HTTP request pipeline.\n            if (app.Environment.IsDevelopment())\n            {\n                app.UseSwagger();\n                app.UseSwaggerUI();\n            }\n \n            app.UseHttpsRedirection();\n \n            app.UseAuthentication();\n            app.UseAuthorization();\n \n            app.UseStaticFiles();\n \n            app.MapControllers();\n \n            app.Run();\n        }\n    }\n}\n \nSetup cookie based authentication\nSetup claims\nClaims is a Key/Value dictionary with properties of an user. ClaimTypes contains a predefined list of Keys, especially ClaimTypes.Role\nvar claims = new List&lt;Claim&gt;()\n{\n    new Claim(&quot;user&quot;, &quot;fleishor&quot;),\n    new Claim(ClaimTypes.Email, &quot;fleishor@fleishor.org&quot;),\n    new Claim(ClaimTypes.Name, &quot;FleisHor&quot;),\n    new Claim(ClaimTypes.Role, &quot;HelloWorldRole&quot;)\n};\nvar identity = new ClaimsIdentity(claims, Program.AuthenticationScheme);\nvar user = new ClaimsPrincipal(identity);\nSignIn\nSignInAsync creates an encrypted cookie with the data from ClaimsPrincipal. The middleware adds and checks this cookie\nawait ctx.HttpContext.SignInAsync(Program.AuthenticationScheme, user);\nAuthenticationController.cs\n\nusing Microsoft.AspNetCore.Authorization;\n \nnamespace CookieBased.Controllers;\n \nusing System.Security.Claims;\nusing System.Text.Json;\nusing Microsoft.AspNetCore.Authentication;\nusing Microsoft.AspNetCore.Mvc;\n \n[ApiController]\n[Route(&quot;[controller]&quot;)]\npublic class AuthenticationController(IHttpContextAccessor ctx) : ControllerBase\n{\n    [AllowAnonymous]\n    [HttpGet(&quot;SignIn&quot;)]\n    public async Task&lt;string&gt; SignIn(CancellationToken cancellationToken)\n    {\n        var claims = new List&lt;Claim&gt;()\n        {\n            new Claim(&quot;user&quot;, &quot;fleishor&quot;),\n            new Claim(ClaimTypes.Email, &quot;fleishor@fleishor.org&quot;),\n            new Claim(ClaimTypes.Name, &quot;FleisHor&quot;),\n            new Claim(ClaimTypes.Role, &quot;HelloWorldRole&quot;)\n        };\n        var identity = new ClaimsIdentity(claims, Program.AuthenticationScheme);\n        var user = new ClaimsPrincipal(identity);\n        if (ctx.HttpContext != null)\n        {\n            await ctx.HttpContext.SignInAsync(Program.AuthenticationScheme, user);\n        }\n \n        return &quot;ok&quot;;\n    }\n \n    [AllowAnonymous]\n    [HttpGet(&quot;GetIdentity&quot;)]\n    public string GetIdentity(CancellationToken cancellationToken)\n    {\n        if (ctx.HttpContext != null)\n        {\n            var result = ctx.HttpContext.User.Identities.Select(identity =&gt; new\n            {\n                identity.IsAuthenticated,\n                identity.AuthenticationType,\n                identity.Name,\n                Claims = identity.Claims.Select(claim =&gt; new\n                {\n                    claim.Type,\n                    claim.Value\n                })\n            });\n \n            return JsonSerializer.Serialize(result, new JsonSerializerOptions { WriteIndented = true });\n        }\n \n        return string.Empty;\n    }\n}\n \nUsage in controller\nAdd AuthorizeAttribute to the endpoints; Policy-based authorization is the modern one.\nHelloWorldController.cs\n\nusing Microsoft.AspNetCore.Authorization;\nusing Microsoft.AspNetCore.Mvc;\n \nnamespace CookieBased.Controllers\n{\n    [ApiController]\n    [Route(&quot;[controller]&quot;)]\n    public class HelloWorldController : ControllerBase\n    {\n        [HttpGet(&quot;HelloWorldInsecure&quot;)]\n        public string HelloWorldInsecure()\n        {\n            return &quot;Hello World (Insecure)&quot;;\n        }\n \n        [Authorize(Roles = &quot;HelloWorldRoleAdmin&quot;)]\n        [HttpGet(&quot;HelloWorldRoleAdmin&quot;)]\n        public string HelloWorldRolesAdmin()\n        {\n            return &quot;Hello World with Role Admin&quot;;\n        }\n \n        [Authorize(Roles = &quot;HelloWorldRole&quot;)]\n        [HttpGet(&quot;HelloWorldRoles&quot;)]\n        public string HelloWorldRoles()\n        {\n            return &quot;Hello World with Roles&quot;;\n        }\n \n        [Authorize(Policy = &quot;HelloWorldPolicy&quot;)]\n        [HttpGet(&quot;HelloWorldPolicy&quot;)]\n        public string HelloWorldPolicy()\n        {\n            return &quot;Hello World with Policy&quot;;\n        }\n \n    }\n}"},"Dotnet/Authentication/JWT-based-Authentication":{"title":"JWT based authentication","links":[],"tags":["Dotnet"],"content":"References\n\nGithub\nAdding Authorization Option in Swagger\nJWT Authentication with Symmetric Encryption in ASP.NET Core\n\nAdd authentication and authorization to program.cs\nAddAuthentication()\nAdd authentication service to DI container. Here we use JWT Bearer based authentication. The JWT Bearer token must be in Authorization HTTP header\n// Load JwtOptions from appsettings.json\nbuilder.Services.ConfigureOptions&lt;JwtConfigureOptions&gt;();\nvar jwtOptions = builder.Configuration.GetSection(&quot;JwtOptions&quot;).Get&lt;JwtOptions&gt;();\n \nbuilder.Services.AddHttpContextAccessor();\n \n// Add JwtBearer Authorization\nbuilder.Services.AddAuthentication(cfg =&gt; {\n    cfg.DefaultAuthenticateScheme = JwtBearerDefaults.AuthenticationScheme;\n    cfg.DefaultChallengeScheme = JwtBearerDefaults.AuthenticationScheme;\n    cfg.DefaultScheme = JwtBearerDefaults.AuthenticationScheme;\n}).AddJwtBearer(options =&gt;\n{\n    options.RequireHttpsMetadata = false;\n    options.SaveToken = true;\n    options.TokenValidationParameters = new()\n    {\n        ValidateIssuer = true,\n        ValidIssuer = jwtOptions!.Issuer,\n        ValidateAudience = true,\n        ValidAudience = jwtOptions.Audience,\n        ValidateLifetime = true,\n        ValidateIssuerSigningKey = true,\n        IssuerSigningKey = new SymmetricSecurityKey(Encoding.UTF8.GetBytes(jwtOptions.SecretKey))\n    };\n});\nAddAuthorization()\nAdd policy to authorization, see also AuthorizationAttribute\n// Add Authorization Policy which can be used in AuthorizeAttribute\nbuilder.Services.AddAuthorization(policyBuilder =&gt;\n{\n    policyBuilder.AddPolicy(&quot;HelloWorldPolicy&quot;, policy =&gt;\n    {\n        policy.RequireAuthenticatedUser()\n            .AddAuthenticationSchemes(JwtBearerDefaults.AuthenticationScheme)\n            .RequireClaim(&quot;Role&quot;, &quot;HelloWorldRole&quot;);\n    });\n});\nAdd authentication and authorization middleware\nAs soon as AuthorizationAttribute is used, authentication and authorization middleware must be added. In order to add this middleware the services must be added to DI container.\napp.UseAuthentication();\napp.UseAuthorization();\nProgram.cs\n\nusing Microsoft.AspNetCore.Authentication.JwtBearer;\nusing Microsoft.IdentityModel.Tokens;\nusing System.Text;\nusing Microsoft.OpenApi.Models;\n \nnamespace JWTBased\n{\n    public class Program\n    {\n        public static void Main(string[] args)\n        {\n            var builder = WebApplication.CreateBuilder(args);\n \n            builder.Services.ConfigureOptions&lt;JwtConfigureOptions&gt;();\n \n            var jwtOptions = builder.Configuration.GetSection(&quot;JwtOptions&quot;).Get&lt;JwtOptions&gt;();\n \n            builder.Services.AddHttpContextAccessor();\n \n            // Add JwtBearer Authorization\n            builder.Services.AddAuthentication(cfg =&gt; {\n                cfg.DefaultAuthenticateScheme = JwtBearerDefaults.AuthenticationScheme;\n                cfg.DefaultChallengeScheme = JwtBearerDefaults.AuthenticationScheme;\n                cfg.DefaultScheme = JwtBearerDefaults.AuthenticationScheme;\n            }).AddJwtBearer(options =&gt;\n            {\n                options.RequireHttpsMetadata = false;\n                options.SaveToken = true;\n                options.TokenValidationParameters = new()\n                {\n                    ValidateIssuer = true,\n                    ValidIssuer = jwtOptions!.Issuer,\n                    ValidateAudience = true,\n                    ValidAudience = jwtOptions.Audience,\n                    ValidateLifetime = true,\n                    ValidateIssuerSigningKey = true,\n                    IssuerSigningKey = new SymmetricSecurityKey(Encoding.UTF8.GetBytes(jwtOptions.SecretKey))\n                };\n            });\n \n            // Add Authorization Policy which can be used in AuthorizeAttribute\n            builder.Services.AddAuthorization(policyBuilder =&gt;\n            {\n                policyBuilder.AddPolicy(&quot;HelloWorldPolicy&quot;, policy =&gt;\n                {\n                    policy.RequireAuthenticatedUser()\n                        .AddAuthenticationSchemes(JwtBearerDefaults.AuthenticationScheme)\n                        .RequireClaim(&quot;Role&quot;, &quot;HelloWorldRole&quot;);\n                });\n            });\n            builder.Services.AddControllers();\n \n            // Learn more about configuring Swagger/OpenAPI at aka.ms/aspnetcore/swashbuckle\n            builder.Services.AddEndpointsApiExplorer();\n            builder.Services.AddSwaggerGen(options =&gt;\n            {\n                options.AddSecurityDefinition(&quot;Bearer&quot;, new OpenApiSecurityScheme()\n                {\n                    Name = &quot;Authorization&quot;,\n                    Description = &quot;Please insert JWT into field&quot;,\n                    In = ParameterLocation.Header,\n                    Type = SecuritySchemeType.Http,\n                    Scheme = &quot;Bearer&quot;\n                });\n \n                options.AddSecurityRequirement(new OpenApiSecurityRequirement\n                {\n                    {\n                        new OpenApiSecurityScheme\n                        {\n                            Reference = new OpenApiReference\n                            {\n                                Type = ReferenceType.SecurityScheme,\n                                Id = &quot;Bearer&quot;\n                            }\n                        },\n                        Array.Empty&lt;string&gt;()\n                    }\n                });\n \n            });\n \n            var app = builder.Build();\n \n            // Configure the HTTP request pipeline.\n            if (app.Environment.IsDevelopment())\n            {\n                app.UseSwagger();\n                app.UseSwaggerUI();\n            }\n \n            app.UseHttpsRedirection();\n \n            app.UseAuthentication();\n            app.UseAuthorization();\n \n            app.UseStaticFiles();\n \n            app.MapControllers();\n \n            app.Run();\n        }\n    }\n}\nSetup JWT based authentication\nSetup claims\nClaims is a Key/Value dictionary with properties of an user. ClaimTypes contains a predefined list of Keys.\nvar claims = new List&lt;Claim&gt;()\n{\n    new Claim(JwtRegisteredClaimNames.Sub, new Guid().ToString()),\n    new Claim(JwtRegisteredClaimNames.Email, &quot;fleishor@fleishor.org&quot;),\n    new Claim(JwtRegisteredClaimNames.Name, &quot;FleisHor&quot;),\n    new Claim(&quot;Role&quot;, &quot;HelloWorldRole&quot;)\n};\nCreate JWT token\n\nCreate the JWT token. It is signed with SecretKey from configuration\nThis token must be added to the Authorization-HTTP-Header at client side, in opposite to cookies they are not added automatically\n\nvar signingCredentials = new SigningCredentials(\n                            new SymmetricSecurityKey(\n                                Encoding.UTF8.GetBytes(this.jwtOptions.SecretKey)\n                                ),\n                            SecurityAlgorithms.HmacSha256Signature);\n \nvar jwtToken = new JwtSecurityToken(\n    this.jwtOptions.Issuer,\n    this.jwtOptions.Audience,\n    claims,\n    DateTime.Now,\n    DateTime.Now.AddSeconds(this.jwtOptions.ExpirationTimeInSeconds),\n    signingCredentials);\n \nvar jwtTokenValue = new JwtSecurityTokenHandler().WriteToken(jwtToken);\nAuthenticationController.cs\n\nusing System.IdentityModel.Tokens.Jwt;\nusing System.Security.Claims;\nusing System.Text;\nusing System.Text.Json;\nusing Microsoft.AspNetCore.Authorization;\nusing Microsoft.AspNetCore.Mvc;\nusing Microsoft.Extensions.Options;\nusing Microsoft.IdentityModel.Tokens;\n \nnamespace JWTBased.Controllers;\n \n[ApiController]\n[Route(&quot;[controller]&quot;)]\npublic class AuthenticationController : ControllerBase\n{\n    private readonly IHttpContextAccessor httpContextAccessor;\n    private readonly JwtOptions jwtOptions;\n \n    public AuthenticationController(IHttpContextAccessor httpContextAccessor,\n        IOptions&lt;JwtOptions&gt; jwtOptions)\n    {\n        this.httpContextAccessor = httpContextAccessor;\n        this.jwtOptions = jwtOptions.Value;\n    }\n \n    [AllowAnonymous]\n    [HttpGet(&quot;SignIn&quot;)]\n    public string SignIn(CancellationToken cancellationToken)\n    {\n        var claims = new List&lt;Claim&gt;()\n        {\n            new Claim(JwtRegisteredClaimNames.Sub, new Guid().ToString()),\n            new Claim(JwtRegisteredClaimNames.Email, &quot;fleishor@fleishor.org&quot;),\n            new Claim(JwtRegisteredClaimNames.Name, &quot;FleisHor&quot;),\n            new Claim(&quot;Role&quot;, &quot;HelloWorldRole&quot;)\n        };\n \n        var signingCredentials = new SigningCredentials(\n                                    new SymmetricSecurityKey(\n                                        Encoding.UTF8.GetBytes(this.jwtOptions.SecretKey)\n                                        ),\n                                    SecurityAlgorithms.HmacSha256Signature);\n \n        var jwtToken = new JwtSecurityToken(\n            this.jwtOptions.Issuer,\n            this.jwtOptions.Audience,\n            claims,\n            DateTime.Now,\n            DateTime.Now.AddSeconds(this.jwtOptions.ExpirationTimeInSeconds),\n            signingCredentials);\n \n        var jwtTokenValue = new JwtSecurityTokenHandler().WriteToken(jwtToken);\n \n        return jwtTokenValue;\n    }\n \n    [AllowAnonymous]\n    [HttpGet(&quot;GetIdentity&quot;)]\n    public string GetIdentity(CancellationToken cancellationToken)\n    {\n        if (httpContextAccessor.HttpContext != null)\n        {\n            var result = httpContextAccessor.HttpContext.User.Identities.Select(identity =&gt; new\n            {\n                identity.IsAuthenticated,\n                identity.AuthenticationType,\n                identity.Name,\n                Claims = identity.Claims.Select(claim =&gt; new\n                {\n                    claim.Type,\n                    claim.Value\n                })\n            });\n \n            return JsonSerializer.Serialize(result, new JsonSerializerOptions { WriteIndented = true });\n        }\n \n        return string.Empty;\n    }\n}\nUsage in controller\nAdd AuthorizeAttribute to the endpoints; Policy-based authorization is the modern one.\nHelloWorldController.cs\n\nusing Microsoft.AspNetCore.Authorization;\nusing Microsoft.AspNetCore.Mvc;\n \nnamespace JWTBased.Controllers\n{\n    [ApiController]\n    [Route(&quot;[controller]&quot;)]\n    public class HelloWorldController : ControllerBase\n    {\n        [HttpGet(&quot;HelloWorldInsecure&quot;)]\n        public string HelloWorldInsecure()\n        {\n            return &quot;Hello World (Insecure)&quot;;\n        }\n \n        [Authorize(Policy = &quot;HelloWorldPolicy&quot;)]\n        [HttpGet(&quot;HelloWorldPolicy&quot;)]\n        public string HelloWorldPolicy()\n        {\n            return &quot;Hello World with Policy&quot;;\n        }\n \n    }\n}\nAdd Swagger UI support\nAdd Security to swagger options\nbuilder.Services.AddSwaggerGen(options =&gt;\n{\n    options.AddSecurityDefinition(&quot;Bearer&quot;, new OpenApiSecurityScheme()\n    {\n        Name = &quot;Authorization&quot;,\n        Description = &quot;Please insert JWT into field&quot;,\n        In = ParameterLocation.Header,\n        Type = SecuritySchemeType.Http,\n        Scheme = &quot;Bearer&quot;\n    });\n \n    options.AddSecurityRequirement(new OpenApiSecurityRequirement\n    {\n        {\n            new OpenApiSecurityScheme\n            {\n                Reference = new OpenApiReference\n                {\n                    Type = ReferenceType.SecurityScheme,\n                    Id = &quot;Bearer&quot;\n                }\n            },\n            Array.Empty&lt;string&gt;()\n        }\n    });\n});\nUI changes in swagger\nIn the SwaggerUI there is now the authorization button.\n\nExecute SignIn endpoint to get JWT\n\nAdd JWT to Swagger\n\n\nDo not add additional “Bearer” in the input field\n\nProtected Endpoint can be called\n\nAuthorization HTTP header is added to all endpoint calls\nlock symbol is also changed to closed\n\n"},"Dotnet/Automapper":{"title":"Automapper and why not to use it","links":[],"tags":["Dotnet"],"content":"References\n\nIs Automapper the most hated library?\nAutoMapper Usage Guidelines\n\nWhy not to use\n“Because it is the perfect example of a solution for a problem that doesn’t exist or is abused for problems it isn’t a good solution for anyway.”"},"Dotnet/Keycloak/Glossar":{"title":"Keycloak - Glossar","links":[],"tags":["Keycloak","OAuth2","OpenIdConnect-(OIDC)"],"content":"Referenzen\n\nAn Introduction to OAuth 2\n\nRealm\nEin Realm in Keycloak ist eine grundlegende Verwaltungseinheit, die eine Gruppe von Benutzern, Anwendungen und Rollen umfasst. Jeder Realm ist eine separate Sicherheitsdomäne mit eigenen Anmelde- und Verwaltungsrichtlinien. Hier sind einige wichtige Punkte:\n\nIsolation: Realms sind voneinander isoliert, sodass Benutzer und Konfigurationen in einem Realm nicht auf andere Realms zugreifen können.\nBenutzerverwaltung: Jeder Realm hat seine eigenen Benutzer, Gruppen und Rollen.\nAnwendungen: Anwendungen und Dienste werden innerhalb eines Realms registriert und verwaltet.\nKonfiguration: Authentifizierungs- und Autorisierungsrichtlinien werden pro Realm konfiguriert.\n\nEin Realm ermöglicht es, verschiedene Sicherheitsdomänen innerhalb einer Keycloak-Instanz zu verwalten, was besonders nützlich ist, wenn du mehrere Projekte oder Mandanten hast, die getrennt voneinander verwaltet werden sollen.\nAudience\nIn Keycloak bezieht sich der Begriff Audience (Zielgruppe) auf die Entität(en), für die ein Access Token bestimmt ist. Die Audience wird im Access Token durch den aud-Claim angegeben. Dieser Claim gibt an, welche Dienste oder APIs das Token akzeptieren und verwenden dürfen.\nDie Audience ist besonders wichtig, um sicherzustellen, dass ein Access Token nur von den vorgesehenen Empfängern verwendet wird. Dies hilft, die Sicherheit zu erhöhen, indem verhindert wird, dass ein Token von nicht autorisierten Diensten missbraucht wird.\nOAuth Rollen\nOAuth definiert vier Hauptrollen:\n\nResource Owner: Die Person oder Entität, die die Daten besitzt und deren Zugriff kontrolliert.\nClient: Die Anwendung, die im Auftrag des Resource Owners auf die Daten zugreifen möchte.\nAuthorization Server: Der Server, der die Authentifizierung des Resource Owners durchführt und Zugriffstoken ausstellt.\nResource Server: Der Server, der die Daten hostet und die Zugriffstoken überprüft, um den Zugriff zu gewähren.\n\nBasis Authentication Ablauf\nsequenceDiagram\n    User -&gt;&gt; Application: 1. Start Application\n    Application -&gt;&gt; User: 2. Authorization Request\n    User -&gt;&gt; Application: 3. Authorization Grant\n    Application -&gt;&gt; Authorization Server: 4. Authorization Grant\n    Authorization Server -&gt;&gt; Application: 5. Access Token\n    Application -&gt;&gt; Resource Server: 6. Access Token\n    Resource Server -&gt;&gt; Application: 7. Daten, ...\n\n\nDer Anwender startet die Anwendung, in dem er z.B. eine Website im Brower öffnet\nDie Webanwendung leitete den Anwender auf eine Login-Seite weiter\nDer Anwender gibt seinen Benutzerkennung und Passwort ein und schickt die Seite ab\nDie Anwendung fordert vom Authorization-Server ein Accesstoken an.\nFalls die Anwendungs-Id (ClientId) und der Authorization-Grant gültig sind, gibt der Authorization-Server eine Accesstoken zurück\nMit dem Accesstoken kann man beim Resource-Server nach den Daten fragen\nFalls der Resource-Server das Accesstoken akzeptiert gibt es die Daten zurück.\n\nDas ist ein grober Ablauf; der aktuelle Ablauf hängt vom verwendeten  “authorization grant type” ab\nAuthorization Grant Types\n\nAuthorization Code Grant: Dies ist der am häufigsten verwendete Grant-Typ, besonders für Webanwendungen. Der Benutzer authentifiziert sich bei einem Autorisierungsserver, der dann einen Autorisierungscode zurückgibt. Dieser Code wird anschließend gegen ein Zugriffstoken eingetauscht.\nImplicit Grant: Dieser Typ wird hauptsächlich für Single-Page-Anwendungen (SPAs) verwendet, bei denen das Zugriffstoken direkt vom Autorisierungsserver an den Client zurückgegeben wird, ohne dass ein Autorisierungscode erforderlich ist.\nResource Owner Password Credentials Grant: Auch bekannt als Direct Grant in Keycloak. Hierbei gibt der Benutzer seine Anmeldedaten direkt an den Client weiter, der diese dann verwendet, um ein Zugriffstoken zu erhalten. Dieser Grant-Typ wird aus Sicherheitsgründen weniger empfohlen.\nClient Credentials Grant: Dieser Grant-Typ wird verwendet, wenn der Client selbst (und nicht ein Benutzer) autorisiert werden muss, um auf Ressourcen zuzugreifen. Dies ist typisch für serverseitige Anwendungen.\nDevice Authorization Grant: Dieser Typ wird für Geräte verwendet, die keine einfache Möglichkeit haben, Benutzereingaben zu akzeptieren, wie z.B. Smart-TVs oder IoT-Geräte. Der Benutzer gibt einen Code auf einem separaten Gerät ein, um den Zugriff zu autorisieren.\n"},"Dotnet/Keycloak/Keycloak-Configuration":{"title":"Keycloak configuration","links":["SmartHome/Docker/Installations/Keycloak-with-Docker"],"tags":["Keycloak"],"content":"Install Keycloak with Docker\nKeycloak-with-Docker\nCreate New Realm fleishor\n\nAdjust Realm Settings\n\nBecause “Unmanaged Attributes” is disabled, all additional user properties must be added in “User profile”, otherwise the property will not be added to Id-Token (even when requested via scope)\n\n\nAdd Additional User Property\n\n\nCreate New Client BaseDataApp\n\n\nVery important is here the “Redirect URI”, it must fit to the redirect_uri sent from the client application\n\n\n\n\n\nCreate New Realm Role\n\nCreate New User fleishor\n\n\n\nAssign Role BaseDataAppManager to User fleishor\n\nExport Keycloak Configuration as JSON\n"},"Dotnet/Keycloak/Node-Express":{"title":"Keycloak Node.js adapter","links":[],"tags":["Keycloak","NodeJS"],"content":"References\nKeycloak Node.js adapter\nKeycloak configuration\nProgram\n// src/index.ts\nimport express from &quot;express&quot;;\nimport { Request, Response } from &quot;express&quot;;\nimport session from &quot;express-session&quot;;\nimport { jwtDecode } from &quot;jwt-decode&quot;;\nimport { MemoryStore } from &quot;express-session&quot;;\nimport KeycloakConnect from &quot;keycloak-connect&quot;;\n \ndeclare module &quot;express-session&quot; {\n   interface SessionData {\n      &quot;keycloak-token&quot;: any;\n   }\n }\n \nKeycloakConnect.prototype.redirectToLogin = function(req: any) {\n   const apiReqMatcher = /\\/api\\//i;\n   return !apiReqMatcher.test(req.originalUrl || req.url);\n   };\n \nconst app = express();\nconst port = 3001;\nconst memoryStore = new MemoryStore();\nconst keycloak = new KeycloakConnect({ \n      store: memoryStore, \n      scope : &quot;&quot;\n   }, &quot;./keycloak.json&quot;);\n \n \n// Session-Management konfigurieren\napp.use(\n   session({\n      secret: &quot;ExpressSessionSecret&quot;,\n      resave: false,\n      saveUninitialized: true,\n      store: memoryStore,\n   })\n);\n \n// Keycloak-Middleware hinzufügen\napp.use(keycloak.middleware());\n \n// Home\napp.get(&quot;/&quot;, (req, res) =&gt; {\n   res.redirect(&quot;/showSessionInfo&quot;);\n});\n \n// Anzeigen von Session-Informationen\napp.get(&quot;/showSessionInfo&quot;,(req, res) =&gt; {\n   var response = generateUrlLinks();\n   response += generateSessionInfoHtml(req);\n   res.send(response);\n});\n \n// Login-Endpunkt\napp.get(&quot;/login&quot;, keycloak.protect(), (req, res) =&gt; {\n   const iss = req.query.iss;\n   if (iss) {\n      var response = generateUrlLinks();\n      response += generateSessionInfoHtml(req);\n      res.send(response);\n   }\n});\n \n// Logout-Endpunkt\napp.post(&quot;/logout&quot;, (req, res) =&gt; {\n  req.session.destroy((err) =&gt; {\n    if (err) {\n      return res.status(500).send(&quot;Failed to logout.&quot;);\n    }\n    var response = generateUrlLinks();\n    response += generateSessionInfoHtml(req);\n    res.send(response);\n });\n});\n \n// Gesicherte Route mit Bearer-Token-Authentifizierung\napp.get(&quot;/api/rolebased/secure&quot;, keycloak.protect(&quot;BaseDataAppManager&quot;), (req, res) =&gt; {\n   res.send(&quot;This is a path is protected by Keycloak.&quot;);\n});\n \napp.listen(port, () =&gt; {\n   console.log(`Server is running at http://localhost:${port}`);\n});\n \nfunction generateSessionInfoHtml(req: Request) {\n   var response = &quot;&quot;;\n   response += &quot;&lt;h1&gt;Session info&lt;/h1&gt;&quot;;\n   response += &quot;&lt;pre&gt;&quot;;\n   response += JSON.stringify(req.session, null, 4);\n   response += &quot;&lt;/pre&gt;&quot;;\n \n   var keycloakTokenStr = req.session[&quot;keycloak-token&quot;];\n   if (keycloakTokenStr) {\n      response += &quot;&lt;h1&gt;Keycloak&lt;/h1&gt;&quot;;\n \n      var keycloakToken = JSON.parse(keycloakTokenStr);\n      response += &quot;&lt;pre&gt;&quot;;\n      response += JSON.stringify(keycloakToken, null, 4);\n      response += &quot;&lt;/pre&gt;&quot;;\n \n      var idToken = keycloakToken[&quot;id_token&quot;];\n      var idTokenDecoded = jwtDecode(idToken);\n \n      var accessToken = keycloakToken[&quot;access_token&quot;];\n      var accessTokenDecoded = jwtDecode(accessToken);\n      \n      response += &quot;&lt;h1&gt;Times&lt;/h1&gt;&quot;;\n      var idTokenExpire = idTokenDecoded.exp \n                              ? new Date(idTokenDecoded.exp * 1000).toISOString() \n                              : &quot;undefined&quot;;\n      var idTokenIssuedAt = idTokenDecoded.iat \n                              ? new Date(idTokenDecoded.iat * 1000).toISOString() \n                              : &quot;undefined&quot;;\n      response += &quot;&lt;pre&gt;&quot;;\n      response += &quot;ID Token issued at: &quot; + idTokenIssuedAt + &quot;\\n&quot;;\n      response += &quot;ID Token expires at: &quot; + idTokenExpire + &quot;\\n&quot;;\n      response += &quot;&lt;/pre&gt;&quot;;\n \n      response += &quot;&lt;h1&gt;Id token&lt;/h1&gt;&quot;;\n      response += &quot;&lt;pre&gt;&quot;;\n      response += JSON.stringify(idTokenDecoded, null, 4);\n      response += &quot;&lt;/pre&gt;&quot;;\n \n      response += &quot;&lt;h1&gt;Access token&lt;/h1&gt;&quot;;\n      response += &quot;&lt;pre&gt;&quot;;\n      response += JSON.stringify(accessTokenDecoded, null, 4);\n      response += &quot;&lt;/pre&gt;&quot;;\n   }\n   return response;\n}\n \nfunction generateUrlLinks() {\n   var response = &quot;&quot;;\n   response += &quot;&lt;h1&gt;URLs&lt;/h1&gt;&quot;;\n   response += &quot;&lt;pre&gt;&quot;;\n   response += &quot;&lt;a href=&#039;/&#039;&gt;Home&lt;/a&gt;\\n&quot;;\n   response += &quot;&lt;a href=&#039;/showSessionInfo&#039;&gt;Show Session Info&lt;/a&gt;\\n&quot;; \n   response += &quot;&lt;a href=&#039;/login&#039;&gt;Login&lt;/a&gt;\\n&quot;;\n   response += &quot;&lt;a href=&#039;/logout&#039;&gt;Logout&lt;/a&gt;\\n&quot;;\n   response += &quot;&lt;a href=&#039;/api/rolebased/secure&#039;&gt;Role-based Secured API&lt;/a&gt;\\n&quot;;\n   response += &quot;&lt;/pre&gt;&quot;;\n \n   return response;\n}\n \nKeycloak configuration\n{\n  &quot;realm&quot;: &quot;fleishor&quot;,\n  &quot;auth-server-url&quot;: &quot;docker.fritz.box:8081/auth/&quot;,\n  &quot;ssl-required&quot;: &quot;none&quot;,\n  &quot;resource&quot;: &quot;BaseDataApp&quot;,\n  &quot;credentials&quot;: {\n    &quot;secret&quot;: &quot;MvYCXMtWwYoM5yxnX4kCmc7UroTCepTe&quot;\n  },\n  &quot;confidential-port&quot;: 0\n}\nAuthentication flow\nsequenceDiagram\n    autonumber\n    Browser(NodeApp) -&gt;&gt; NodeApp(Express): http://localhost:3001/login\n    NodeApp(Middleware) -&gt;&gt; Browser(NodeApp): Redirect to Keycloak login\n    Browser(NodeApp) -&gt;&gt; Keycloak: Show Keycloak login dialog\n    Browser(Keycloak) -&gt;&gt; Keycloak: Authenticate with User and Password\n    Keycloak -&gt;&gt; Browser(NodeApp): Redirect to NodeApp/login?auth_callback=1&amp;code=***\n    Browser(NodeApp) -&gt;&gt; NodeApp(Middleware): Get http://localhost:3001/login?auth_callback=1&amp;code=***\n\n    rect rgb(192, 192, 192) \n        NodeApp(Middleware) -&gt;&gt; KeyCloak: IdToken and AccessToken for code\n        KeyCloak -&gt;&gt; NodeApp(Middleware): Set IdToken and AccessToken\n        NodeApp(Middleware) -&gt;&gt; NodeApp(Express): Save IdToken and AccessToken&lt;br/&gt;in Express session\n    end\n\n    NodeApp(Middleware) -&gt;&gt; Browser(NodeApp): Redirect to http://localhost:3001/login?iss=docker.fritz.box:8081/auth/realms/fleishor\n    Browser(NodeApp) -&gt;&gt; NodeApp(Express): http://localhost:3001/login?iss=docker.fritz.box:8081/auth/realms/fleishor\n\nOpen application login page\nBecause there are no tokens in express session, the Keycloak middleware redirects to Keycloak login page; see http code 302 and location\n\nLocation property\ndocker.fritz.box:8081/auth/realms/fleishor/protocol/openid-connect/auth\n   ?client_id=BaseDataApp\n   &amp;state=dfcbea09-9427-4bdd-8d44-b71154c9536d\n   &amp;redirect_uri=http://localhost:3001/login?auth_callback=1\n   &amp;scope=openid\n   &amp;response_type=code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nurlTaken from Keycloak configuration properties auth-server-url and realmclient_idtaken from Keycloak configuration property resourcestateguidredirect_uriAfter successful authentication to which uri should be redirected, must be also configured in Keycloakscopeopenid is the default scope, additional scopes can be addedresponse_typeCode-based authentication should be used\nThe browser is redirected to login page from Keycloak\n\n\nSign-In in Keycloak\nAfter Sign-In the username and password are sent to Keycloak (post body); In case the authentication was successful the response is a redirect to the application\n\n\nLocation property:\nhttp://localhost:3001/login\n   ?auth_callback=1\n   &amp;state=dfcbea09-9427-4bdd-8d44-b71154c9536d\n   &amp;session_state=b3f08049-35c4-4aab-9bb8-89d48dccad95\n   &amp;iss=docker.fritz.box:8081/auth/realms/fleishor\n   &amp;code=b9a5cb19-b3e0-4319-9574-905f4ebec9a6.b3f08049-35c4-4aab-9bb8-89d48dccad95.f9aaeed0-047b-4e82-9f6f-da374668e089\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nurlThe redirect_uri from Keycloak and from the redirect aboveauth_callbacklogin is called from Keycloak redirect, which causes that this request will be catched by Keycloak middleware in Expressstateguid from the original login requestsession_stateissURI that verified the authenticationcodeThe requested code which will later be replaced by Keycloak middleware with Id-Token/Access-Token, so the Browser only see this code, but not the tokens\nRedirect again to application login page\nThe browser is now redirect to the login page of the application, due to auth_callback=1 the Keycloak intercepts this request and asks internally Keycloak for an Id-Token and Access-Token using the code.\n\nKeycloak middleware requests Id-Token/Access-Token\nAfter Keycloak could “replace” the code with an Id-Token and Access-Token the browser is again redirected to the application login page, but his time with the parameter iss.\n\nLocation property\n/login?iss=docker.fritz.box:8081/auth/realms/fleishor\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nissIssuer, which authenticated the user\nProtect a route with role-based authentication\nWith keycloak.protect() a route can be protected:\n// Gesicherte Route mit Bearer-Token-Authentifizierung\napp.get(&quot;/api/rolebased/secure&quot;, keycloak.protect(&quot;BaseDataAppManager&quot;), (req, res) =&gt; {\n   res.send(&quot;This is a path is protected by Keycloak.&quot;);\n});\nIn the Access-Token we have a list of granted roles for each client:\n&quot;resource_access&quot;: {\n        &quot;BaseDataApp&quot;: {\n            &quot;roles&quot;: [\n                &quot;BaseDataAppManager&quot;\n            ]\n        }\n    }\nBy default the Keycloak middleware redirects every authentication to the Keycloak login page. For WebAPI calls this is bad, there for this behavior can be overwritten:\nKeycloakConnect.prototype.redirectToLogin = function(req: any) {\n   const apiReqMatcher = /\\/api\\//i;\n   return !apiReqMatcher.test(req.originalUrl || req.url);\n   };\nKeycloak data in express session\nSession Info\n{\n    &quot;cookie&quot;: {\n        &quot;originalMaxAge&quot;: null,\n        &quot;expires&quot;: null,\n        &quot;httpOnly&quot;: true,\n        &quot;path&quot;: &quot;/&quot;\n    },\n    &quot;auth_redirect_uri&quot;: &quot;http://localhost:3001/login?auth_callback=1&quot;,\n    &quot;keycloak-token&quot;: &quot;{\\&quot;access_token\\&quot;:\\&quot;eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJMTVE0b2VGbm9OendlaEFqaWxoek9vOTVxcnJzcU5mQ2w3YmtRZzBvbDk4In0.eyJleHAiOjE3NDQyMTYwNjYsImlhdCI6MTc0NDIxNTc2NiwiYXV0aF90aW1lIjoxNzQ0MjE1NzY2LCJqdGkiOiIzOTFmNmI0Ny0xNzE3LTRiYmUtODc1Yi0xNTcwZDRlNjgwOTYiLCJpc3MiOiJodHRwOi8vZG9ja2VyLmZyaXR6LmJveDo4MDgxL2F1dGgvcmVhbG1zL2ZsZWlzaG9yIiwic3ViIjoiYTIwMTUzZWQtYWFmZi00NzE0LWJmNmQtOGRjZTQ5NzIwM2M2IiwidHlwIjoiQmVhcmVyIiwiYXpwIjoiQmFzZURhdGFBcHAiLCJzaWQiOiJlOTEyNWZkNS05ZDJmLTQxZGQtYjc5Zi01Yjk5NzcxMGIwYzUiLCJhY3IiOiIxIiwiYWxsb3dlZC1vcmlnaW5zIjpbImh0dHA6Ly9sb2NhbGhvc3Q6MzAwMSJdLCJyZWFsbV9hY2Nlc3MiOnsicm9sZXMiOlsiUmVhbG1NYW5hZ2VyIl19LCJyZXNvdXJjZV9hY2Nlc3MiOnsiQmFzZURhdGFBcHAiOnsicm9sZXMiOlsiQmFzZURhdGFBcHBNYW5hZ2VyIl19fSwic2NvcGUiOiJvcGVuaWQgcHJvZmlsZSBlbWFpbCByb2xlcyIsIm5hbWUiOiJIb3JzdCAgRmxlaXNjaGVyIiwicHJlZmVycmVkX3VzZXJuYW1lIjoiZmxlaXNob3IiLCJnaXZlbl9uYW1lIjoiSG9yc3QgIiwiZmFtaWx5X25hbWUiOiJGbGVpc2NoZXIifQ.M7IGYTq8yR3_uRecEZ47SypqXI6F9KQjDkXvl8XY7JMIQTJX0rLJVaE-KeHJ-ff7XpdFAIVGu2SncuwC-KnUtxD7hJYo7uae0EmcHzknA57D9uxOCjSwqdMNCaDBiY7GuULXVwy3UW7epETcm6zo0h34vO6cG3zoG7xUYaH51C_xzTW9dcmG_4xBjdfhsdQM15tk29I-mX_4_P8z0XdUkwTJ0Cb94k1tQvnm4BzJB3UCexNfV_EgWSVz5-tlRUi9Rogn16mDJ1lPC1kesaE7BhZRa1sUkYUurDQ4YkQuNa3hARMLkjnT2QfTqklDwSJaRtQF6uLX6tHv4_926lixHg\\&quot;,\\&quot;expires_in\\&quot;:300,\\&quot;refresh_expires_in\\&quot;:1800,\\&quot;refresh_token\\&quot;:\\&quot;eyJhbGciOiJIUzUxMiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICI0NWJhMjFjMS0xZDFiLTQ3OGMtOGQ2Zi1iMzNmMzBkMGY1MzEifQ.eyJleHAiOjE3NDQyMTc1NjYsImlhdCI6MTc0NDIxNTc2NiwianRpIjoiNjhkYjJiNDYtMjM5YS00MWEwLTg5NGItOTJhYjRjYWQ5ZWI0IiwiaXNzIjoiaHR0cDovL2RvY2tlci5mcml0ei5ib3g6ODA4MS9hdXRoL3JlYWxtcy9mbGVpc2hvciIsImF1ZCI6Imh0dHA6Ly9kb2NrZXIuZnJpdHouYm94OjgwODEvYXV0aC9yZWFsbXMvZmxlaXNob3IiLCJzdWIiOiJhMjAxNTNlZC1hYWZmLTQ3MTQtYmY2ZC04ZGNlNDk3MjAzYzYiLCJ0eXAiOiJSZWZyZXNoIiwiYXpwIjoiQmFzZURhdGFBcHAiLCJzaWQiOiJlOTEyNWZkNS05ZDJmLTQxZGQtYjc5Zi01Yjk5NzcxMGIwYzUiLCJzY29wZSI6Im9wZW5pZCBhY3IgcHJvZmlsZSB3ZWItb3JpZ2lucyBlbWFpbCByb2xlcyBiYXNpYyJ9.V7xwM1G7z43Uz7HUyZHwnvtF9TeRegXhJNNxLX3-fhWZM_7j4RfnZcxixKWsMPyya2Bwhas9U3NsELbXsPFMgw\\&quot;,\\&quot;token_type\\&quot;:\\&quot;Bearer\\&quot;,\\&quot;id_token\\&quot;:\\&quot;eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJMTVE0b2VGbm9OendlaEFqaWxoek9vOTVxcnJzcU5mQ2w3YmtRZzBvbDk4In0.eyJleHAiOjE3NDQyMTYwNjYsImlhdCI6MTc0NDIxNTc2NiwiYXV0aF90aW1lIjoxNzQ0MjE1NzY2LCJqdGkiOiI4MzQwYzViNS02ZWMzLTQ2ZTYtODZhOS0yOTYxNTI1MWIzY2YiLCJpc3MiOiJodHRwOi8vZG9ja2VyLmZyaXR6LmJveDo4MDgxL2F1dGgvcmVhbG1zL2ZsZWlzaG9yIiwiYXVkIjoiQmFzZURhdGFBcHAiLCJzdWIiOiJhMjAxNTNlZC1hYWZmLTQ3MTQtYmY2ZC04ZGNlNDk3MjAzYzYiLCJ0eXAiOiJJRCIsImF6cCI6IkJhc2VEYXRhQXBwIiwic2lkIjoiZTkxMjVmZDUtOWQyZi00MWRkLWI3OWYtNWI5OTc3MTBiMGM1IiwiYXRfaGFzaCI6InpqYWhkM1RPOVM5MWdhcUZYVzIxdlEiLCJhY3IiOiIxIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsInJlYWxtX2FjY2VzcyI6eyJyb2xlcyI6WyJSZWFsbU1hbmFnZXIiXX0sIm5hbWUiOiJIb3JzdCAgRmxlaXNjaGVyIiwicHJlZmVycmVkX3VzZXJuYW1lIjoiZmxlaXNob3IiLCJnaXZlbl9uYW1lIjoiSG9yc3QgIiwiZmFtaWx5X25hbWUiOiJGbGVpc2NoZXIiLCJlbWFpbCI6ImhvcnN0LmZsZWlzY2hlckB3ZWIuZGUifQ.rXiGXGi1CIX5izYs8VCPLX5QK01Bh28MGjyhv0_Fnpbms2K-JAIpSfeVGnY7kBOt_N6NPdDwt8mEducCqVZGftdsm5h-9GJAr1KzLYc-_Zw_sYaokmmknBgCkuPJQ_WoLuRTMO7wPlcqJhaX9WhJ7cvU_jklOOSybu2IqbrnWvK4b3PjHE-ATu7ddOOg_5gFkmCNGLMJJiP-tKeFvthW2d0r4oA699gKfsnJ_h6_o6LDyCrIAFwq8BD3PxpwoQ0i4xpCYRBRJJpOEjr8Kuaat7fIMZ_LPMF8w6tERkrQvt0UUSKJlbJFE8X-zkJGIPsI6MyiKCNmEqUziVKUHLPvFw\\&quot;,\\&quot;not-before-policy\\&quot;:1742214942,\\&quot;session_state\\&quot;:\\&quot;e9125fd5-9d2f-41dd-b79f-5b997710b0c5\\&quot;,\\&quot;scope\\&quot;:\\&quot;openid profile email roles\\&quot;}&quot;\n}\nKeycloak\n{\n    &quot;access_token&quot;: &quot;eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJMTVE0b2VGbm9OendlaEFqaWxoek9vOTVxcnJzcU5mQ2w3YmtRZzBvbDk4In0.eyJleHAiOjE3NDQyMTYwNjYsImlhdCI6MTc0NDIxNTc2NiwiYXV0aF90aW1lIjoxNzQ0MjE1NzY2LCJqdGkiOiIzOTFmNmI0Ny0xNzE3LTRiYmUtODc1Yi0xNTcwZDRlNjgwOTYiLCJpc3MiOiJodHRwOi8vZG9ja2VyLmZyaXR6LmJveDo4MDgxL2F1dGgvcmVhbG1zL2ZsZWlzaG9yIiwic3ViIjoiYTIwMTUzZWQtYWFmZi00NzE0LWJmNmQtOGRjZTQ5NzIwM2M2IiwidHlwIjoiQmVhcmVyIiwiYXpwIjoiQmFzZURhdGFBcHAiLCJzaWQiOiJlOTEyNWZkNS05ZDJmLTQxZGQtYjc5Zi01Yjk5NzcxMGIwYzUiLCJhY3IiOiIxIiwiYWxsb3dlZC1vcmlnaW5zIjpbImh0dHA6Ly9sb2NhbGhvc3Q6MzAwMSJdLCJyZWFsbV9hY2Nlc3MiOnsicm9sZXMiOlsiUmVhbG1NYW5hZ2VyIl19LCJyZXNvdXJjZV9hY2Nlc3MiOnsiQmFzZURhdGFBcHAiOnsicm9sZXMiOlsiQmFzZURhdGFBcHBNYW5hZ2VyIl19fSwic2NvcGUiOiJvcGVuaWQgcHJvZmlsZSBlbWFpbCByb2xlcyIsIm5hbWUiOiJIb3JzdCAgRmxlaXNjaGVyIiwicHJlZmVycmVkX3VzZXJuYW1lIjoiZmxlaXNob3IiLCJnaXZlbl9uYW1lIjoiSG9yc3QgIiwiZmFtaWx5X25hbWUiOiJGbGVpc2NoZXIifQ.M7IGYTq8yR3_uRecEZ47SypqXI6F9KQjDkXvl8XY7JMIQTJX0rLJVaE-KeHJ-ff7XpdFAIVGu2SncuwC-KnUtxD7hJYo7uae0EmcHzknA57D9uxOCjSwqdMNCaDBiY7GuULXVwy3UW7epETcm6zo0h34vO6cG3zoG7xUYaH51C_xzTW9dcmG_4xBjdfhsdQM15tk29I-mX_4_P8z0XdUkwTJ0Cb94k1tQvnm4BzJB3UCexNfV_EgWSVz5-tlRUi9Rogn16mDJ1lPC1kesaE7BhZRa1sUkYUurDQ4YkQuNa3hARMLkjnT2QfTqklDwSJaRtQF6uLX6tHv4_926lixHg&quot;,\n    &quot;expires_in&quot;: 300,\n    &quot;refresh_expires_in&quot;: 1800,\n    &quot;refresh_token&quot;: &quot;eyJhbGciOiJIUzUxMiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICI0NWJhMjFjMS0xZDFiLTQ3OGMtOGQ2Zi1iMzNmMzBkMGY1MzEifQ.eyJleHAiOjE3NDQyMTc1NjYsImlhdCI6MTc0NDIxNTc2NiwianRpIjoiNjhkYjJiNDYtMjM5YS00MWEwLTg5NGItOTJhYjRjYWQ5ZWI0IiwiaXNzIjoiaHR0cDovL2RvY2tlci5mcml0ei5ib3g6ODA4MS9hdXRoL3JlYWxtcy9mbGVpc2hvciIsImF1ZCI6Imh0dHA6Ly9kb2NrZXIuZnJpdHouYm94OjgwODEvYXV0aC9yZWFsbXMvZmxlaXNob3IiLCJzdWIiOiJhMjAxNTNlZC1hYWZmLTQ3MTQtYmY2ZC04ZGNlNDk3MjAzYzYiLCJ0eXAiOiJSZWZyZXNoIiwiYXpwIjoiQmFzZURhdGFBcHAiLCJzaWQiOiJlOTEyNWZkNS05ZDJmLTQxZGQtYjc5Zi01Yjk5NzcxMGIwYzUiLCJzY29wZSI6Im9wZW5pZCBhY3IgcHJvZmlsZSB3ZWItb3JpZ2lucyBlbWFpbCByb2xlcyBiYXNpYyJ9.V7xwM1G7z43Uz7HUyZHwnvtF9TeRegXhJNNxLX3-fhWZM_7j4RfnZcxixKWsMPyya2Bwhas9U3NsELbXsPFMgw&quot;,\n    &quot;token_type&quot;: &quot;Bearer&quot;,\n    &quot;id_token&quot;: &quot;eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJMTVE0b2VGbm9OendlaEFqaWxoek9vOTVxcnJzcU5mQ2w3YmtRZzBvbDk4In0.eyJleHAiOjE3NDQyMTYwNjYsImlhdCI6MTc0NDIxNTc2NiwiYXV0aF90aW1lIjoxNzQ0MjE1NzY2LCJqdGkiOiI4MzQwYzViNS02ZWMzLTQ2ZTYtODZhOS0yOTYxNTI1MWIzY2YiLCJpc3MiOiJodHRwOi8vZG9ja2VyLmZyaXR6LmJveDo4MDgxL2F1dGgvcmVhbG1zL2ZsZWlzaG9yIiwiYXVkIjoiQmFzZURhdGFBcHAiLCJzdWIiOiJhMjAxNTNlZC1hYWZmLTQ3MTQtYmY2ZC04ZGNlNDk3MjAzYzYiLCJ0eXAiOiJJRCIsImF6cCI6IkJhc2VEYXRhQXBwIiwic2lkIjoiZTkxMjVmZDUtOWQyZi00MWRkLWI3OWYtNWI5OTc3MTBiMGM1IiwiYXRfaGFzaCI6InpqYWhkM1RPOVM5MWdhcUZYVzIxdlEiLCJhY3IiOiIxIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsInJlYWxtX2FjY2VzcyI6eyJyb2xlcyI6WyJSZWFsbU1hbmFnZXIiXX0sIm5hbWUiOiJIb3JzdCAgRmxlaXNjaGVyIiwicHJlZmVycmVkX3VzZXJuYW1lIjoiZmxlaXNob3IiLCJnaXZlbl9uYW1lIjoiSG9yc3QgIiwiZmFtaWx5X25hbWUiOiJGbGVpc2NoZXIiLCJlbWFpbCI6ImhvcnN0LmZsZWlzY2hlckB3ZWIuZGUifQ.rXiGXGi1CIX5izYs8VCPLX5QK01Bh28MGjyhv0_Fnpbms2K-JAIpSfeVGnY7kBOt_N6NPdDwt8mEducCqVZGftdsm5h-9GJAr1KzLYc-_Zw_sYaokmmknBgCkuPJQ_WoLuRTMO7wPlcqJhaX9WhJ7cvU_jklOOSybu2IqbrnWvK4b3PjHE-ATu7ddOOg_5gFkmCNGLMJJiP-tKeFvthW2d0r4oA699gKfsnJ_h6_o6LDyCrIAFwq8BD3PxpwoQ0i4xpCYRBRJJpOEjr8Kuaat7fIMZ_LPMF8w6tERkrQvt0UUSKJlbJFE8X-zkJGIPsI6MyiKCNmEqUziVKUHLPvFw&quot;,\n    &quot;not-before-policy&quot;: 1742214942,\n    &quot;session_state&quot;: &quot;e9125fd5-9d2f-41dd-b79f-5b997710b0c5&quot;,\n    &quot;scope&quot;: &quot;openid profile email roles&quot;\n}\nId Token\n{\n    &quot;exp&quot;: 1744216066,\n    &quot;iat&quot;: 1744215766,\n    &quot;auth_time&quot;: 1744215766,\n    &quot;jti&quot;: &quot;8340c5b5-6ec3-46e6-86a9-29615251b3cf&quot;,\n    &quot;iss&quot;: &quot;docker.fritz.box:8081/auth/realms/fleishor&quot;,\n    &quot;aud&quot;: &quot;BaseDataApp&quot;,\n    &quot;sub&quot;: &quot;a20153ed-aaff-4714-bf6d-8dce497203c6&quot;,\n    &quot;typ&quot;: &quot;ID&quot;,\n    &quot;azp&quot;: &quot;BaseDataApp&quot;,\n    &quot;sid&quot;: &quot;e9125fd5-9d2f-41dd-b79f-5b997710b0c5&quot;,\n    &quot;at_hash&quot;: &quot;zjahd3TO9S91gaqFXW21vQ&quot;,\n    &quot;acr&quot;: &quot;1&quot;,\n    &quot;email_verified&quot;: true,\n    &quot;realm_access&quot;: {\n        &quot;roles&quot;: [\n            &quot;RealmManager&quot;\n        ]\n    },\n    &quot;name&quot;: &quot;Horst  Fleischer&quot;,\n    &quot;preferred_username&quot;: &quot;fleishor&quot;,\n    &quot;given_name&quot;: &quot;Horst &quot;,\n    &quot;family_name&quot;: &quot;Fleischer&quot;,\n    &quot;email&quot;: &quot;horst.fleischer@web.de&quot;\n}\nAccess Token\n{\n    &quot;exp&quot;: 1744216066,\n    &quot;iat&quot;: 1744215766,\n    &quot;auth_time&quot;: 1744215766,\n    &quot;jti&quot;: &quot;391f6b47-1717-4bbe-875b-1570d4e68096&quot;,\n    &quot;iss&quot;: &quot;docker.fritz.box:8081/auth/realms/fleishor&quot;,\n    &quot;sub&quot;: &quot;a20153ed-aaff-4714-bf6d-8dce497203c6&quot;,\n    &quot;typ&quot;: &quot;Bearer&quot;,\n    &quot;azp&quot;: &quot;BaseDataApp&quot;,\n    &quot;sid&quot;: &quot;e9125fd5-9d2f-41dd-b79f-5b997710b0c5&quot;,\n    &quot;acr&quot;: &quot;1&quot;,\n    &quot;allowed-origins&quot;: [\n        &quot;http://localhost:3001&quot;\n    ],\n    &quot;realm_access&quot;: {\n        &quot;roles&quot;: [\n            &quot;RealmManager&quot;\n        ]\n    },\n    &quot;resource_access&quot;: {\n        &quot;BaseDataApp&quot;: {\n            &quot;roles&quot;: [\n                &quot;BaseDataAppManager&quot;\n            ]\n        }\n    },\n    &quot;scope&quot;: &quot;openid profile email roles&quot;,\n    &quot;name&quot;: &quot;Horst  Fleischer&quot;,\n    &quot;preferred_username&quot;: &quot;fleishor&quot;,\n    &quot;given_name&quot;: &quot;Horst &quot;,\n    &quot;family_name&quot;: &quot;Fleischer&quot;\n}"},"Dotnet/Kiota-OpenAPI-Client-Generator":{"title":"Kiota OpenApi client generator","links":[],"tags":["Dotnet"],"content":"References\n\nKiota Github\nKiota documentation\n\nInstall Kiota as .NET tool\ndotnet tool install --global Microsoft.OpenApi.Kiota\n\nAdd required Nuget packages to the project\ndotnet add package Microsoft.Kiota.Abstractions\ndotnet add package Microsoft.Kiota.Http.HttpClientLibrary\ndotnet add package Microsoft.Kiota.Serialization.Form\ndotnet add package Microsoft.Kiota.Serialization.Json\ndotnet add package Microsoft.Kiota.Serialization.Text\ndotnet add package Microsoft.Kiota.Serialization.Multipart\n\nGenerate the client API for autobahn.api.bund.dev\nkiota generate \\\n   --exclude-backward-compatible \\\n   --additional-data false\n   --language CSharp \\\n   --class-name AutobahnClient \\\n   --namespace-name Bund.API.Autobahn.Client \\\n   --openapi autobahn.api.bund.dev/openapi.yaml \\\n   --output ./AutobahnClient\n"},"Dotnet/Mediatr/CQRS-with-MediatR":{"title":"CQRS with MediatR","links":["Dotnet/Kiota-OpenAPI-Client-Generator"],"tags":["Dotnet"],"content":"References\n\nGitHub\nCQRS Pattern With MediatR\nKiota OpenAPI Client Generator\n\nProject settings\nDirectory.Build.props\nGlobal settings for all projects in subfolders\n\n&lt;Project&gt;\n  &lt;PropertyGroup&gt;\n    &lt;LangVersion&gt;latest&lt;/LangVersion&gt;\n    &lt;Nullable&gt;enable&lt;/Nullable&gt;\n    &lt;ProduceReferenceAssembly&gt;True&lt;/ProduceReferenceAssembly&gt;\n    &lt;EnableNETAnalyzers&gt;True&lt;/EnableNETAnalyzers&gt;\n    &lt;EnforceCodeStyleInBuild&gt;True&lt;/EnforceCodeStyleInBuild&gt;\n  &lt;/PropertyGroup&gt;\n \n  &lt;PropertyGroup Condition=&quot;&#039;$(Configuration)&#039; == &#039;Release&#039;&quot;&gt;\n    &lt;MSBuildTreatWarningsAsErrors&gt;True&lt;/MSBuildTreatWarningsAsErrors&gt;\n    &lt;TreatWarningsAsErrors&gt;True&lt;/TreatWarningsAsErrors&gt;\n  &lt;/PropertyGroup&gt;\n&lt;/Project&gt;\nDirectory.Build.targets\nNuget packages which are referenced in projects in subfolders\n\n&lt;Project&gt;\n  &lt;PropertyGroup&gt;\n    &lt;!-- Common ruleset shared by all projects --&gt;\n    &lt;CodeAnalysisRuleset&gt;$(MSBuildThisFileDirectory)Mediatr.ruleset&lt;/CodeAnalysisRuleset&gt;\n  &lt;/PropertyGroup&gt;\n  &lt;ItemGroup&gt;\n    &lt;!-- Add reference to StyleCop analyzers to all projects  --&gt;\n    &lt;PackageReference Include=&quot;StyleCop.Analyzers&quot; Version=&quot;1.2.0-beta.*&quot; Condition=&quot;&#039;$(DisableAdditionalAnalyzers)&#039; != &#039;True&#039;&quot; /&gt;\n    &lt;AdditionalFiles Include=&quot;$(MSBuildThisFileDirectory)StyleCop.json&quot; /&gt;\n  &lt;/ItemGroup&gt;\n  &lt;ItemGroup&gt;\n    &lt;PackageReference Include=&quot;SonarAnalyzer.CSharp&quot; Version=&quot;9.*&quot;&gt;\n      &lt;PrivateAssets&gt;all&lt;/PrivateAssets&gt;\n      &lt;IncludeAssets&gt;runtime; build; native; contentfiles; analyzers; buildtransitive&lt;/IncludeAssets&gt;\n    &lt;/PackageReference&gt;\n  &lt;/ItemGroup&gt;\n&lt;/Project&gt;\nKiota extension\n\nnamespace Road.API;\n \nusing Microsoft.Kiota.Http.HttpClientLibrary;\n \npublic static class KiotaServiceCollectionExtensions\n{\n    // Adds the Kiota handlers to the service collection.\n    public static IServiceCollection AddKiotaHandlers(this IServiceCollection services)\n    {\n        // Dynamically load the Kiota handlers from the Client Factory\n        var kiotaHandlers = KiotaClientFactory.GetDefaultHandlerTypes();\n \n        // And register them in the DI container\n        foreach (var handler in kiotaHandlers)\n        {\n            services.AddTransient(handler);\n        }\n \n        return services;\n    }\n \n    // Adds the Kiota handlers to the http client builder.\n    public static IHttpClientBuilder AttachKiotaHandlers(this IHttpClientBuilder builder)\n    {\n        // Dynamically load the Kiota handlers from the Client Factory\n        var kiotaHandlers = KiotaClientFactory.GetDefaultHandlerTypes();\n \n        // And attach them to the http client builder\n        foreach (var handler in kiotaHandlers)\n        {\n            builder.AddHttpMessageHandler((sp) =&gt; (DelegatingHandler)sp.GetRequiredService(handler));\n        }\n \n        return builder;\n    }\n}\nIn program.cs we can simply add all handlers to DI:\n...\n// Add Kiota handlers to the dependency injection container\nbuilder.Services.AddKiotaHandlers();\n \n// Register the factory for the Autobahn client\nbuilder.Services\n   .AddHttpClient&lt;AutobahnClientFactory&gt;(\n      (_, client) =&gt;\n      {\n         client.DefaultRequestHeaders.Add(&quot;Accept&quot;, &quot;application/json&quot;);\n      })\n \n   // Attach the Kiota handlers to the http client, this is to enable all the Kiota features.\n   .AttachKiotaHandlers();\n \n// Register the Autobahn client\nbuilder.Services.AddTransient(sp =&gt; sp.GetRequiredService&lt;AutobahnClientFactory&gt;().GetClient());\n...\nRegister MediatR handler and behaviors in DI\nThe registration order of the behaviors is also the execution order, which means:\n\nLoggingBehavior\nValidationBehavior\nQueryCachingBehavior\n\n...\nbuilder.Services.AddMediatR(cfg =&gt;\n{\n   cfg.RegisterServicesFromAssembly(typeof(RoadWarningsQueryHandler).Assembly);\n \n   // Order of AddOpenBehavior() is important\n   cfg.AddOpenBehavior(typeof(QueryLoggingBehavior&lt;,&gt;));\n   cfg.AddOpenBehavior(typeof(QueryValidationBehavior&lt;,&gt;));\n   cfg.AddOpenBehavior(typeof(QueryCachingBehavior&lt;,&gt;));\n});\n...        \nQueryCachingBehavior\nCache the web service call to autobahn.de for x minutes. The Query (MediatR request) must be additionally implement the interface ICacheable.\n\nnamespace Road.BusinessLayer.Behaviors.Query;\n \nusing System.Text.Json;\nusing MediatR;\nusing Microsoft.Extensions.Caching.Memory;\nusing Microsoft.Extensions.Logging;\nusing Road.BusinessLayer.Interfaces.Query;\n \npublic class QueryCachingBehavior&lt;TQuery, TQueryResult&gt;(\n        ILogger&lt;QueryCachingBehavior&lt;TQuery, TQueryResult&gt;&gt; logger,\n        IMemoryCache cache)\n    : IPipelineBehavior&lt;TQuery, TQueryResult&gt;\n        where TQuery : IQuery&lt;IQueryResult&gt;, ICacheable\n{\n    public async Task&lt;TQueryResult&gt; Handle(TQuery request, RequestHandlerDelegate&lt;TQueryResult&gt; next, CancellationToken cancellationToken)\n    {\n        logger.LogInformation(&quot;Checking MemoryCache; CacheKey: \\&quot;{CacheKey}\\&quot;&quot;, request.CacheKey);\n \n        TQueryResult response;\n        if (cache.Get(request.CacheKey) is string cachedResponse)\n        {\n            response = JsonSerializer.Deserialize&lt;TQueryResult&gt;(cachedResponse)!;\n            logger.LogInformation(&quot;Fetched from cache; CacheKey: \\&quot;{CacheKey}\\&quot;&quot;, request.CacheKey);\n        }\n        else\n        {\n            response = await GetResponseAndAddToCache(request, next);\n            logger.LogInformation(&quot;Added to cache; CacheKey: \\&quot;{CacheKey}\\&quot;&quot;, request.CacheKey);\n        }\n \n        return response;\n    }\n \n    private async Task&lt;TQueryResult&gt; GetResponseAndAddToCache(TQuery request, RequestHandlerDelegate&lt;TQueryResult&gt; next)\n    {\n        var response = await next();\n        if (response is not null)\n        {\n            var slidingExpirationInMinutes = request.SlidingExpirationInMinutes == 0 ? 30 : request.SlidingExpirationInMinutes;\n            var options = new MemoryCacheEntryOptions().SetSlidingExpiration(TimeSpan.FromMinutes(slidingExpirationInMinutes));\n \n            var serializedData = JsonSerializer.Serialize(response);\n            cache.Set(request.CacheKey, serializedData, options);\n        }\n \n        return response;\n    }\n}\n\nnamespace Road.BusinessLayer.Queries.RoadWarnings\n{\n    using Road.BusinessLayer.Interfaces.Query;\n \n    public record RoadWarningsQuery(string RoadId, int SlidingExpirationInMinutes = 0) : IQuery&lt;RoadWarningsQueryResult&gt;, ICacheable\n    {\n        public string CacheKey { get; } = RoadId;\n \n        public int SlidingExpirationInMinutes { get; } = SlidingExpirationInMinutes;\n    }\n}\nQueryLoggingBehavior\nWrite a logging message when query handling is started and finished\n\nnamespace Road.BusinessLayer.Behaviors.Query;\n \nusing MediatR;\nusing Microsoft.Extensions.Logging;\nusing Road.BusinessLayer.Interfaces.Query;\n \npublic class QueryLoggingBehavior&lt;TQuery, TQueryResult&gt; : IPipelineBehavior&lt;TQuery, TQueryResult&gt;\n    where TQuery : IQuery&lt;IQueryResult&gt;\n{\n    private readonly ILogger&lt;QueryLoggingBehavior&lt;TQuery, TQueryResult&gt;&gt; logger;\n \n    public QueryLoggingBehavior(ILogger&lt;QueryLoggingBehavior&lt;TQuery, TQueryResult&gt;&gt; logger)\n    {\n        this.logger = logger;\n    }\n \n    public async Task&lt;TQueryResult&gt; Handle(TQuery request, RequestHandlerDelegate&lt;TQueryResult&gt; next, CancellationToken cancellationToken)\n    {\n        logger.LogInformation(&quot;Handling query {QueryName}&quot;, typeof(TQuery).Name);\n        var response = await next();\n        logger.LogInformation(&quot;Handled query {QueryName}&quot;, typeof(TQuery).Name);\n \n        return response;\n    }\n}\nQueryValidationBehavior\n\nThe query parameter are validated if there is a corresponding Validator in DI container.\nThere is also a nuget package: MediatR.Extensions.FluentValidation.AspNetCore\n\n\nnamespace Road.BusinessLayer.Behaviors.Query;\n \nusing FluentValidation;\nusing MediatR;\nusing Road.BusinessLayer.Interfaces.Query;\n \npublic class QueryValidationBehavior&lt;TQuery, TQueryResult&gt;(IEnumerable&lt;IValidator&lt;TQuery&gt;&gt; validators)\n    : IPipelineBehavior&lt;TQuery, TQueryResult&gt;\n    where TQuery : IQuery&lt;IQueryResult&gt;\n{\n    public async Task&lt;TQueryResult&gt; Handle(TQuery request, RequestHandlerDelegate&lt;TQueryResult&gt; next, CancellationToken cancellationToken)\n    {\n        if (validators.Any())\n        {\n            var context = new ValidationContext&lt;TQuery&gt;(request);\n \n            var validationResults = await Task.WhenAll(\n                validators.Select(v =&gt; v.ValidateAsync(context, cancellationToken)));\n \n            var failures = validationResults\n                .Where(r =&gt; r.Errors.Count &gt; 0)\n                .SelectMany(r =&gt; r.Errors)\n                .ToList();\n \n            if (failures.Count &gt; 0)\n            {\n                throw new ValidationException(failures);\n            }\n        }\n \n        return await next();\n    }\n}\n\nnamespace Road.BusinessLayer.Queries.RoadWarnings;\n \nusing FluentValidation;\n \npublic class RoadWarningsQueryValidator : AbstractValidator&lt;RoadWarningsQuery&gt;\n{\n    public RoadWarningsQueryValidator()\n    {\n        RuleFor(x =&gt; x.RoadId).NotEmpty().MaximumLength(4);\n    }\n}\nProgram.cs\n\nnamespace Road.API;\n \nusing FluentValidation;\nusing Road.BusinessLayer;\nusing Road.BusinessLayer.Behaviors.Query;\nusing Road.BusinessLayer.Queries.RoadWarnings;\n \npublic static class Program\n{\n    public static void Main(string[] args)\n    {\n        var builder = WebApplication.CreateBuilder(args);\n \n        // Add services to the container.\n        builder.Services.AddControllers();\n \n        // Learn more about configuring Swagger/OpenAPI at aka.ms/aspnetcore/swashbuckle\n        builder.Services.AddEndpointsApiExplorer();\n        builder.Services.AddSwaggerGen();\n \n        builder.Services.AddValidatorsFromAssembly(typeof(RoadWarningsQueryHandler).Assembly);\n \n        builder.Services.AddMediatR(cfg =&gt;\n        {\n            cfg.RegisterServicesFromAssembly(typeof(RoadWarningsQueryHandler).Assembly);\n \n            // Order of AddOpenBehavior() is important\n            cfg.AddOpenBehavior(typeof(QueryLoggingBehavior&lt;,&gt;));\n            cfg.AddOpenBehavior(typeof(QueryValidationBehavior&lt;,&gt;));\n            cfg.AddOpenBehavior(typeof(QueryCachingBehavior&lt;,&gt;));\n        });\n \n        // Add Kiota handlers to the dependency injection container\n        builder.Services.AddKiotaHandlers();\n \n        // Register the factory for the Autobahn client\n        builder.Services\n            .AddHttpClient&lt;AutobahnClientFactory&gt;(\n                (_, client) =&gt;\n                {\n                    client.DefaultRequestHeaders.Add(&quot;Accept&quot;, &quot;application/json&quot;);\n                })\n \n            // Attach the Kiota handlers to the http client, this is to enable all the Kiota features.\n            .AttachKiotaHandlers();\n \n        // Register the Autobahn client\n        builder.Services.AddTransient(sp =&gt; sp.GetRequiredService&lt;AutobahnClientFactory&gt;().GetClient());\n \n        builder.Services.AddMemoryCache();\n \n        var app = builder.Build();\n \n        // Configure the HTTP request pipeline.\n        if (app.Environment.IsDevelopment())\n        {\n            app.UseSwagger();\n            app.UseSwaggerUI();\n        }\n \n        app.UseAuthorization();\n \n        app.MapControllers();\n \n        app.Run();\n    }\n}"},"Dotnet/Mediatr/Serilog-for-CQRS-with-MediatR":{"title":"Add Serilog to \"CQRS with MediatR\"","links":[],"tags":["Dotnet"],"content":"References\n\nGitHub\nHTTP logging in ASP.NET Core\nCompact Log Event Format (CLEF)\nSerilog\nSerilog Enrichers\nSerilog ClientInfo Enricher\n\nNuget packages\n  &lt;ItemGroup&gt;\n    ...\n    &lt;PackageReference Include=&quot;Serilog.AspNetCore&quot; Version=&quot;8.0.2&quot; /&gt;\n    &lt;PackageReference Include=&quot;Serilog.Enrichers.ClientInfo&quot; Version=&quot;2.1.1&quot; /&gt;\n    &lt;PackageReference Include=&quot;Serilog.HttpClient&quot; Version=&quot;3.0.0&quot; /&gt;\n    ...\n  &lt;/ItemGroup&gt;\nProgram.cs\npublic static void Main(string[] args)\n{\n   var builder = WebApplication.CreateBuilder(args);\n \n   // Add Serilog and use configuration from appsettings.json\n   builder.Host.UseSerilog((context, config) =&gt;\n   {\n      config.ReadFrom.Configuration(context.Configuration);\n   });\n \n   // Required by ClientInfo enricher\n   builder.Services.AddHttpContextAccessor();\n   \n   ...\n   \n   builder.Services.AddSwaggerGen(config =&gt;\n   {\n      // Add CorrelationId to SwaggerUI\n      config.OperationFilter&lt;AddHeaderParameters&gt;();\n   });\n \n   ...\n   \n   // Log also ASP.Net request to Serilog\n   app.UseSerilogRequestLogging();\n \n   ...\n \n}\nConfiguration\n{\n  &quot;Serilog&quot;: {\n    &quot;Using&quot;: [ &quot;Serilog.Sinks.Console&quot;, &quot;Serilog.Sinks.File&quot; ],\n    &quot;MinimumLevel&quot;: {\n      &quot;Default&quot;: &quot;Debug&quot;,\n      &quot;Override&quot;: {\n        &quot;Microsoft.AspNetCore&quot;: &quot;Information&quot;,\n        &quot;Microsoft.AspNetCore.HttpLogging.HttpLoggingMiddleware&quot;: &quot;Information&quot;\n      }\n    },\n    &quot;WriteTo&quot;: [\n      {\n        &quot;Name&quot;: &quot;Console&quot;\n      },\n      {\n        &quot;Name&quot;: &quot;File&quot;,\n        &quot;Args&quot;: {\n          &quot;path&quot;: &quot;./logs/Road.API_.json&quot;,\n          &quot;rollingInterval&quot;: &quot;Day&quot;,\n          &quot;formatter&quot;: &quot;Serilog.Formatting.Compact.CompactJsonFormatter, Serilog.Formatting.Compact&quot;\n        }\n      }\n \n    ],\n    &quot;Enrich&quot;: [\n      &quot;FromLogContext&quot;,\n      {\n        &quot;Name&quot;: &quot;WithCorrelationId&quot;,\n        &quot;Args&quot;: {\n          &quot;headerName&quot;: &quot;x-correlation-id&quot;,\n          &quot;addValueIfHeaderAbsence&quot;: true\n        }\n      }\n    ],\n    &quot;Properties&quot;: {\n      &quot;Application&quot;: &quot;Road.API&quot;\n    }\n  },\n  &quot;AllowedHosts&quot;: &quot;*&quot;\n}\nInteresting is the Override, here we can set different levels for each logger.\nLogging Scope\npublic async Task&lt;RoadWarningsQueryResult&gt; GetRoadWarnings(string roadId, CancellationToken cancellationToken)\n{\n   using var loggerScope = logger.BeginScope(&quot;Get road warnings for highway {RoadId}&quot;, roadId);\n \n   var roadWarningsQuery = new RoadWarningsQuery(roadId.ToUpper(), SlidingExpirationInMinutes: 1);\n   var result = await mediator.Send(roadWarningsQuery, cancellationToken);\n \n   return result;\n}\nWith BeginScope, the property RoadId will be added to all log messages within this scope.\nCorrelationId\nThe CorrelationId can be set from external (HTTP Header) and will be added to all log messages.\nHTTP logging\npublic static void Main(string[] args)\n{\n \n    ...\n \n    // Add HttpLogging, but may cause performance issues\n   builder.Services.AddHttpLogging(options =&gt;\n   {\n      options.LoggingFields = HttpLoggingFields.RequestPropertiesAndHeaders | HttpLoggingFields.ResponsePropertiesAndHeaders;\n      options.RequestHeaders.Add(&quot;x-correlation-id&quot;);\n      options.CombineLogs = false;\n   });\n \n    ...\n \n   app.UseStaticFiles();\n \n    ... \n \n   // Log also HTTP to Serilog, static files are excluded because it called after UseStaticFiles()\n   // But may cause performance issues\n   app.UseHttpLogging();\n \n    ...\n}\nHTTPClient logging\npublic static void Main(string[] args)\n{\n   \n   ...\n   \n   // Add Serilog and use configuration from appsettings.json\n   builder.Host.UseSerilog((context, config) =&gt;\n   {\n      config.ReadFrom.Configuration(context.Configuration)\n   \n         // necessary for Serilog.LogRequestResponse\n         .AddJsonDestructuringPolicies();\n   });\n   \n   ...\n   \n   // Register the factory for the Autobahn client\n   builder.Services\n      .AddHttpClient&lt;AutobahnClientFactory&gt;(\n         (_, client) =&gt;\n         {\n            client.DefaultRequestHeaders.Add(&quot;Accept&quot;, &quot;application/json&quot;);\n         })\n   \n      // Attach the Kiota handlers to the http client, this is to enable all the Kiota features.\n      .AttachKiotaHandlers()\n      // HTTPClient logging\n      .LogRequestResponse();\n   \n   ...\n \n}"},"Dotnet/PDFSharp":{"title":"PDFSharp","links":[],"tags":["PDF","Dotnet"],"content":"References\nPDFSharp\nCode\nusing PdfSharp.Drawing;\nusing PdfSharp.Pdf;\nusing PdfSharp.Pdf.Advanced;\nusing PdfSharp.Pdf.IO;\n \nSystem.Text.Encoding.RegisterProvider(System.Text.CodePagesEncodingProvider.Instance);\n \nPdfDocument origDocument = PdfReader.Open(&quot;C:\\\\Users\\\\fleishor\\\\MyDevelopment\\\\MyPdfSharp\\\\MyPdfSharp\\\\Formular.pdf&quot;, PdfDocumentOpenMode.Import);\nPdfDocument newDocument = new PdfDocument();\n \nfor (int pageIndex = 0; pageIndex &lt; origDocument.Pages.Count; pageIndex++)\n{\n    newDocument.AddPage(origDocument.Pages[pageIndex]);\n}\n \nPdfPage page = newDocument.Pages[0];\n \nXGraphics gfx = XGraphics.FromPdfPage(page);\n \n/*\nXColor colorHelpLines = XColors.LightGreen;\nXPen lineRed = new XPen(colorHelpLines, 1);\nXFont font = new XFont(&quot;Verdana&quot;, 6, XFontStyle.Regular);\nXRect rect = new XRect(0, 0, 20, 10);\nfor (int y = 0; y &lt; page.Height; y = y + 24)\n{\n    gfx.DrawLine(lineRed, 0, y, page.Width, y);\n    rect.Y = y;\n    gfx.DrawString(y.ToString(), font, XBrushes.LightGreen, rect, XStringFormats.BottomLeft);\n}\n*/\n \nXFont fontText = new XFont(&quot;Verdana&quot;, 10, XFontStyle.Regular);\n \n \n \nXRect rectText = new XRect(65, 264, 350, 24);\ngfx.DrawString(&quot;Hello, World!&quot;, fontText, XBrushes.Black, rectText, XStringFormats.BottomLeft);\n \nrectText = new XRect(65, 288, 350, 24);\ngfx.DrawString(&quot;Hello, World!&quot;, fontText, XBrushes.Black, rectText, XStringFormats.BottomLeft);\n \n \nnewDocument.Save(&quot;C:\\\\Users\\\\fleishor\\\\MyDevelopment\\\\MyPdfSharp\\\\MyPdfSharp\\\\FormularAusgefuellt.pdf&quot;);"},"Elastic8/Glossar":{"title":"Elastic8 - Übersicht","links":["SmartHome/Docker/Installations/Promtail-with-Docker","SmartHome/Docker/Installations/Telegraf-with-Docker","Dotnet/HealthChecks"],"tags":["Elastic"],"content":"References\n\nInstall Elasticsearch with Docker\nElastic\n\nTerms\nIndex\nEin Index in Elasticsearch/Elastic 8 ist eine Sammlung von Dokumenten. Jedes Dokument besteht aus einer Reihe von Feldern, die als Schlüssel-Wert-Paare organisiert sind und die eigentlichen Daten repräsentieren. (Dokumenten-Datenbank)\nHier sind einige wichtige Eigenschaften zu Indizes:\n\nStruktur: Ein Elasticsearch-Index entspricht in einer relationalen Datenbank einer Tabelle. Allerdings können Indizes in Elasticsearch viele unterschiedlich strukturierte Dokumente enthalten.\nSchema: Sie müssen kein festes Schema definieren, bevor Sie Daten hinzufügen. Elasticsearch unterstützt dynamische Typen, was bedeutet, dass Sie Daten ohne vorherige Definition von Feldern hochladen können.\nSharding: Ein Index wird in Shards unterteilt, die eine parallele Verarbeitung und verteilte Speicherung ermöglichen. Dies verbessert die Skalierbarkeit und Leistung\n\nNodes\nEin Node ist eine Elasticsearch instance.\n\nShards\nJeder Index in Elasticsearch wird in Shards unterteilt.\nBeats\nÜbersicht\n\nFilebeat\nFilebeat ist ein leichtgewichtiger Log-Shipper, der im Elastic Stack verwendet wird, um Logdaten von verschiedenen Quellen zu sammeln und an Elasticsearch zu senden. Die Hauptfunktionen von Filebeat sind:\n\nLogdaten sammeln: Filebeat kann Logdaten von verschiedenen Quellen wie Servern, Containern oder Cloud-Diensten erfassen. Es “tailt” die Logdateien, was bedeutet, dass es die neuesten Einträge kontinuierlich überwacht und verarbeitet.\nDatenweiterleitung: Die gesammelten Logdaten werden entweder direkt an Elasticsearch zur Indizierung oder an Logstash zur weiteren Verarbeitung gesendet. Dies ermöglicht eine zentrale Verwaltung und Analyse der Logs in Echtzeit.\nZuverlässigkeit: Filebeat merkt sich den letzten verarbeiteten Logeintrag, sodass es nach einer Unterbrechung nahtlos dort weitermachen kann, wo es aufgehört hat. Dies ist besonders wichtig in Umgebungen, in denen Ausfälle auftreten können.\nModule und Integrationen: Filebeat bietet vorgefertigte Module für gängige Logformate, die das Sammeln, Parsen und Visualisieren von Daten erheblich vereinfachen. Diese Module sind so konzipiert, dass sie mit minimalem Aufwand eingerichtet werden können.\nCloud- und Container-freundlich: Filebeat ist ideal für moderne Cloud- und Container-Umgebungen, da es automatisch neue Container erkennt und deren Logs überwacht.\n\nVergleichbar zu Promtail-with-Docker\nMetricbeat\nMetricbeat ist ein leichtgewichtiger Metrik-Shipper. Er wird auf Servern installiert, um regelmäßig Metriken vom Betriebssystem und von darauf laufenden Diensten zu sammeln. Hier sind einige der Hauptfunktionen von Metricbeat:\n\nMetrikensammlung: Metricbeat erfasst verschiedene Systemmetriken, wie CPU- und Speicherauslastung, sowie spezifische Metriken von Diensten wie z.B. Apache, NGINX, MySQL.\nDatenversand: Die gesammelten Metriken werden an einen definierten Output gesendet, typischerweise an Elasticsearch oder Logstash, wo sie weiterverarbeitet werden können.\nContainer-Monitoring: Metricbeat kann auch in Container-Umgebungen eingesetzt werden, um Statistiken über andere Container auf demselben Host zu sammeln, ohne dass privilegierter Zugriff auf die Docker-API erforderlich ist.\n\nVergleichbar zu Telegraf-with-Docker\nHeartbeat\nHeartbeat ist ein leichtgewichtiger Shipper, der speziell für die Überwachung der Verfügbarkeit von Diensten entwickelt wurde. Hier sind die Hauptfunktionen von Heartbeat:\n\nUptime-Überwachung: Heartbeat führt regelmäßige Prüfungen durch, um festzustellen, ob bestimmte Dienste oder Endpunkte (wie HTTP, TCP oder ICMP) erreichbar sind.\nFlexible Konfiguration: Du kannst Heartbeat so konfigurieren, dass es verschiedene Endpunkte in unterschiedlichen Intervallen überprüft. Zum Beispiel kann ein Monitor alle 10 Minuten laufen, während ein anderer nur während der Bürozeiten aktiv ist.\nErweiterte Prüfungen: Heartbeat kann nicht nur die Erreichbarkeit prüfen, sondern auch spezifische Antworten, wie Statuscodes oder Inhalte, validieren.\n\nVergleichbar zu HealthChecks\nPacketbeat\nPacketbeat ist ein leichtgewichtiger Netzwerkpaketanalyzer, der speziell für die Überwachung und Analyse von Netzwerkverkehr entwickelt wurde. Hier sind die Hauptfunktionen von Packetbeat:\n\nEchtzeit-Überwachung: Packetbeat erfasst Netzwerkpakete in Echtzeit und sendet die gesammelten Daten an Elasticsearch oder Logstash.\nProtokollunterstützung: Es unterstützt eine Vielzahl von Anwendungsprotokollen, darunter HTTP, DNS, MySQL und viele andere. Dadurch kannst du die Leistung und Verfügbarkeit deiner Anwendungen überwachen.\nLeistungsanalyse: Mit Packetbeat kannst du wichtige Metriken wie Latenzzeiten, Fehler und Antwortzeiten analysieren, um die Leistung deiner Anwendungen zu optimieren.\nPassive Erfassung: Packetbeat arbeitet passiv, was bedeutet, dass es den Netzwerkverkehr ohne nennenswerte Latenz oder Störungen erfasst.\n\nWinlogbeat\nWinlogbeat ist ein leichtgewichtiger Shipper, der speziell für die Erfassung und den Versand von Windows-Ereignisprotokollen entwickelt wurde. Hier sind die Hauptfunktionen von Winlogbeat:\n\nEreignisprotokollierung: Winlogbeat liest Ereignisse aus verschiedenen Windows-Ereignisprotokollen (z. B. Sicherheits-, Anwendungs- und Systemprotokolle) und filtert diese basierend auf benutzerdefinierten Kriterien.\nDatenversand: Die gesammelten Ereignisdaten werden an Elasticsearch oder Logstash gesendet, wo sie analysiert und visualisiert werden können.\nEchtzeitüberwachung: Winlogbeat überwacht die Ereignisprotokolle kontinuierlich und sendet neue Ereignisse in Echtzeit, sodass du sofortige Einblicke in die Systemaktivitäten erhältst.\nFlexibilität: Du kannst Winlogbeat so konfigurieren, dass es aus beliebigen Ereignisprotokollkanälen liest, was dir Zugang zu den für dich wichtigsten Daten gibt.\n\nAuditbeat\nAuditbeat ist ein leichtgewichtiger Shipper, der speziell für die Überwachung von Benutzeraktivitäten und Prozessen auf Systemen entwickelt wurde. Hier sind die Hauptfunktionen von Auditbeat:\n\nEreignissammlung: Auditbeat kommuniziert direkt mit dem Linux-Audit-Framework und sammelt Auditdaten in Echtzeit. Es kann auch Ereignisse von Windows-Systemen erfassen.\nÜberwachung von Benutzeraktivitäten: Es ermöglicht die Überwachung von Benutzeraktionen und Prozessen, was hilft, potenzielle Sicherheitsverletzungen zu erkennen und die Einhaltung von Sicherheitsrichtlinien zu überprüfen.\nDateiintegritätsüberwachung: Auditbeat kann Änderungen an kritischen Dateien, wie Konfigurationsdateien oder Binärdateien, überwachen und meldet diese in Echtzeit.\nEinfache Integration: Die gesammelten Daten werden an Elasticsearch oder Logstash gesendet, wo sie analysiert und visualisiert werden können, oft in Kombination mit Kibana.\nEffiziente Datenverarbeitung: Auditbeat gruppiert verwandte Ereignisse zu einem einzigen Datensatz, was die Analyse erleichtert und die Datenstruktur verbessert.\n\nElasticsearch\nData Streams\nIn Elasticsearch sind Data Streams eine spezielle Funktion, die es ermöglicht, zeitbasierte Daten in einem append-only Format über mehrere Indizes hinweg zu speichern. Hier sind einige wichtige Punkte zu Data Streams:\n\nZweck: Data Streams sind ideal für kontinuierlich generierte Daten wie Logs, Ereignisse und Metriken. Sie bieten eine einfache Möglichkeit, Daten zu indizieren und abzufragen, ohne sich um die zugrunde liegenden Indizes kümmern zu müssen.\nAutomatische Verwaltung: Data Streams bestehen aus einer oder mehreren automatisch generierten, versteckten Indizes, die die Daten speichern. Sie können Index-Lifecycle-Management (ILM) verwenden, um die Verwaltung dieser Indizes zu automatisieren, z. B. um ältere Indizes auf kostengünstigere Hardware zu verschieben oder nicht mehr benötigte Indizes zu löschen.\nSchreibindex: Der zuletzt erstellte Index innerhalb eines Data Streams fungiert als Schreibindex, in den neue Dokumente eingefügt werden. Sie können keine neuen Dokumente in andere Indizes innerhalb des Streams einfügen.\nZeitstempel: Jedes Dokument, das in einen Data Stream indiziert wird, muss ein Feld mit dem Namen @timestamp enthalten, das als Datum oder Datum_Nanos typisiert ist. Dies ist entscheidend für die Verwaltung und Abfrage der Daten.\nEinschränkungen: Data Streams sind hauptsächlich für Szenarien gedacht, in denen bestehende Daten selten aktualisiert werden. Direkte Aktualisierungen oder Löschungen von Dokumenten sind nicht möglich, es sei denn, Sie greifen direkt auf den zugrunde liegenden Index zu.\n\n\nIndexes\nJeder Index hat einen DataStream aus dem die Daten kommen bzw. die Daten von einem DataStream werden an einen Index weitergeleitet.\n\n\nData Views\nCreate Data View\n\nDiscover Data View\n\nIndex Livecyle Management\nEin DataStream verweist auf eine Index livecycle policy:\n\n\nKibana UI\nDiscover Data View\n\nDiscover a Data View\n\nShow detailed information of a document\n\nObservability Logs\n\nSpaces\nIn Kibana sind Spaces eine Funktion, die es ermöglicht, Dashboards, Visualisierungen und andere gespeicherte Objekte in sinnvolle Kategorien zu organisieren. Hier sind einige wichtige Punkte zu Spaces:\n\nOrganisierung: Jeder Space ist unabhängig, was bedeutet, dass die Objekte in einem Space nicht die in einem anderen Space beeinflussen. Dies hilft, die Benutzeroberfläche übersichtlich zu halten.\nZugriffssteuerung: Der Zugriff auf Spaces kann basierend auf Benutzerrollen gesteuert werden. Das bedeutet, dass bestimmte Benutzer nur auf die Spaces zugreifen können, für die sie Berechtigungen haben.\nStandard-Space: Kibana erstellt automatisch einen Standard-Space, in dem alle bestehenden gespeicherten Objekte zu finden sind, wenn du von einer früheren Version aktualisierst.\nBenutzerfreundlichkeit: Bei der Anmeldung in Kibana wirst du aufgefordert, einen Space auszuwählen, und du kannst jederzeit zwischen den Spaces wechseln.\nAnpassung: Du kannst Spaces benennen, beschreiben und sogar ein individuelles Avatar-Bild hinzufügen, um sie visuell zu unterscheiden.\n\nAktuell ausgewählter Space\n"},"Kubernetes/ClusterIP-Service":{"title":"ClusterIP Service","links":[],"tags":["Kubernetes","Nginx","ClusterIP"],"content":"ClusterIP Service\nÜbersicht\nCheck ClusterIP service\n\nDer ClusterIP Service hat wieder einen eigenen IP-Bereich (10.110.0.0/16)\n\nfleishor@desktop:~/vagrant-kubernetes-cluster$ kubectl get services\nNAME                      TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE\nkubernetes                ClusterIP   10.96.0.1     &lt;none&gt;        443/TCP    17d\nnginx-service-clusterip   ClusterIP   10.110.1.81   &lt;none&gt;        8080/TCP   23h\n\nEigenschaften vom ClusterIP service\n\nDer ClusterIP Service (Adresse) ist nur innerhalb des Pod Netzwerkes verfügbar\nDer ClusterIP Service Name kann gleichzeitig als DomainName verwendet werden (FQDN: nginx-server-clusterip.default.svc.cluster.local)\nHTTP Anfragen an den ClusterIP Service leitet dieser an die Pods weiter (Round-Robin-Verfahren oder sowas ähnliches)\nHauptaufgabe der ClusterIP ist das zusammensuchen der Pods und ein LoadBalancing auf die Pods\nInnerhalb eines Pods kann auch direkt auf einen anderen Pod zugegriffen werden.\n\nfleishor@desktop:~/vagrant-kubernetes-cluster$ kubectl exec -it nginx-deployment-c4b4cb578-4jqtr -c netshoot-container -- /bin/bash\nbash-5.2# curl nginx-service-clusterip:8080\nnginx-deployment-c4b4cb578-4jqtr\nbash-5.2# curl nginx-service-clusterip:8080\nnginx-deployment-c4b4cb578-5jcxd\nbash-5.2# curl nginx-service-clusterip:8080\nnginx-deployment-c4b4cb578-8qs5h\n\nService file\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service-clusterip\n  labels:\n    app: nginx-clusterip # label for the ClusterIP service\nspec:\n  selector:\n    app: nginx-pods # Which Pods should be handled by this service\n  type: ClusterIP\n  ports:\n  - name: http\n    port: 8080 # Exposed Port by ClusterIP\n    targetPort: 80 # Port in Pod which should be exposed\n    protocol: TCP"},"Kubernetes/Create-Vagrant-BaseBox-For-Ubuntu2204":{"title":"Create a Vagrant base box for Ubuntu 22.04","links":[],"tags":["Vagrant"],"content":"Quellen\n\nGithub\n\nVagrantfile\n\n# Set IP Adresses of Master and Worker nodes\nDESKTOP_IP      = &quot;192.168.56.1&quot;\nUBUNTU_IP      = &quot;192.168.56.8&quot;\n \nVagrant.configure(&quot;2&quot;) do |config|\n  config.vm.box = &quot;generic/ubuntu2204&quot;\n  config.vm.box_check_update = false\n  config.vm.synced_folder &quot;.&quot;, &quot;/vagrant&quot;\n  config.ssh.insert_key=true\n \n  # disable vbguest additions\n  if Vagrant.has_plugin(&quot;vagrant-vbguest&quot;)\n    config.vbguest.auto_update = true\n  end\n \n  # use proxy at host machine\n  if Vagrant.has_plugin(&quot;vagrant-proxyconf&quot;)\n    config.apt_proxy.http = &quot;http://192.168.178.44:3142&quot;\n    config.apt_proxy.https = &quot;http://192.168.178.44:3142&quot;\n  end\n \n  # define cpu/memory of nodes\n  nodes = [\n    { :name =&gt; &quot;ubuntu2204&quot;  , :ip =&gt; UBUNTU_IP,   :cpus =&gt; 2, :memory =&gt; 4096, :disksize =&gt; &quot;32GB&quot; },\n  ]\n \n  # create virtual machine\n  nodes.each do |opts|\n    config.vm.define opts[:name] do |node|\n      node.vm.hostname = opts[:name]\n#      node.vm.network &quot;private_network&quot;, ip: opts[:ip]\n \n      node.vm.provider &quot;virtualbox&quot; do |vb|\n        vb.name = opts[:name]\n        vb.cpus = opts[:cpus]\n        vb.memory = opts[:memory]\n      end\n      \n       # special provision for ubuntu\n      if node.vm.hostname == &quot;ubuntu2204&quot; then\n        node.vm.provision &quot;shell&quot;, path:&quot;./provision_ubuntu.sh&quot;\n      end\n    end\n  end\nend\nprovision_ubuntu.sh\n\n#!/bin/bash -e\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Disable IPv6&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nsudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\ncat &lt;&lt;EOF | sudo tee -a /etc/default/grub\nGRUB_CMDLINE_LINUX=&quot;ipv6.disable=1&quot;\nEOF\nsudo update-grub\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Prepare /etc/hosts&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nsudo tee /etc/hosts&lt;&lt;EOF\n192.168.56.1    desktop.fritz.box    desktop\n192.168.56.8    admin.vboxnet0       admin\n192.168.56.9    nfsserver.vboxnet0   nfsserver\n192.168.56.10   master.vboxnet0      master\n192.168.56.11   node-01.vboxnet0     node-01\n192.168.56.12   node-02.vboxnet0     node-02\n192.168.56.13   node-03.vboxnet0     node-03\nEOF\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Disable SWAP&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nsudo sed -i &#039;/\\sswap\\s/ s/^\\(.*\\)$/#\\1/g&#039; /etc/fstab\nsudo swapoff -a\nsudo rm /swap.img || true\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Load Kernel modules&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nsudo modprobe overlay\nsudo modprobe br_netfilter\nsudo cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/kubernetes.conf\noverlay\nbr_netfilter\nEOF\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Kernel settings&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nsudo tee /etc/sysctl.d/kubernetes.conf&lt;&lt;EOF\nnet.bridge.bridge-nf-call-ip6tables = 0\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\nEOF\nsudo sysctl --system\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Switch apt from https to http in order to squid-deb&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nsudo cp /etc/apt/sources.list /etc/apt/sources.list.backup\nsudo sed -i &#039;s/https:\\/\\//http:\\/\\//g&#039; /etc/apt/sources.list\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Update Ubuntu (1. Round)&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nsudo apt-get -y update\nsudo apt-get -y upgrade\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Update SSH login&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nwget -O /home/vagrant/.ssh/authorized_keys raw.githubusercontent.com/hashicorp/vagrant/master/keys/vagrant.pub\n \nVagrant commands\nPackage base box\nvagrant package --base ubuntu2204 --output ubuntu2204.box\n\nAdd base box to repository\nvagrant box list\nvagrant box remove fleishor/ubuntu2204\nvagrant box add ubuntu2204.box --name fleishor/ubuntu2204\n"},"Kubernetes/Deploy-Nginx-und-NetShoot":{"title":"Deploy Nginx und NetShoot","links":[],"tags":["Kubernetes"],"content":"Übersicht\nDiese Deployment erzeugt 6 Pods mit jeweils 2 Containers (nginx und netshoot) und 1 Init-Container\nSkripte\nDeployment file\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx-deployment # Label of the Deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pods # Which Pods should be handled by this ReplicaSet\n  template:\n    metadata:\n      labels:\n        app: nginx-pods # Label of the Pods which is used by ReplicaSet and \n                        # ClusterIP Service\n    spec:\n      containers:\n      - name: nginx-container # Name of the container\n        image: nginx:latest # Image name and version\n        ports:\n        - containerPort: 80 # Exposed port from the container\n        volumeMounts:\n        - name: local-volume # Name of the volumeMount to use in the container\n          mountPath: /usr/share/nginx/html # The mountpoint inside the container\n      - name: netshoot-container # SideCar container for network analyzes\n        image: nicolaka/netshoot\n        volumeMounts:\n        - name: local-volume\n          mountPath: /html\n        command: [ &quot;sleep&quot; ] # Sleep infinite, because only shell access \n                             # should be possible\n        args: [ &quot;infinity&quot; ]\n      initContainers:\n      - name: init-container # InitContainer for creating /html/index.html\n        image: busybox\n        volumeMounts:\n        - name: local-volume\n          mountPath: /html\n        command: [&quot;/bin/sh&quot;, &quot;-c&quot;]\n        args:\n          - hostname &gt; /html/index.html; sleep 5;\n      volumes:\n      - name: nfs-volume # NFS volumn from NFS server which is currently not used\n        nfs:\n          server: nfsserver\n          path: /mnt/nfs_share # The exported directory/share at NFS server\n      - name: local-volume # for each Pod local volumne, is initialized by InitContainer\n        emptyDir: {}  # Empty directory within each Pod; \n                      # shared directory over all containers within the same pod\nnginx container\n\nFührt eine nginx Webserver im Pod aus.\nDas nginx Verzeichnis /usr/share/nginx/html wird ein Pod-internes Verzeichnis gemounted\nAusserhalt des Pods ist dieses Verzeichnis nicht verfügbar und wenn der Pod sich beendet wird der Inhalt des Verzeichnisses auch gelöscht. (nicht persistent)\n\ninit container\n\nDer init container schreibt den hostname/podname nach /html/index.html\nDie index.html wird dann über den nginx ausgeliefert\nAnschliessend beendet sich der init container und gibt somit den nginx container und netshoot container &quot;&quot;frei”\n\nnetshoot container\n\nNetshoot container ist ein side-car-container und wird nur in dem Pod mit installiert, damit auf auf das Pod-Netzwerk zugegriffen werden kann. Im nginx container fehlen zahlreiche tools, z.B. ip, curl, …\n"},"Kubernetes/Installation-eines-Kubernetes-Cluster-mit-Vagrant":{"title":"Installation eines Kubernetes Clusters mit Vagrant","links":[],"tags":["Kubernetes","Vagrant"],"content":"Quellen\n\nGithub\nCreating a Local Kubernetes Cluster with Vagrant\n3 Virtual Machines Kubernetes cluster\n\nNetzwerk\n\nSkripte\nVagrantfile\n\n# Set IP Adresses of Master and Worker nodes\nADMIN_IP        = &quot;192.168.56.8&quot;\nNFSSERVER_IP    = &quot;192.168.56.9&quot;\nMASTER_IP       = &quot;192.168.56.10&quot;\nNODE_01_IP      = &quot;192.168.56.11&quot;\nNODE_02_IP      = &quot;192.168.56.12&quot;\nNODE_03_IP      = &quot;192.168.56.13&quot;\n \nVagrant.configure(&quot;2&quot;) do |config|\n  config.vm.box = &quot;fleishor/ubuntu2204-2023-12-01&quot;\n  config.vm.box_check_update = false\n  config.vm.synced_folder &quot;.&quot;, &quot;/vagrant&quot;\n \n  # disable vbguest additions\n  if Vagrant.has_plugin(&quot;vagrant-vbguest&quot;)\n    config.vbguest.auto_update = false \n  end\n \n  # use proxy at host machine\n  if Vagrant.has_plugin(&quot;vagrant-proxyconf&quot;)\n    config.apt_proxy.http = &quot;desktop.fritz.box:3142&quot;\n    config.apt_proxy.https = &quot;DIRECT&quot;\n  end\n \n  # define cpu/memory of nodes\n  nodes = [\n    { :name =&gt; &quot;admin&quot;,     :ip =&gt; ADMIN_IP,     :cpus =&gt; 2, :memory =&gt; 2096, :disksize =&gt; &quot;32GB&quot; },\n    { :name =&gt; &quot;nfsserver&quot;, :ip =&gt; NFSSERVER_IP, :cpus =&gt; 2, :memory =&gt; 2096, :disksize =&gt; &quot;32GB&quot; },\n    { :name =&gt; &quot;master&quot;,    :ip =&gt; MASTER_IP,    :cpus =&gt; 2, :memory =&gt; 4096, :disksize =&gt; &quot;32GB&quot; },\n    { :name =&gt; &quot;node-01&quot;,   :ip =&gt; NODE_01_IP,   :cpus =&gt; 2, :memory =&gt; 4096, :disksize =&gt; &quot;32GB&quot; },\n    { :name =&gt; &quot;node-02&quot;,   :ip =&gt; NODE_02_IP,   :cpus =&gt; 2, :memory =&gt; 4096, :disksize =&gt; &quot;32GB&quot; },\n    { :name =&gt; &quot;node-03&quot;,   :ip =&gt; NODE_03_IP,   :cpus =&gt; 2, :memory =&gt; 4096, :disksize =&gt; &quot;32GB&quot; },\n  ]\n \n  # create virtual machine\n  nodes.each do |opts|\n    config.vm.define opts[:name] do |node|\n      node.vm.hostname = opts[:name]\n      node.vm.network &quot;private_network&quot;, ip: opts[:ip]\n#      node.vm.disk :disk, size: opts[:disksize], primary: true\n \n      node.vm.provider &quot;virtualbox&quot; do |vb|\n        vb.name = opts[:name]\n        vb.cpus = opts[:cpus]\n        vb.memory = opts[:memory]\n      end\n \n      # special provision for admin\n      if node.vm.hostname == &quot;admin&quot; then \n        node.vm.provision &quot;shell&quot;, path:&quot;./provision_admin.sh&quot;\n      end\n      \n      # special provision for nfsserver\n      if node.vm.hostname == &quot;nfsserver&quot; then \n        node.vm.provision &quot;shell&quot;, path:&quot;./provision_nfsserver.sh&quot;\n      end\n      \n      # special provision for master\n      if node.vm.hostname == &quot;master&quot; then \n        node.vm.provision &quot;shell&quot;, path:&quot;./provision_kubernetes.sh&quot;\n        node.vm.provision &quot;shell&quot;, path:&quot;./provision_master.sh&quot;\n      end\n \n      # special provision for worker nodes\n      if node.vm.hostname =~ /node-[0-9]*/ then\n        node.vm.provision &quot;shell&quot;, path:&quot;./provision_kubernetes.sh&quot;\n        node.vm.provision &quot;shell&quot;, path:&quot;./provision_node.sh&quot;\n      end\n    end\n  end\nend\nprovision_nfsserver.sh\n\n#!/bin/bash -e\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Install NFS server&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nsudo apt-get update -y\nsudo apt-get install -y nfs-kernel-server\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Create NFS share&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nsudo mkdir -p /mnt/nfs_share\nsudo chown -R nobody:nogroup /mnt/nfs_share/\nsudo chmod 777 /mnt/nfs_share/\n \n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Export NFS share&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;/mnt/nfs_share 192.168.56.9/24(rw,sync,no_subtree_check,no_root_squash)&quot; &gt;&gt; /etc/exports\nsudo exportfs -a\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Restart NFS server&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nsudo systemctl restart nfs-kernel-server\nprovision_kubernetes.sh\n\n#!/bin/bash -e\necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Install containerd&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nsudo apt-get update -y\nsudo apt-get install -y nfs-common\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Install containerd&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nsudo apt-get update -y\nsudo apt-get install -y containerd\nsudo mkdir -p /etc/containerd\ncontainerd config default | sudo tee /etc/containerd/config.toml\nsudo sed -i &#039;s/SystemdCgroup = false/SystemdCgroup = true/&#039; /etc/containerd/config.toml\nsudo systemctl daemon-reload \nsudo systemctl restart containerd\nsudo systemctl enable containerd\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Install Kubernetes&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nsudo apt-get update -y\nsudo apt-get install -y ca-certificates curl\ncurl -fsSL packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-archive-keyring.gpg\necho &quot;deb [signed-by=/etc/apt/keyrings/kubernetes-archive-keyring.gpg] apt.kubernetes.io/ kubernetes-xenial main&quot; | sudo tee /etc/apt/sources.list.d/kubernetes.list\nsudo apt-get update\nsudo apt-get install -y kubelet kubeadm kubectl kubernetes-cni\nsudo apt-mark hold kubelet kubeadm kubectl\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Mount NFS share&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nmkdir /mnt/nfs_share\nsudo mount nfsserver:/mnt/nfs_share /mnt/nfs_share\nprovision_master.sh\n\n#!/bin/bash -e\n \nmaster_node=192.168.56.10\npod_network_cidr=10.244.0.0/16\nnode_name=$(hostname -s)\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Start kubernetes services&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nsudo systemctl enable kubelet\nsudo systemctl start kubelet\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Download kubernetes images&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nsudo kubeadm config images pull\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Initialize kubernetes&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nsudo kubeadm init --apiserver-advertise-address=$master_node --apiserver-cert-extra-sans=$master_node --pod-network-cidr=$pod_network_cidr --node-name $node_name\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Wait 60s&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nsleep 60\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Copy kubernetes config file to shared folder /vagrant/admin.conf&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nsudo cp -f /etc/kubernetes/admin.conf /vagrant/admin.conf\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Copy kubernetes config file from shared folder to /root/.kube&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nmkdir -p /root/.kube\nsudo cp -f /vagrant/admin.conf /root/.kube/config\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Copy kubernetes config file from shared folder to /home/vagrant/.kube&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nmkdir -p /home/vagrant/.kube\nsudo cp -f /vagrant/admin.conf /home/vagrant/.kube/config\nsudo chown 1000:1000 /home/vagrant/.kube/config\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Generate join command&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nkubeadm token create --print-join-command | tee /vagrant/join_command.sh\nchmod +x /vagrant/join_command.sh\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Install flannel network&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nkubectl apply -f /vagrant/flannel.yaml\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Install HELM&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\ncurl baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\necho &quot;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] baltocdn.com/helm/stable/debian/ all main&quot; | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\nprovision_node.sh\n\n#!/bin/bash -e\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Copy kubernetes config file from shared folder to /root/.kube&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nmkdir -p /root/.kube\nsudo cp -f /vagrant/admin.conf /root/.kube/config\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Copy kubernetes config file from shared folder to /home/vagrant/.kube&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nmkdir -p /home/vagrant/.kube\nsudo cp -f /vagrant/admin.conf /home/vagrant/.kube/config\nsudo chown 1000:1000 /home/vagrant/.kube/config\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Join worker node to kubernetes cluster&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nsudo /vagrant/join_command.sh\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Set worker node name in kubernetes cluster&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nwhoami\nkubectl label node $(hostname -s) node-role.kubernetes.io/worker=worker\nprovision_metallb.sh\n\necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Install metallb&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\n# github.com/metallb/metallb/issues/1540\n# Update failurePolicy=Ignore for rule ValidatingWebhookConfiguration for metallb-webhook-configuration\nkubectl apply -f /vagrant/metallb-native.yaml\nkubectl create secret generic -n metallb-system memberlist --from-literal=secretkey=&quot;$(openssl rand -base64 128)&quot;\n \necho &quot;--------------------------------------------------------------------------------&quot;\necho &quot;Create configuration for metallb&quot;\necho &quot;--------------------------------------------------------------------------------&quot;\nkubectl apply -f /vagrant/metallb-configuration.yaml\nprovision_prometheus.sh\n\nhelm repo add prometheus-community prometheus-community.github.io/helm-charts\nhelm repo add stable charts.helm.sh/stable\n \nkubectl create namespace prometheus\nhelm install prometheus-community/kube-prometheus-stack --generate-name --namespace prometheus"},"Kubernetes/Inter-Pod-Kommunikation-mit-Flannel":{"title":"Inter-Pod-Kommunkation mit Flannel","links":["tags/Kubernetes"],"tags":["Kubernetes"],"content":"Übersicht\nInnerhalb Kubernetes kann sich jeder Pod mit jedem anderem Pod unterhalten, unabhängig ob er auf dem gleichen Knoten läuft oder auf einem anderem. Gelöst wird das über das CNI (Container Network Interface) von Kubernetes. Hier wird erklärt, wie es mit Flannel funktioniert.\nAblauf\n\nKonfiguration Flannel\n\nIn der flannel.yaml muß Flannel auf die richtige (enp0s8) Netzwerk-Karte gesetzt werden\nStandardmäßig wird immer die erste verwendet und das ist im Falle von vagrant der ssh Zugang (NAT)\n\ncontainers:\n      - name: kube-flannel\n        image: docker.io/rancher/mirrored-flannelcni-flannel:v0.19.2\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        - --iface=enp0s8\n        resources:\n\nCurl von einem Pod auf node-01 zu einem Pod auf node-02\n\nIch verbinde mich mit dem netshoot-container im Pod -7xztr und mache einen curl aufruf in den Pod -cthsp der auf dem Node-02 läuft.\nAnhand der Antwort (Hostname) sehen wir, dass wirklich der nginx auf dem anderen Knoten aufgerufen wurde.\n\nfleishor@desktop:~/vagrant-kubernetes-cluster$ kubectl get pods -o wide\nNAME                                READY   STATUS    RESTARTS       AGE    IP            NODE      NOMINATED NODE   READINESS GATES\nnginx-deployment-84d477cd8f-7xztr   2/2     Running   4 (3h2m ago)   7d1h   10.244.1.54   node-01   &lt;none&gt;           &lt;none&gt;\nnginx-deployment-84d477cd8f-cthsp   2/2     Running   0              3h     10.244.2.42   node-02   &lt;none&gt;           &lt;none&gt;\nnginx-deployment-84d477cd8f-dqf5x   2/2     Running   4 (3h1m ago)   7d1h   10.244.3.28   node-03   &lt;none&gt;           &lt;none&gt;\nnginx-deployment-84d477cd8f-gqvj4   2/2     Running   0              3h     10.244.3.29   node-03   &lt;none&gt;           &lt;none&gt;\nnginx-deployment-84d477cd8f-mmgcr   2/2     Running   4 (3h1m ago)   7d1h   10.244.2.41   node-02   &lt;none&gt;           &lt;none&gt;\nnginx-deployment-84d477cd8f-vmpsn   2/2     Running   0              3h     10.244.1.55   node-01   &lt;none&gt;           &lt;none&gt;\nfleishor@desktop:~/vagrant-kubernetes-cluster$ kubectl exec -it nginx-deployment-84d477cd8f-7xztr -c netshoot-container -- /bin/bash\nbash-5.2# ^C\nbash-5.2# curl 10.244.2.42\nnginx-deployment-84d477cd8f-cthsp\nbash-5.2#\n\nDer Auflauf ist wie folgt:\n\nAufruf von ‘curl 10.244.2.42’ im Container netshoot-container im Pod -7xztr\nNetzwerkanfrage wird auf veth (10.244.1.54) und weiter auf den veth-Endpunkt im cni0 weiter geleitet\nCNI stellt fest, dass die IP-Adresse ausserhalb der Bridge-Adresse (10.244.1.0/24) ist und gibt die Netzwerk-Pakete an den zugehörigen default router weiter (10.244.1.1)\nDie Routing-Einträge auf dem Host node-01 geben vor, dass alles auf 10.244.2.0/24 and das Netzwerk-Interface flannel.1 geleitet werden sollen\nAuf flannel.1 nimmt der flanneld-Daemon die Netzwerk-Pakete entgegen, verpackt diese ins VXLAN-Format und schickt sie via UDP an den node-02\nDie Zuordnung welcher Knoten welches Pod-Netzwerk hat, wird in der ETC-Datenbank von Kubernetes gespeichert\nAuf node-02 nimmt flannel.d das Packet entgegen, entpackt es und gibt es and die Netzwerk-Bridge cni0 auf node-02 weiter.\nDie Netzwerk-Bridge letzte das Netzwerkpackt dann an den zugehörigen Pod weiter\n\nAuf den Node-01 laufen 2 Pods (mit jeweils 2 Containern drinnen)\nfleishor@desktop:~/vagrant-kubernetes-cluster$ kubectl get pods -o wide | grep node-01\nnginx-deployment-84d477cd8f-7xztr   2/2     Running   4 (91m ago)   6d23h   10.244.1.54   node-01   &lt;none&gt;           &lt;none&gt;\nnginx-deployment-84d477cd8f-vmpsn   2/2     Running   0             89m     10.244.1.55   node-01   &lt;none&gt;           &lt;none&gt;\nfleishor@desktop:~/vagrant-kubernetes-cluster$\n\nNetzwerk-Konfiguration im Pod -7xztr\nIch verbinde mich direkt mit dem netshoot-Container im Pod -7xztr und lass mir die Netzwerk-Konfiguration anzeigen:\nfleishor@desktop:~/vagrant-kubernetes-cluster$ kubectl exec -it nginx-deployment-84d477cd8f-7xztr -c netshoot-container -- /bin/bash\n\nbash-5.2# ip addr show\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN \n       group default qlen 1000\n       link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n       inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n3: eth0@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP \n             group default\n             link/ether 3e:2e:2e:5c:cd:87 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n             inet 10.244.1.54/24 brd 10.244.1.255 scope global eth0\n             valid_lft forever preferred_lft forever\n\n\nEth0@if7 ist eine virtuelle Netzwerk-Karte dessen Gegenstück mit der Bridge cni0 im Node-01 verbunden ist.\nBeachte auch die IP-Adresse, entspricht der IP-Adresse des Pods.\nJeder Pod bekommt eine IP-Adresse aus dem Bereich der dem Node zugeordent ist. Für Node-01 ist das 10.244.1.0/24. Für den Node-02 wäre das 10.244.2.0/24.\n\nBridge-Interface cni0 auf Host node-01\nDas CNI legt auch auf dem Host-Rechner node-01 eine Netzwerk-Bridge cni0 an.\nvagrant@node-01:~$ ip link show type bridge\n5: cni0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP mode \n         DEFAULT group default qlen 1000\n         link/ether 2a:c4:20:2a:2b:de brd ff:ff:ff:ff:ff:ff\nvagrant@node-01:~$\n\nDieser Bridge werden alle Endpunkte der Virtuellen Netzwerkkarten aus den Pods zugeordnet.\nvagrant@node-01:~$ ip link show type veth\n6: veth4df30ebd@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue \n                     master cni0 state UP mode DEFAULT group default\n                     link/ether 92:c0:05:2e:28:b4 brd ff:ff:ff:ff:ff:ff \n                     link-netns cni-6c483948-280e-5b26-4fc8-706a417619c0\n7: veth47afc3a1@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue \n                     master cni0 state UP mode DEFAULT group default\n                     link/ether ca:74:81:e9:3c:70 brd ff:ff:ff:ff:ff:ff \n                     link-netns cni-5eca8fa8-eeac-23d8-0cc0-f62e2411cb43\n8: veth6a213ca6@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue \n                     master cni0 state UP mode DEFAULT group default\n                     link/ether a6:07:77:a0:05:b6 brd ff:ff:ff:ff:ff:ff \n                     link-netns cni-a1923bef-9f99-069d-a164-39227325bdf0\nvagrant@node-01:~$\n\nVirtuelle Netzwerk-Interface die mit der Bridge CNI0 verbunden sind\nvagrant@node-01:~$ bridge link show\n6: veth4df30ebd@enp0s8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 \n                        master cni0 state forwarding priority 32 cost 2\n7: veth47afc3a1@enp0s8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 \n                        master cni0 state forwarding priority 32 cost 2\n8: veth6a213ca6@enp0s8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 \n                        master cni0 state forwarding priority 32 cost 2\nvagrant@node-01:~$\n\nAlle Netzwerk-Interface auf Host node-01\nvagrant@node-01:~$ ip link show\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode\n       DEFAULT group default qlen 1000\n       link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n2: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP mode \n           DEFAULT group default qlen 1000\n           link/ether 08:00:27:64:75:a1 brd ff:ff:ff:ff:ff:ff\n3: enp0s8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP mode \n           DEFAULT group default qlen 1000\n           link/ether 08:00:27:af:59:f2 brd ff:ff:ff:ff:ff:ff\n4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue \n              state UNKNOWN mode DEFAULT group default\n              link/ether c2:08:ec:f4:a0:6a brd ff:ff:ff:ff:ff:ff\n5: cni0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue \n         state UP mode DEFAULT group default qlen 1000\n         link/ether 2a:c4:20:2a:2b:de brd ff:ff:ff:ff:ff:ff\n6: veth4df30ebd@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc \n                     noqueue master cni0 state UP mode DEFAULT group default\n                     link/ether 92:c0:05:2e:28:b4 brd ff:ff:ff:ff:ff:ff \n                     link-netns cni-6c483948-280e-5b26-4fc8-706a417619c0\n7: veth47afc3a1@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc \n                     noqueue master cni0 state UP mode DEFAULT group default\n                     link/ether ca:74:81:e9:3c:70 brd ff:ff:ff:ff:ff:ff \n                     link-netns cni-5eca8fa8-eeac-23d8-0cc0-f62e2411cb43\n8: veth6a213ca6@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc \n                     noqueue master cni0 state UP mode DEFAULT group default\n                     link/ether a6:07:77:a0:05:b6 brd ff:ff:ff:ff:ff:ff \n                     link-netns cni-a1923bef-9f99-069d-a164-39227325bdf0\n\nIP-Adresses of Host node-01\nvagrant@node-01:~$ ip addr show\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group \n        default qlen 1000\n        link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n        inet 127.0.0.1/8 scope host lo\n        valid_lft forever preferred_lft forever\n2: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel \n           state UP group default qlen 1000\n           link/ether 08:00:27:64:75:a1 brd ff:ff:ff:ff:ff:ff\n           inet 10.0.2.15/24 metric 100 brd 10.0.2.255 scope global dynamic enp0s3\n           valid_lft 79395sec preferred_lft 79395sec\n3: enp0s8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel \n           state UP group default qlen 1000\n           link/ether 08:00:27:af:59:f2 brd ff:ff:ff:ff:ff:ff\n           inet 192.168.56.11/24 brd 192.168.56.255 scope global enp0s8\n           valid_lft forever preferred_lft forever\n4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue \n              state UNKNOWN group default\n              link/ether c2:08:ec:f4:a0:6a brd ff:ff:ff:ff:ff:ff\n              inet 10.244.1.0/32 scope global flannel.1\n              valid_lft forever preferred_lft forever\n5: cni0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP \n         group default qlen 1000\n         link/ether 2a:c4:20:2a:2b:de brd ff:ff:ff:ff:ff:ff\n         inet 10.244.1.1/24 brd 10.244.1.255 scope global cni0\n         valid_lft forever preferred_lft forever\n6: veth4df30ebd@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc \n                     noqueue master cni0 state UP group default\n                     link/ether 92:c0:05:2e:28:b4 brd ff:ff:ff:ff:ff:ff \n                     link-netns cni-6c483948-280e-5b26-4fc8-706a417619c0\n7: veth47afc3a1@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc \n                     noqueue master cni0 state UP group default\n                     link/ether ca:74:81:e9:3c:70 brd ff:ff:ff:ff:ff:ff \n                     link-netns cni-5eca8fa8-eeac-23d8-0cc0-f62e2411cb43\n8: veth6a213ca6@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc \n                     noqueue master cni0 state UP group default\n                     link/ether a6:07:77:a0:05:b6 brd ff:ff:ff:ff:ff:ff \n                     link-netns cni-a1923bef-9f99-069d-a164-39227325bdf0\n\nRouting auf Host node-01\nEnsprechend wird auch das Routing auf dem Host node-01 angepasst\nvagrant@node-01:~$ ip route\ndefault via 10.0.2.2 dev enp0s3 proto dhcp src 10.0.2.15 metric 100\n10.0.2.0/24 dev enp0s3 proto kernel scope link src 10.0.2.15 metric 100\n10.0.2.2 dev enp0s3 proto dhcp scope link src 10.0.2.15 metric 100\n10.0.2.3 dev enp0s3 proto dhcp scope link src 10.0.2.15 metric 100\n10.244.0.0/24 via 10.244.0.0 dev flannel.1 onlink\n10.244.1.0/24 dev cni0 proto kernel scope link src 10.244.1.1\n10.244.2.0/24 via 10.244.2.0 dev flannel.1 onlink\n10.244.3.0/24 via 10.244.3.0 dev flannel.1 onlink\n192.168.56.0/24 dev enp0s8 proto kernel scope link src 192.168.56.11\n\nFlanneld Daemon\n\nFlanneld erzeugt entsprechende Routingeinträge und horcht auf das Netzwerk-Interface fannel.1.\nErnimmt diese Packet verpackt sie in VXLAN und schickt sie via UDP an den ensprechenden Cluster-Node weiter\nFlannedld horcht auch auf das externe Netzwerk Interface, nimmt VXLAN-Pakete entgegen, entpackt diese und leitete sie an die Netzwerk-Bridge cni0 weiter\n\nroot        2704    2199  0 13:12 ?        00:00:03 /opt/bin/flanneld --ip-masq --kube-subnet-mgr --iface=enp0s8\nQuellen\n\nUnderstanding Kubernetes Networking. Part 2: POD Network, CNI, and Flannel CNI Plug-in\nKubernetes network stack fundamentals: How pods on different nodes communicate\n\nTags\nNetzwerk Kubernetes\nPflegeanleitung\n\n15.11.2022 gepflanzt\n28.11.2022 Flannel Konfiguration hinzugefügt\n"},"Linux/Backup-HDD-to-Image-And-Mount":{"title":"Backup HDD to image file and mount the image file","links":[],"tags":["Linux"],"content":"Get partitions of a HDD\nsudo parted -l\n \nModell: ATA SanDisk SSD PLUS (scsi)\nFestplatte  /dev/sda:  480GB\nSektorgröße (logisch/physisch): 512B/512B\nPartitionstabelle: gpt\nDisk-Flags: \n \nNummer  Anfang  Ende   Größe  Dateisystem  Name                  Flags\n 1      1049kB  538MB  537MB  fat32        EFI System Partition  boot, esp\n 2      538MB   480GB  480GB  ext4\n \n \nModell: ATA WDC WD10EZEX-21W (scsi)\nFestplatte  /dev/sdb:  1000GB\nSektorgröße (logisch/physisch): 512B/4096B\nPartitionstabelle: gpt\nDisk-Flags: \n \nNummer  Anfang  Ende    Größe   Dateisystem  Name  Flags\n 1      1049kB  1000GB  1000GB  ext4\n \n \nModell: ST950032 5AS (scsi)\nFestplatte  /dev/sdc:  500GB\nSektorgröße (logisch/physisch): 512B/512B\nPartitionstabelle: gpt\nDisk-Flags: \n \nNummer  Anfang  Ende   Größe   Dateisystem  Name                          Flags\n 1      1049kB  420MB  419MB   ntfs         Basic data partition          versteckt, diag\n 2      420MB   735MB  315MB   fat32        EFI system partition          boot, esp\n 3      735MB   869MB  134MB                Microsoft reserved partition  msftres\n 4      869MB   243GB  242GB   ntfs         Basic data partition          msftdata\n 5      243GB   483GB  241GB   ntfs         Basic data partition          msftdata\n 6      483GB   500GB  16,6GB  ntfs         Basic data partition          versteckt, diag\nDump HDD to image file\nsudo dd if=/dev/sdc4 of=sdc4.img status=progress\nMount image to file system /mnt/ntfs1\nsudo mount -o loop sdc4.img /mnt/ntfs1\nUnmount\nsudo umount /mnt/ntfs1/"},"Linux/Command-line-tool-for-Trash":{"title":"Command line tool for Trash in Ubuntu","links":[],"tags":["Linux"],"content":"Quellen\ngithub.com/andreafrancia/trash-cli\nTrash cli tools\nInstall trash-cli tools\nsudo apt install trash-cli\ntrash cli tools\n\ntrash-put: trash files and directories.\ntrash-empty: empty the trashcan(s).\ntrash-list: list trashed files.\ntrash-restore: restore a trashed file.\ntrash-rm: remove individual files from the trashcan.\n"},"Linux/Linux-Kommandos-zur-Audio-Video-Verwaltung":{"title":"Linux Kommandos zur Audio/Video Verwaltung","links":[],"tags":["Linux"],"content":"Extract MP3 from video (MP4)\nffmpeg -i myvideo.mp4 -codec:a libmp3lame -q:a QUALITY audio.mp3\nSpecify the QUALITY in the 0-9 range, where 0 is best, 9 is worst, and 4 is the default value. A table presenting each FFmpeg VBR option is available here .\nFFmpeg: Extract Audio From Video In Original Format Or Converting It To MP3 Or Ogg Vorbis"},"Linux/Linux-Kommandos-zur-Bilderverwaltung":{"title":"Linux Kommandos zur Bilderverwaltung","links":[],"tags":["Linux"],"content":"Identische Dateien finden\nfdupes --recurse --reverse  /srv/dev-disk-by-label-Disk01/Bilder\n… und löschen\nIm Set der Duplicate wird die erste behalten und alle anderen gelöscht.\nfdupes --recurse --reverse --delete --noprompt /srv/dev-disk-by-label-Disk01/Bilder\nBilder nach Datum YYYY-MM in Unterverzeichnisse sortieren\nexiftool -d %Y-%m &quot;-directory&lt;createdate&quot; *.jpg\nBilder aus allen Unterverzeichnissen in ein Verzeichnis kopieren\nfind ./*__ -type f -exec cp --backup=numbered -t ./tmp {} +"},"Linux/SNMP-client":{"title":"SNMP client","links":[],"tags":["Linux","SNMP"],"content":"SNMP client\nInstallation\n&gt; sudo apt install snmp\nsnmpget\npi@docker:~ $ snmpget -v2c -c geheim 192.168.178.4 SNMPv2-MIB::sysName.0\nSNMPv2-MIB::sysName.0 = STRING: NetgearWohnzimmer\npi@docker:~ $\nsnmptable\npi@docker:~ $ snmptable -v2c -c geheim 192.168.178.4 IF-MIB::ifXTable\nSNMP table: IF-MIB::ifXTable\n \nifName               ifHCInOctets ifHCOutOctets \nGigabitEthernet1     234679312    451727834\nGigabitEthernet2     62873258     176562137\nGigabitEthernet3     310072382    189007549\nGigabitEthernet4     0            0\nGigabitEthernet5     0            0\nGigabitEthernet6     0            0\nGigabitEthernet7     0            0\nGigabitEthernet8     0            0\npi@docker:~ $\n \nsnmpwalk\npi@docker:~ $ snmpwalk -v2c -c geheim 192.168.178.4 IF-MIB::ifXTable\nIF-MIB::ifName.1 = STRING: GigabitEthernet1\nIF-MIB::ifName.2 = STRING: GigabitEthernet2\nIF-MIB::ifName.3 = STRING: GigabitEthernet3\nIF-MIB::ifName.4 = STRING: GigabitEthernet4\nIF-MIB::ifName.5 = STRING: GigabitEthernet5\nIF-MIB::ifName.6 = STRING: GigabitEthernet6\nIF-MIB::ifName.7 = STRING: GigabitEthernet7\nIF-MIB::ifName.8 = STRING: GigabitEthernet8\nIF-MIB::ifName.1000 = STRING: LAG1\nIF-MIB::ifName.1001 = STRING: LAG2\nIF-MIB::ifName.1002 = STRING: LAG3\nIF-MIB::ifName.1003 = STRING: LAG4\nIF-MIB::ifName.1004 = STRING: LAG5\nIF-MIB::ifName.1005 = STRING: LAG6\nIF-MIB::ifName.1006 = STRING: LAG7\nIF-MIB::ifName.1007 = STRING: LAG8\nIF-MIB::ifName.10000 = STRING: CPU\nIF-MIB::ifInMulticastPkts.1 = Counter32: 260607\nIF-MIB::ifInMulticastPkts.2 = Counter32: 22014\nIF-MIB::ifInMulticastPkts.3 = Counter32: 26716\nIF-MIB::ifInMulticastPkts.4 = Counter32: 0\n..."},"Linux/Setup-password-less-SSH-login":{"title":"Setup password-less SSH login","links":[],"tags":["Linux"],"content":"Reference\n\n# How to Setup Passwordless SSH Login\n\nGenerate a new SSH key pair\nfleishor@desktop:~$ ssh-keygen -t rsa -b 4096\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/fleishor/.ssh/id_rsa): \n/home/fleishor/.ssh/id_rsa already exists.\nOverwrite (y/n)? y\nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in /home/fleishor/.ssh/id_rsa\nYour public key has been saved in /home/fleishor/.ssh/id_rsa.pub\nThe key fingerprint is:\nSHA256:tF6w34/VwiU0A6Zv0INQbKrjnd6JtkyrSREcYva2LJ4 fleishor@desktop\nThe key&#039;s randomart image is:\n+---[RSA 4096]----+\n|    + . .o. o    |\n|   o + . .o= .   |\n|      = oo+ o +  |\n|     o +.+ o o o |\n|    . +.S . o . .|\n|   . ooo o o . + |\n|    E..oo.. . + .|\n|     ..+++ . + . |\n|      o+*.o . .  |\n+----[SHA256]-----+\nfleishor@desktop:~$\n\nCopy the public key to remote machine\nfleishor@desktop:~$ ssh-copy-id pi@docker.fritz.box\n/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/home/fleishor/.ssh/id_rsa.pub&quot;\nThe authenticity of host &#039;docker.fritz.box (192.168.178.19)&#039; can&#039;t be established.\nED25519 key fingerprint is SHA256:/ozSko5DtpT/z7+JbkGwTSAcwUn2RJt3BsE26Cx7Ah8.\nThis key is not known by any other names\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\n/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed\n/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys\npi@docker.fritz.box&#039;s password: \n\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   &quot;ssh &#039;pi@docker.fritz.box&#039;&quot;\nand check to make sure that only the key(s) you wanted were added.\n\nfleishor@desktop:~$ \n\nCheck ssh to remote machine\nleishor@desktop:~$ ssh &#039;pi@docker.fritz.box&#039;\nLinux docker 5.10.103-v8+ #1529 SMP PREEMPT Tue Mar 8 12:26:46 GMT 2022 aarch64\n\nThe programs included with the Debian GNU/Linux system are free software;\nthe exact distribution terms for each program are described in the\nindividual files in /usr/share/doc/*/copyright.\n\nDebian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\npermitted by applicable law.\nLast login: Fri Nov  8 16:44:21 2024 from 192.168.178.44\n\nSSH is enabled and the default password for the &#039;pi&#039; user has not been changed.\nThis is a security risk - please login as the &#039;pi&#039; user and type &#039;passwd&#039; to set a new password.\n\n\nWi-Fi is currently blocked by rfkill.\nUse raspi-config to set the country before use.\n\npi@docker:~ $\n"},"Linux/Show-disk-usage-in-Linux":{"title":"Show disk usage in Linux","links":[],"tags":["Linux"],"content":"NCDU\nInstall ncdu\nsudo apt install ncdu\nUse ncdu\nfleishor@desktop:~$ ncdu\n\nNavigate through directories with CursorUp, CursorDown, Enter\n"},"Linux/iPerf3":{"title":"iPerf(3)","links":[],"tags":["Linux","FritzBox"],"content":"References\n\nSo testen Sie die Netzwerkbandbreite mit iPerf3\nFRITZ!Box: Durchsatz des WLAN mit iperf messen\nNetzwerk-Performance mit in Fritzbox integriertem Iperf testen\n\niperf3 als Server auf port 7575 starten\nfleishor@openmediavault:~ $ iperf3 -s -p 7575\n-----------------------------------------------------------\nServer listening on 7575 (test #1)\n-----------------------------------------------------------\n\niperf3 auf dem Client laufen lassen\npi@docker:~ $ iperf3 -c openmediavault.fritz.box -p 7575\nConnecting to host openmediavault.fritz.box, port 7575\n[  5] local 192.168.178.19 port 48214 connected to 192.168.178.20 port 7575\n[ ID] Interval           Transfer     Bitrate         Retr  Cwnd\n[  5]   0.00-1.00   sec   109 MBytes   918 Mbits/sec    0    419 KBytes\n[  5]   1.00-2.00   sec   111 MBytes   930 Mbits/sec    0    419 KBytes\n[  5]   2.00-3.00   sec   112 MBytes   937 Mbits/sec    0    468 KBytes\n[  5]   3.00-4.00   sec   111 MBytes   931 Mbits/sec    0    520 KBytes\n[  5]   4.00-5.00   sec   111 MBytes   929 Mbits/sec    0    520 KBytes\n[  5]   5.00-6.00   sec   112 MBytes   939 Mbits/sec    0    710 KBytes\n[  5]   6.00-7.00   sec   108 MBytes   902 Mbits/sec  210    510 KBytes\n[  5]   7.00-8.00   sec   111 MBytes   933 Mbits/sec   70    414 KBytes\n[  5]   8.00-9.00   sec   110 MBytes   923 Mbits/sec    0    450 KBytes\n[  5]   9.00-10.00  sec   112 MBytes   944 Mbits/sec    0    450 KBytes\n- - - - - - - - - - - - - - - - - - - - - - - - -\n[ ID] Interval           Transfer     Bitrate         Retr\n[  5]   0.00-10.00  sec  1.08 GBytes   929 Mbits/sec  280             sender\n[  5]   0.00-10.01  sec  1.08 GBytes   925 Mbits/sec                  receiver\n\niperf Done.\npi@docker:~ $\n\niperf auf der Fritzbox aktivieren\nAufpassen, iperf(!) nicht iperf3\nSupport page aufrufen: fritz.box/#support\n\niperf auf dem Client laufen lassen\npi@docker:~ $ iperf -c router.fritz.box -p 4711\n------------------------------------------------------------\nClient connecting to router.fritz.box, TCP port 4711\nTCP window size:  187 KByte (default)\n------------------------------------------------------------\n[  3] local 192.168.178.19 port 40576 connected with 192.168.178.1 port 4711\n[ ID] Interval       Transfer     Bandwidth\n[  3]  0.0-10.0 sec  95.2 MBytes  79.9 Mbits/sec\n\npi@docker:~ $ iperf -c 192.168.178.2 -p 4711\n------------------------------------------------------------\nClient connecting to 192.168.178.2, TCP port 4711\nTCP window size:  212 KByte (default)\n------------------------------------------------------------\n[  3] local 192.168.178.19 port 37968 connected with 192.168.178.2 port 4711\n[ ID] Interval       Transfer     Bandwidth\n[  3]  0.0-10.0 sec   111 MBytes  92.5 Mbits/sec\npi@docker:~ $\n"},"Obsidian/Important-commands-in-Quartz4":{"title":"Important commands in Quartz4","links":[],"tags":["Obsidian"],"content":"Important commands in Quartz4\n\nnpx quartz sync --no-pull: Sync Obsidian with Github\nnpx quartz update: Update Quartz\nnpx quartz build --serve: Build Static Website from Obsidian and start Web server\n"},"SmartHome/Docker/Add-SNMP-to-Telegraf":{"title":"Send SNMP statistics via Telegraf to InfluxDB","links":[],"tags":["Telegraf","SNMP"],"content":"References\n\nGrafana: Monitor SNMP devices with Telegraf and InfluxDB\n\nKonfiguration\nBucket Docker\n[[outputs.influxdb_v2]]\n  urls = [&quot;http://influxdb2:8086&quot;]\n  token = &quot;***&quot;\n  organization = &quot;fleishor&quot;\n  bucket = &quot;Docker&quot;\n  namedrop = [&quot;NetgearWohnzimmer&quot;, &quot;NetgearKeller&quot;, &quot;Buero&quot;, &quot;Router&quot;]\n\nBy default all Telegraf statistics are sent to bucket Docker, except name (measurements) NetgearWohnzimmer, NetgearKeller, Buero, Router\n\nBucket Fritzbox\n[[outputs.influxdb_v2]]\n  urls = [&quot;http://influxdb2:8086&quot;]\n  token = &quot;***&quot;\n  organization = &quot;fleishor&quot;\n  bucket = &quot;Fritzbox&quot;\n  namepass = [&quot;Buero&quot;, &quot;Router&quot;]\n\nAll measurements for Buero and Router are written to bucket Fritzbox\n\nBucket Snmp\n[[outputs.influxdb_v2]]\n  urls = [&quot;http://influxdb2:8086&quot;]\n  token = &quot;***&quot;\n  organization = &quot;fleishor&quot;\n  bucket = &quot;Snmp&quot;\n  namepass = [&quot;NetgearWohnzimmer&quot;,&quot;NetgearKeller&quot;]\n\n\ntelegraf.conf\n# Telegraf Configuration\n#\n# Global tags can be specified here in key=&quot;value&quot; format.\n[global_tags]\n\n# Configuration for telegraf agent\n[agent]\n  interval = &quot;60s&quot;\n  round_interval = true\n  metric_batch_size = 1000\n  metric_buffer_limit = 10000\n  collection_jitter = &quot;0s&quot;\n  flush_interval = &quot;60s&quot;\n  flush_jitter = &quot;0s&quot;\n  precision = &quot;&quot;\n  hostname = &quot;docker.fritz.box&quot;\n  omit_hostname = false\n\n[[outputs.influxdb_v2]]\n  urls = [&quot;http://influxdb2:8086&quot;]\n  token = &quot;***&quot;\n  organization = &quot;fleishor&quot;\n  bucket = &quot;Docker&quot;\n  namedrop = [&quot;NetgearWohnzimmer&quot;, &quot;NetgearKeller&quot;, &quot;Buero&quot;, &quot;Router&quot;]\n\n[[inputs.cpu]]\n  percpu = true\n  totalcpu = true\n  collect_cpu_time = false\n  report_active = false\n\n[[inputs.disk]]\n  ignore_fs = [&quot;tmpfs&quot;, &quot;devtmpfs&quot;, &quot;devfs&quot;, &quot;iso9660&quot;, &quot;overlay&quot;, &quot;aufs&quot;, &quot;squashfs&quot;]\n\n[[inputs.diskio]]\n\n[[inputs.kernel]]\n\n[[inputs.mem]]\n\n[[inputs.processes]]\n\n[[inputs.swap]]\n\n[[inputs.system]]\n\n[[outputs.influxdb_v2]]\n  urls = [&quot;http://influxdb2:8086&quot;]\n  token = &quot;***&quot;\n  organization = &quot;fleishor&quot;\n  bucket = &quot;Fritzbox&quot;\n  namepass = [&quot;Buero&quot;, &quot;Router&quot;]\n\n[[inputs.socket_listener]]\n  service_address = &quot;tcp://:8094&quot;\n  data_format = &quot;influx&quot;\n\n[[inputs.file]]\n  files = [&quot;/sys/class/thermal/thermal_zone0/temp&quot;]\n  name_override = &quot;cpu_temperature&quot;\n  data_format = &quot;value&quot;\n  data_type = &quot;integer&quot;\n\n[[outputs.influxdb_v2]]\n  urls = [&quot;http://influxdb2:8086&quot;]\n  token = &quot;***&quot;\n  organization = &quot;fleishor&quot;\n  bucket = &quot;Snmp&quot;\n  namepass = [&quot;NetgearWohnzimmer&quot;,&quot;NetgearKeller&quot;]\n\n[[inputs.snmp]]\n  agents = [ &quot;netgearwohnzimmer.fritz.box:161&quot; ]\n  version = 2\n  community = &quot;geheim&quot;\n  name = &quot;NetgearWohnzimmer&quot;\n  name_override = &quot;NetgearWohnzimmer&quot;\n\n [[inputs.snmp.field]]\n    name = &quot;hostname&quot;\n    oid = &quot;SNMPv2-MIB::sysName.0&quot;\n    is_tag = true\n\n  [[inputs.snmp.table]]\n    name = &quot;snmp&quot;\n    inherit_tags = [ &quot;hostname&quot; ]\n    oid = &quot;IF-MIB::ifXTable&quot;\n\n    [[inputs.snmp.table.field]]\n      name = &quot;ifName&quot;\n      oid = &quot;IF-MIB::ifName&quot;\n      is_tag = true\n\n[[inputs.snmp]]\n  agents = [ &quot;netgearkeller.fritz.box:161&quot; ]\n  version = 2\n  community = &quot;geheim&quot;\n  name = &quot;NetgearKeller&quot;\n  name_override = &quot;NetgearKeller&quot;\n\n [[inputs.snmp.field]]\n    name = &quot;hostname&quot;\n    oid = &quot;SNMPv2-MIB::sysName.0&quot;\n    is_tag = true\n\n  [[inputs.snmp.table]]\n    name = &quot;snmp&quot;\n    inherit_tags = [ &quot;hostname&quot; ]\n    oid = &quot;IF-MIB::ifXTable&quot;\n\n    [[inputs.snmp.table.field]]\n      name = &quot;ifName&quot;\n      oid = &quot;IF-MIB::ifName&quot;\n      is_tag = true\n"},"SmartHome/Docker/Fritzbox2Telegraf":{"title":"Forward all AVM statistics to Telegraf and to InfluxDB2","links":[],"tags":["Docker","Telegraf","FritzBox","SmartHome"],"content":"References\n\ngithub.com/Schmidsfeld/TelegrafFritzBox\ngithub.com/kbr/fritzconnection\nfritzconnection.readthedocs.io/en/1.13.2/\navm.de/service/schnittstellen/\n\nCreate new user fritzbox\nsudo useradd -m fritzbox\nAdd user fritzbox to docker group\nsudo usermod -aG docker fritzbox\nLogin as user fritzbox\nsudo -u fritzbox -i\nGet uid and gid for user fritzbox\nfritzbox@docker:~ $ id\nuid=1005(fritzbox) gid=1005(fritzbox) groups=1005(fritzbox),995(docker)\nInstall required packages\napt install python3-pip\npip3 install fritzconnection\ncrontab\n\n*/5 * * * * /home/fritzbox/telegrafFritzBox.sh\ntelegrafFritzBox.sh\n\n#!/bin/sh\nexport PATH=${PATH}:/home/fritzbox/.local/bin\npython3 /home/fritzbox/telegrafFritzBox.4040.py -u ${BUERO_USERNAME} -p ${BUERO_PASSWORD} -i ${BUERO_ADDRESS} | nc -q 1 ${TELEGRAF_HOSTNAME} ${TELEGRAF_PORT}\npython3 /home/fritzbox/telegrafFritzBox.7490.py -u ${ROUTER_USERNAME} -p ${ROUTER_PASSWORD} -i ${ROUTER_ADDRESS} | nc -q 1 ${TELEGRAF_HOSTNAME} ${TELEGRAF_PORT}\npython3 /home/fritzbox/telegrafFritzBox.SmartHome.py -u ${ROUTER_USERNAME} -p ${ROUTER_PASSWORD} -i ${ROUTER_ADDRESS} | nc -q 1 ${TELEGRAF_HOSTNAME} ${TELEGRAF_PORT}\nDockerfile\n\nFROM alpine:latest\n \n#install python3\nRUN apk add python3 py3-pip netcat-openbsd sudo busybox-suid\n \nRUN addgroup -g 995 -S fritzbox &amp;&amp; \\\n    adduser -h &quot;/home/fritzbox&quot; -S -G fritzbox -u 1005 fritzbox &amp;&amp; \\\n    echo &quot;fritzbox ALL=(ALL) NOPASSWD: ALL&quot; &gt; /etc/sudoers.d/fritzbox \\\n        &amp;&amp; chmod 0440 /etc/sudoers.d/fritzbox\n \nUSER fritzbox\n \nWORKDIR /home/fritzbox\n \nRUN pip3 install fritzconnection ping3 --break-system-packages\nCOPY --chown=fritzbox:fritzbox ./telegrafFritzBox.4040.py .\nCOPY --chown=fritzbox:fritzbox ./telegrafFritzBox.7490.py .\nCOPY --chown=fritzbox:fritzbox ./telegrafFritzBox.SmartHome.py .\nCOPY --chown=fritzbox:fritzbox ./telegrafFritzBox.sh .\nRUN chmod +x ./telegrafFritzBox.sh\n \n# Configure cron\nCOPY crontab /etc/cron/crontab\n \n# Init cron\nRUN crontab /etc/cron/crontab\n \nCMD [&quot;sudo&quot;, &quot;-E&quot;, &quot;crond&quot;, &quot;-f&quot;]\nBuild docker image\ndocker image build -t fritzbox:20240621 .\ndocker-compose.yaml file\n\nversion: &quot;3.5&quot;\n \nservices:\n  fritzbox:\n    image: fritzbox:20240621\n    container_name: fritzbox\n    user: 1005:995\n    environment:\n      - ROUTER_ADDRESS=192.168.178.1\n      - ROUTER_USERNAME=telegraf\n      - ROUTER_PASSWORD=${ROUTER_PASSWORD}\n      - BUERO_ADDRESS=192.168.178.2\n      - BUERO_USERNAME=telegraf\n      - BUERO_PASSWORD=${BUERO_PASSWORD}\n      - TELEGRAF_HOSTNAME=telegraf\n      - TELEGRAF_PORT=8094\n    restart: always\n    networks:\n        - smarthome\n        \nnetworks:\n    smarthome:\n        external: true\n        name: &quot;smarthome&quot;\nConfiguration of Telegraf\nWith namedrop and namepass it’s possible to redirect messages to different buckets.\n...\n[[outputs.influxdb_v2]]\n  urls = [&quot;http://influxdb2:8086&quot;]\n  token = &quot;***&quot;\n  organization = &quot;fleishor&quot;\n  bucket = &quot;Docker&quot;\n  namedrop = [&quot;NetgearWohnzimmer&quot;, &quot;NetgearKeller&quot;, &quot;Buero&quot;, &quot;Router&quot;]\n\n...\n\n[[outputs.influxdb_v2]]\n  urls = [&quot;http://influxdb2:8086&quot;]\n  token = &quot;***&quot;\n  organization = &quot;fleishor&quot;\n  bucket = &quot;Fritzbox&quot;\n  namepass = [&quot;Buero&quot;, &quot;Router&quot;]\n\n[[inputs.socket_listener]]\n  service_address = &quot;tcp://:8094&quot;\n  data_format = &quot;influx&quot;\n"},"SmartHome/Docker/Fritzconnect":{"title":"Command line tools of fritzconnect","links":[],"tags":["Linux","FritzBox"],"content":"Fritzconnect\nReferences\n\navm.de/service/schnittstellen/\nfritzconnection.readthedocs.io/en/1.13.2/\ngithub.com/kbr/fritzconnection\ngithub.com/Schmidsfeld/TelegrafFritzBox\n\nInstallation\nfritzbox@docker:~$ pip3 install fritzconnection\nService discovery\nfritzbox@docker:~ $ fritzconnection -s -i 192.168.178.1\n\nfritzconnection v1.13.2\nFRITZ!Box 7490 at http://192.168.178.1\nFRITZ!OS: 7.57\n\nServicenames:\n                    any1\n                    WANCommonIFC1\n                    WANDSLLinkC1\n                    WANIPConn1\n                    WANIPv6Firewall1\n                    DeviceInfo1\n                    DeviceConfig1\n                    Layer3Forwarding1\n                    LANConfigSecurity1\n                    ManagementServer1\n                    Time1\n                    UserInterface1\n                    X_AVM-DE_Storage1\n                    X_AVM-DE_WebDAVClient1\n                    X_AVM-DE_UPnP1\n                    X_AVM-DE_Speedtest1\n                    X_AVM-DE_RemoteAccess1\n                    X_AVM-DE_MyFritz1\n                    X_VoIP1\n                    X_AVM-DE_OnTel1\n                    X_AVM-DE_Dect1\n                    X_AVM-DE_TAM1\n                    X_AVM-DE_AppSetup1\n                    X_AVM-DE_Homeauto1\n                    X_AVM-DE_Homeplug1\n                    X_AVM-DE_Filelinks1\n                    X_AVM-DE_Auth1\n                    X_AVM-DE_HostFilter1\n                    X_AVM-DE_USPController1\n                    WLANConfiguration1\n                    WLANConfiguration2\n                    WLANConfiguration3\n                    Hosts1\n                    LANEthernetInterfaceConfig1\n                    LANHostConfigManagement1\n                    WANCommonInterfaceConfig1\n                    WANDSLInterfaceConfig1\n                    X_AVM-DE_WANMobileConnection1\n                    WANDSLLinkConfig1\n                    WANEthernetLinkConfig1\n                    WANPPPConnection1\n                    WANIPConnection1\n\nfritzbox@docker:~ $\n\nGet list of actions offered by a service\nfritzbox@docker:~ $ fritzconnection -i 192.168.178.1 -S LANEthernetInterfaceConfig1\n\nfritzconnection v1.13.2\nFRITZ!Box 7490 at http://192.168.178.1\nFRITZ!OS: 7.57\n\n\nServicename:        LANEthernetInterfaceConfig1\nActionnames:\n                    SetEnable\n                    GetInfo\n                    GetStatistics\n\nfritzbox@docker:~ $\n\nInspect Action\nfritzbox@docker:~ $ fritzconnection -i 192.168.178.1 -A LANEthernetInterfaceConfig1 GetInfo\n\nfritzconnection v1.13.2\nFRITZ!Box 7490 at http://192.168.178.1\nFRITZ!OS: 7.57\n\n\nService:            LANEthernetInterfaceConfig1\nAction:             GetInfo\nParameters:\n\n    Name                                  direction     data type\n\n    NewEnable                                out -&gt;     boolean\n    NewStatus                                out -&gt;     string\n    NewMACAddress                            out -&gt;     string\n    NewMaxBitRate                            out -&gt;     string\n    NewDuplexMode                            out -&gt;     string\n\nfritzbox@docker:~ $\n\nTool fritzstatus\nfritzbox@docker:~ $ fritzstatus -i 192.168.178.1\n\nfritzconnection v1.13.2\nFRITZ!Box 7490 at http://192.168.178.1\nFRITZ!OS: 7.57\n\nFritzStatus:\n\n    is linked             : True\n    is connected          : True\n    external ip (v4)      : 109.199.164.81\n    external ip (v6)      : ::\n    internal ipv6-prefix  : ::\n    uptime                : 15:56:10\n    bytes send            : 53531425479\n    bytes received        : 626125259214\n    max. bit rate         : (&#039;5.5 MBit/s&#039;, &#039;33.0 MBit/s&#039;)\n\nfritzbox@docker:~ $\n\nTool fritzwlan\nfritzbox@docker:~/.local/bin $ fritzwlan -i 192.168.178.1 -u {user} -p {password}\n\nfritzconnection v1.13.2\nFRITZ!Box 7490 at http://192.168.178.1\nFRITZ!OS: 7.57\n\nHosts registered at WLANConfiguration1:\nWLAN name: router.fleishor\nchannel  : 1\nindex  active                 mac                ip  signal   speed\n    0       1   38:37:8B:A2:28:DD    192.168.178.57      55      70\n    1       1   C8:C2:FA:9B:10:0C    192.168.178.34      51      72\n    2       1   00:22:61:FC:A2:D2    192.168.178.45      32      60\n    3       1   4C:79:75:1C:36:D9    192.168.178.35      55     142\n    4       1   48:55:19:C9:0E:B7    192.168.178.65      29      19\n    5       1   B8:8C:29:CC:B8:46    192.168.178.51      83      72\n\nHosts registered at WLANConfiguration2:\nWLAN name: router.fleishor\nchannel  : 36\nindex  active                 mac                ip  signal   speed\n    0       1   4C:6B:E8:D5:71:23    192.168.178.42      32     263\n\nfritzbox@docker:~/.local/bin $\n\nTool fritzhosts\nfritzbox@docker:~/.local/bin $ fritzhosts -i 192.168.178.1 -u {user} -p {password}\n\nfritzconnection v1.13.2\nFRITZ!Box 7490 at http://192.168.178.1\nFRITZ!OS: 7.57\n\nFritzHosts:\nList of registered hosts:\n\n  n: ip               name                         mac                 status\n\n  1: 192.168.178.41   AVM1220-b82f993              98:9B:CB:82:F9:93   active\n  2: 192.168.178.40   AVM1220Heizungskeller        98:9B:CB:82:F9:34   active\n...\n  7: 192.168.178.19   Docker                       E4:5F:01:20:16:A5   active\n  8: 192.168.178.52   Grundig                      00:11:E1:4F:FF:8D   -\n  9: 192.168.178.34   HUAWEI-Mate-20-lite-82a27    C8:C2:FA:9B:10:0C   active\n...\n 12: 192.168.178.57   HuaweiP10                    38:37:8B:A2:28:DD   active\n...\n 24: 192.168.178.44   desktop                      F4:4D:30:01:23:06   -\n...\n 30: 192.168.178.20   openmediavault               DC:A6:32:B7:D4:7A   -\n\n\nfritzbox@docker:~/.local/bin $\n\nTool fritzhomeauto\nfritzbox@docker:~/.local/bin $ fritzhomeauto -i 192.168.178.1 -u {user} -p {password}\n\nfritzconnection v1.13.2\nFRITZ!Box 7490 at http://192.168.178.1\nFRITZ!OS: 7.57\n\nFritzHomeautomation:\nStatus of registered home-automation devices:\n\nDevice Name             AIN                 Power[W]   t[°C]   switch\niPhone2020              &#039;xxx&#039;                  0.000     0.0   off\nOpenMediaVault          &#039;xxx&#039;                  0.000    24.0   off\nBuero                   &#039;xxx&#039;                107.350    20.0   on\nGefriertruhe            &#039;xxx&#039;                  1.390    20.0   on\nKueche                  &#039;xxx&#039;                  0.330    23.5   on\n"},"SmartHome/Docker/Installations/Grafana-with-Docker":{"title":"Grafana with Docker","links":[],"tags":["Docker","Grafana","SmartHome"],"content":"Create new user grafana\nsudo useradd -m grafana\nAdd user grafana to docker group\nsudo usermod -aG docker grafana\nLogin as user grafana\nsudo -u grafana -i\nGet uid and gid for user grafana\ngrafana@docker:~ $ id\nuid=1004(grafana) gid=1004(grafana) groups=1004(grafana),995(docker)\nCreate directories for Grafana\nmkdir var_lib\ndocker-compose.yaml file\nversion: &quot;3.5&quot;\n \nservices:\n  grafana8:\n    image: grafana/grafana:latest\n    container_name: grafana\n    user: 1004:995\n    volumes:\n      - /home/grafana/var_lib:/var/lib/grafana:rw\n    ports:\n      - &quot;3000:3000&quot;\n    restart: always\n    networks:\n        - influxdb2\n        \nnetworks:\n    influxdb2:\n        external: true\n        name: &quot;influxdb2&quot;"},"SmartHome/Docker/Installations/InfluxDB2-with-Docker":{"title":"InfluxDB 2.x with Docker","links":[],"tags":["Docker","InfluxDB","SmartHome"],"content":"Create new user influxdb2\nsudo useradd -m influxdb2\nAdd user influxdb2 to docker group\nsudo usermod -aG docker influxdb2\nLogin as user influxdb2\nsudo -u influxdb2 -i\nGet uid and gid for user influxdb2\ninfluxdb2@docker:~ $ id\nuid=1002(influxdb2) gid=1002(influxdb2) groups=1002(influxdb2),995(docker)\nCreate directories for influxdb2\ninfluxdb2@docker:~ $ mkdir etc_influxdb2\ninfluxdb2@docker:~ $ mkdir var_lib_influxdb2\n \ndrwxr-xr-x 2 influxdb2 influxdb2 4096 Jul  6 08:08 etc_influxdb2\ndrwxr-xr-x 2 influxdb2 influxdb2 4096 Jul  6 08:08 var_lib_influxdb2\ndocker-compose.yaml file\n\nwith long syntax for volumes\ncreate a new bridge network for influxdb2 and related containers\n\nversion: &quot;3.5&quot;\n \nservices:\n  influxdb2:\n    image: influxdb:latest\n    container_name: influxdb2\n    ports:\n      - &quot;8086:8086&quot;\n    volumes:\n      - type: bind\n        source: /home/influxdb2/var_lib_influxdb2\n        target: /var/lib/influxdb2\n      - type: bind\n        source: /home/influxdb2/etc_influxdb2\n        target: /etc/influxdb2\n    environment:\n      - DOCKER_INFLUXDB_INIT_MODE=setup\n      - DOCKER_INFLUXDB_INIT_USERNAME=fleishor\n      - DOCKER_INFLUXDB_INIT_PASSWORD=***\n      - DOCKER_INFLUXDB_INIT_ORG=fleishor\n      - DOCKER_INFLUXDB_INIT_BUCKET=fleishor\n      - DOCKER_INFLUXDB_INIT_RETENTION=90d\n      - DOCKER_INFLUXDB_INIT_ADMIN_TOKEN=***\n    restart: always\n    networks:\n      - influxdb2\n \nnetworks:\n  influxdb2:\n    name: &quot;influxdb2&quot;\n    driver: &quot;bridge&quot;"},"SmartHome/Docker/Installations/Keycloak-with-Docker":{"title":"Keycloak with Docker","links":[],"tags":["Docker","Keycloak"],"content":"References\n\nGet started with Keycloak on Docker\nDurchstarten mit Keycloak und Docker\n\nCreate new user keycloak\nsudo useradd -m keycloak\nAdd user keycloak to docker group\nsudo usermod -aG docker keycloak\nLogin as user keycloak\nsudo -u keycloak -i\nGet uid and gid for user keycloak\ngrafana@docker:~ $ id\nuid=1019(keycloak) gid=1019(keycloak) groups=1019(keycloak),995(docker)\nCreate separate network keycloak\n \nInstall MariaDB as persistence layer\ndocker-compose.yaml file\nservices:\n   mariadb:\n      container_name: keycloak_mariadb\n      image: mariadb:latest\n      ports:\n         - 3306:3306\n      environment:\n         - MARIADB_DATABASE=keycloak\n         - MARIADB_ROOT_PASSWORD=mariadb\n         - MARIADB_USER=keycloak\n         - MARIADB_PASSWORD=keycloak\n      volumes:\n         - /home/keycloak/var_lib_mysql:/var/lib/mysql:rw\n      restart: always\n      networks:\n         - keycloak\n \n   keycloak:\n      container_name: keycloak_server\n      image: quay.io/keycloak/keycloak:latest\n      environment:\n         KEYCLOAK_ADMIN: admin\n         KEYCLOAK_ADMIN_PASSWORD: admin\n         KC_DB: mariadb\n         KC_DB_URL_DATABASE: keycloak\n         KC_DB_URL: jdbc:mariadb://mariadb/keycloak\n         KC_DB_USERNAME: keycloak\n         KC_DB_PASSWORD: keycloak\n         KC_HTTP_RELATIVE_PATH: /auth\n      command: [&#039;start-dev&#039;]\n      ports:\n         - &quot;8081:8080&quot; # Externe-Portnummer:Interne-Portnummer\n      restart: always\n      depends_on:\n         mariadb:\n            condition: service_started\n      networks:\n         - keycloak\n \nnetworks:\n    keycloak:\n        external: true\n        name: &quot;keycloak&quot;"},"SmartHome/Docker/Installations/Loki-with-Docker":{"title":"Loki with Docker","links":[],"tags":["Docker","Loki","SmartHome"],"content":"Create new user loki\nsudo useradd -m loki\nAdd user loki to docker group\nsudo usermod -aG docker loki\nLogin as user loki\nsudo -u loki -i\nGet uid and gid for user loki\nloki@docker:~ $ id\nuid=1007(loki) gid=1007(loki) groups=1007(loki),995(docker)\nCreate directories for loki\nmkdir etc_loki\ndocker-compose.yaml file\nversion: &quot;3.5&quot;\n \nservices:\nloki:\n   image: grafana/loki:latest\n    pid: &quot;host&quot;\n    container_name: loki\n    user: 1007:995\n   volumes:\n     - ./etc_loki/loki.yaml:/etc/loki/loki.yaml\n   entrypoint:\n     - /usr/bin/loki\n     - -config.file=/etc/loki/loki.yaml\n   ports:\n     - &quot;3100:3100&quot;\n    restart: always\n    networks:\n      - influxdb2\n \nnetworks:\n  influxdb2:\n    external: true\n    name: &quot;influxdb2&quot;"},"SmartHome/Docker/Installations/Mosquitto-with-Docker":{"title":"Mosquitto with Docker","links":[],"tags":["Docker","MQTT","SmartHome"],"content":"Create new user mosquitto\nsudo useradd -m mosquitto\nAdd user mosquitto to docker group\nsudo usermod -aG docker mosquitto\nLogin as user mosquitto\nsudo -u mosquitto -i\nGet uid and gid for user mosquitto\nmosquitto@docker:~ $ id\nuid=1013(mosquitto) gid=1013(mosquitto) groups=1013(mosquitto),995(docker)\nCreate directories for Mosquitto\nmkdir mosquitto\ndocker-compose.yaml file\nversion: &quot;3.5&quot;\n \nservices:\n  mosquitto:\n    image: eclipse-mosquitto:2.0.15\n    container_name: mosquitto\n    user: 1013:995\n    volumes:\n      - /home/mosquitto/mosquitto/config:/mosquitto/config:rw\n      - /home/mosquitto/mosquitto/log:/mosquitto/log:rw\n      - /home/mosquitto/mosquitto/data:/mosquitto/data:rw\n    ports:\n      - &quot;1883:1883&quot;\n      - &quot;9001:9001&quot;\n    restart: always\n    networks:\n        - influxdb2\n \nnetworks:\n    influxdb2:\n        external: true\n        name: &quot;influxdb2&quot;\nmosquitto.conf\nallow_anonymous true\nlistener 1883\npersistence true\npersistence_location /mosquitto/data/\nlog_dest file /mosquitto/log/mosquitto.log\n"},"SmartHome/Docker/Installations/NodeRed-with-Docker":{"title":"NodeRed with Docker","links":[],"tags":["Docker","NodeRed","SmartHome"],"content":"Create new user nodered\nsudo useradd -m nodered\nAdd user nodered to docker group\nsudo usermod -aG docker nodered\nLogin as user nodered\nsudo -u nodered -i\nGet uid and gid for user nodered\nnodered@docker:~ $ id\nuid=1006(nodered) gid=1006(nodered) groups=1006(nodered),995(docker)\nCreate directories for nodered\nmkdir nodered_data\ndocker-compose.yaml file\nversion: &#039;3.5&#039;\n \nservices:\n  nodered:\n    image: nodered/node-red:latest\n    container_name: nodered\n    environment:\n      - TZ=Europe/Berlin\n    user: 1006:995\n    restart: always\n    ports:\n      - 1880:1880\n    volumes:\n      - /home/nodered/nodered_data:/data\n    networks:\n        - influxdb2\n        \nnetworks:\n    influxdb2:\n        external: true\n        name: &quot;influxdb2&quot;"},"SmartHome/Docker/Installations/Portainer-with-Docker":{"title":"Installation of Portainer","links":[],"tags":["Docker","Portainer","SmartHome"],"content":"Create new user portainer\nsudo useradd -m portainer\nAdd user portainer to docker group\nsudo usermod -aG docker portainer\nLogin as user portainer\nsudo -u portainer -i\nGet uid and gid for user portainer\nportainer@docker:~ $ id\nuid=1001(portainer) gid=1001(portainer) groups=1001(portainer),995(docker)\nCreate directories for portainer\nmkdir portainer_data\ndocker-compose.yaml file\nversion: &#039;3.5&#039;\n \nservices:\n  portainer:\n    image: portainer/portainer-ce:latest\n    container_name: portainer\n    user: 1001:995\n    restart: always\n    ports:\n      - 9000:9000\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - /home/portainer/portainer_data:/data\nUpdate portainer\ndocker stop portainer\n \ndocker pull portainer/portainer-ce"},"SmartHome/Docker/Installations/Promtail-with-Docker":{"title":"Promtail with Docker","links":[],"tags":["Docker","Promtail","SmartHome"],"content":"Create new user promtail\nsudo useradd -m promtail\nAdd user promtail to docker group\nsudo usermod -aG docker promtail\nLogin as user promtail\nsudo -u promtail -i\nCreate directories for promtail\nmkdir etc_promtail\ndocker-compose.yaml file\nversion: &quot;3.5&quot;\n \nservices:\n   promtail:\n      image: grafana/promtail:latest\n      pid: &quot;host&quot;\n      container_name: promtail\n      command: -config.file=/etc/promtail/promtail.yaml\n      volumes:\n         - /run/log/journal:/run/log/journal:ro\n         - /var/lib/docker/containers:/var/lib/docker/containers:ro\n         - ./etc_promtail/promtail.yaml:/etc/promtail/promtail.yaml\n      restart: always\n      networks:\n         - influxdb2\n \nnetworks:\n  influxdb2:\n    external: true\n    name: &quot;influxdb2&quot;\nGather journald logs and Docker container logs\n\n/etc/docker/daemon.json\ndocs.docker.com/config/containers/logging/log_tags/\nAdd tag log option so ImageName and Name will be written to Docker log files;\nnew tag flag will be only add after containers are recreated\n{\n  &quot;log-driver&quot;: &quot;json-file&quot;,\n  &quot;log-opts&quot;: {\n    &quot;max-size&quot;: &quot;10m&quot;,\n    &quot;max-file&quot;: &quot;3&quot;,\n    &quot;tag&quot;: &quot;{{.ImageName}}|{{.Name}}&quot;\n  }\n}\nExample log line\n{\n   &quot;log&quot;:&quot;t=2021-12-30T10:38:14+0000 lvl=info msg=\\&quot;Successful Login\\&quot; logger=http.server User=admin@localhost\\n&quot;,\n   &quot;stream&quot;:&quot;stdout&quot;,\n   &quot;attrs&quot;:\n   {\n      &quot;tag&quot;:&quot;grafana/grafana:latest|grafana&quot;\n   },\n   &quot;time&quot;:&quot;2021-12-30T10:38:14.566975699Z&quot;\n}\n/etc/promtail/promtail.yaml\nPromtail configuration for sending\n\nsystemd journal messages (job_name: journal) and\ncontainer logs (job_name: container) to Loki\n\nPath /run/log/journal and /var/lib/docker/containers/*/*log from host system\nmust be also mounted to docker container (see docker-compose.yaml)\nserver:\n  http_listen_address: 0.0.0.0\n  http_listen_port: 9080\n \npositions:\n  filename: /tmp/positions.yaml\n \nclients:\n  - url: http://loki:3100/loki/api/v1/push\n \nscrape_configs:\n- job_name: journal\n  journal:\n    json: false\n    max_age: 12h\n    path: /run/log/journal\n    labels:\n      job: systemd-journal\n      host: docker.fritz.box\n  relabel_configs:\n    - source_labels: [&#039;__journal__systemd_unit&#039;]\n      target_label: &#039;unit&#039;\n- job_name: containers\n  static_configs:\n  - targets:\n      - localhost\n    labels:\n      job: containerlogs\n      host: docker.fritz.box\n      __path__: /var/lib/docker/containers/*/*log\n \n  pipeline_stages:\n  - json:\n      expressions:\n        output: log\n        stream: stream\n        attrs:\n        log:\n  - json:\n      expressions:\n        tag:\n      source: attrs\n  - regex:\n      expression: (?P&lt;image_name&gt;(?:[^|]*[^|])).(?P&lt;container_name&gt;(?:[^|]*[^|]))\n      source: tag\n  - regex:\n      expression: &#039;.*lvl=(?P&lt;level&gt;[a-zA-Z]+).*&#039;\n      source: log\n  - regex:\n      expression: &#039;.*level=(?P&lt;level&gt;[a-zA-Z]+).*&#039;\n      source: log\n  - timestamp:\n      format: RFC3339Nano\n      source: time\n  - labels:\n      stream:\n      image_name:\n      container_name:\n      level:\n  - output:\n      source: output\nPipeline stages\n"},"SmartHome/Docker/Installations/Telegraf-with-Docker":{"title":"Telegraf with Docker","links":[],"tags":["Docker","Telegraf","SmartHome"],"content":"Create new user telegraf\nsudo useradd -m telegraf\nAdd user telegraf to docker group\nsudo usermod -aG docker telegraf\nLogin as user telegraf\nsudo -u telegraf -i\nGet uid and gid for user telegraf\ntelegraf@docker:~ $ id\nuid=1003(telegraf) gid=1003(telegraf) groups=1003(telegraf),995(docker)\nCreate directories for telegraf\nmkdir etc_telegraf\ndocker-compose.yaml\nversion: &quot;3.5&quot;\n \nservices:\n  telegraf:\n    image: telegraf:latest\n    pid: &quot;host&quot;\n    container_name: telegraf\n    user: 1003:995\n    ports:\n      - &quot;8094:8094&quot;\n      - &quot;8125:8125&quot;\n    volumes:\n      - /home/telegraf/etc_telegraf/telegraf.conf:/etc/telegraf/telegraf.conf:ro\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n      - /sys:/host/sys:ro\n      - /proc:/host/proc:ro\n      - /etc:/host/etc:ro\n    environment:\n      - HOST_PROC=/host/proc\n      - HOST_SYS=/host/sys\n      - HOST_ETC=/host/etc\n    restart: always\n    networks:\n      - influxdb2\n \nnetworks:\n  influxdb2:\n    external: true\n    name: &quot;influxdb2&quot;\ntelegraf.conf\n# Telegraf Configuration\n#\n# Global tags can be specified here in key=&quot;value&quot; format.\n[global_tags]\n \n# Configuration for telegraf agent\n[agent]\n  interval = &quot;10s&quot;\n  round_interval = true\n  metric_batch_size = 1000\n  metric_buffer_limit = 10000\n  collection_jitter = &quot;0s&quot;\n  flush_interval = &quot;60s&quot;\n  flush_jitter = &quot;0s&quot;\n  precision = &quot;&quot;\n  hostname = &quot;docker.fritz.box&quot;\n  omit_hostname = false\n \n[[outputs.influxdb_v2]]\nurls = [&quot;http://influxdb2:8086&quot;]\ntoken = &quot;***&quot;\norganization = &quot;fleishor&quot;\nbucket = &quot;fleishor&quot;\n \n[[inputs.cpu]]\n  percpu = true\n  totalcpu = true\n  collect_cpu_time = false\n  report_active = false\n \n[[inputs.disk]]\n  ignore_fs = [&quot;tmpfs&quot;, &quot;devtmpfs&quot;, &quot;devfs&quot;, &quot;iso9660&quot;, &quot;overlay&quot;, &quot;aufs&quot;, &quot;squashfs&quot;]\n \n[[inputs.diskio]]\n \n[[inputs.kernel]]\n \n[[inputs.mem]]\n \n[[inputs.processes]]\n \n[[inputs.swap]]\n \n[[inputs.system]]\n \n[[inputs.docker]]\ncontainer_names = []\ncontainer_name_include = []\nperdevice = false\nperdevice_include = []\ntotal = true\ntotal_include = [&quot;cpu&quot;, &quot;blkio&quot;, &quot;network&quot;]\n \n[[inputs.socket_listener]]\nservice_address = &quot;tcp://:8094&quot;\ndata_format = &quot;influx&quot;"},"SmartHome/Docker/Smtp2MQTT":{"title":"Smtp2MQTT","links":[],"tags":["Docker","NodeJS","ExpressJS","MQTT","Typescript","Dockerfile"],"content":"References\n\nnodemailer.com/extras/smtp-server/\ngithub.com/trentm/node-bunyan\nwww.steves-internet-guide.com/using-node-mqtt-client/\nnodejs.org/en/docs/guides/nodejs-docker-webapp/\n\nFile tsconfig.json\n{\n  &quot;compilerOptions&quot;: {\n    &quot;module&quot;: &quot;commonjs&quot;,\n    &quot;esModuleInterop&quot;: true,\n    &quot;target&quot;: &quot;es6&quot;,\n    &quot;moduleResolution&quot;: &quot;node&quot;,\n    &quot;sourceMap&quot;: true,\n    &quot;outDir&quot;: &quot;dist&quot;\n  },\n  &quot;lib&quot;: [&quot;es2015&quot;]\n}\nFile  package.json\n{\n  &quot;name&quot;: &quot;smtp2mqtt&quot;,\n  &quot;version&quot;: &quot;1.0.0&quot;,\n  &quot;description&quot;: &quot;&quot;,\n  &quot;main&quot;: &quot;./dist/smtp2mqtt.js&quot;,\n  &quot;scripts&quot;: {\n    &quot;buildImage&quot;: &quot;docker build . -t smtp2mqtt&quot;,\n    &quot;build&quot;: &quot;npx tsc&quot;,\n    &quot;smtp2mqtt&quot;: &quot;node ./dist/smtp2mqtt.js&quot;\n  },\n  &quot;author&quot;: &quot;&quot;,\n  &quot;license&quot;: &quot;ISC&quot;,\n  &quot;devDependencies&quot;: {\n    &quot;@types/express&quot;: &quot;^4.17.15&quot;,\n    &quot;@types/node&quot;: &quot;^18.14.0&quot;,\n    &quot;@typescript-eslint/eslint-plugin&quot;: &quot;^5.53.0&quot;,\n    &quot;@typescript-eslint/parser&quot;: &quot;^5.53.0&quot;,\n    &quot;eslint&quot;: &quot;^8.34.0&quot;,\n    &quot;mqtt&quot;: &quot;^4.3.7&quot;,\n    &quot;typescript&quot;: &quot;^4.9.4&quot;\n  },\n  &quot;dependencies&quot;: {\n    &quot;@types/bunyan&quot;: &quot;^1.8.8&quot;,\n    &quot;@types/nodemailer&quot;: &quot;^6.4.7&quot;,\n    &quot;bunyan&quot;: &quot;^1.8.15&quot;,\n    &quot;mailparser&quot;: &quot;^3.1.0&quot;,\n    &quot;smtp-server&quot;: &quot;^3.8.0&quot;\n  }\n}\nFile .eslintrc.jrc\nmodule.exports = {\n   extends: [&quot;eslint:recommended&quot;, &quot;plugin:@typescript-eslint/recommended&quot;],\n   parser: &quot;@typescript-eslint/parser&quot;,\n   plugins: [&quot;@typescript-eslint&quot;],\n   root: true,\n};\nDockerfile\nFROM node:19.3.0-alpine3.16\n \n# Create app directory\nWORKDIR /smtp2mqtt\n \n# Install app dependencies\n# A wildcard is used to ensure both package.json AND package-lock.json are copied\n# where available (npm@5+)\nCOPY package*.json ./\n \nRUN npm install\n \n# If you are building your code for production\n# RUN npm ci --only=production\n \n# Bundle app source\nCOPY ./dist/smtp2mqtt.js .\n \nEXPOSE 3001\n \nCMD [ &quot;node&quot;, &quot;/smtp2mqtt/smtp2mqtt.js&quot; ]\nGenerate Javascript from Typescript\nnpm run build\nRun smtp2mqtt.js\nnpm run smtp2mqtt\nCreate Docker image\nnpm run buildImage\nSource code smtp2mqtt.ts\nimport { SMTPServer } from &quot;smtp-server&quot;;\nimport { simpleParser } from &quot;mailparser&quot;;\nimport mqtt from &quot;mqtt&quot;;\nimport fs from &quot;fs&quot;;\nimport bunyan from &quot;bunyan&quot;;\n \nfunction sessionInfoSerializer(sessionInfo) {\n   return {\n      id: sessionInfo.id,\n      remoteAddress: sessionInfo.remoteAddress,\n      clientHostname: sessionInfo.clientHostname,\n      transaction: sessionInfo.transaction,\n   };\n}\n \nfunction parsedMailInfoSerializer(parsedMailInfo) {\n   return {\n      messageId: parsedMailInfo.messageId,\n      from: parsedMailInfo.from,\n   };\n}\n \nconst logger = bunyan.createLogger({\n   name: &quot;smtp2mqtt&quot;,\n   level: &quot;debug&quot;,\n   serializers: {\n      sessionInfo: sessionInfoSerializer,\n      parsedMailInfo: parsedMailInfoSerializer,\n   },\n});\n \nconst smtpStorage = &quot;/storage&quot;;\nconst smtpServerPort = 2525;\nconst smtpUserName = &quot;pi@fleishor.localdomain&quot;;\nconst smtpPassword = &quot;pi&quot;;\nconst mqttBroker = &quot;mqtt://docker.fritz.box&quot;;\nconst mqttClientId = &quot;smtp2mqtt&quot;;\n \n// Connect to MQTT Broker\nlet mqttClientConnected = false;\nconst mqttClient = mqtt.connect(mqttBroker, { clientId: mqttClientId });\n \nconst smtpServer = new SMTPServer({\n   // Disable STARTTLS to allow authentication in clear text mode\n   disabledCommands: [&quot;STARTTLS&quot;],\n   logger: logger,\n   onConnect(session, callback) {\n      logger.info({ sessionInfo: session }, &quot;SMTPServer.onConnect from &quot; + session.clientHostname);\n      return callback();\n   },\n   onClose(session) {\n      logger.info({ sessionInfo: session }, &quot;SMTPServer.onClose from &quot; + session.clientHostname);\n   },\n   onData(stream, session, callback) {\n      // Receive email\n      logger.info({ sessionInfo: session }, &quot;SMTPServer.onData from &quot; + session.clientHostname);\n      simpleParser(stream, {}, (err, parsedMail) =&gt; {\n         // Parse email\n         logger.info({ sessionInfo: session, parsedMailInfo: parsedMail }, &quot;SMTPServer.onData.simpleParser: start&quot;);\n         if (err) {\n            logger.error({ sessionInfo: session }, &quot;SMTPServer.onData.simpleParser: &quot; + err);\n         }\n \n         logger.info({ sessionInfo: session, parsedMailInfo: parsedMail }, &quot;SMTPServer.onData.simpleParser: parsed email with subject: &quot; + parsedMail.subject);\n \n         // Parsed mail without attachments\n         const parsedMailWithoutAttachments = Object.assign({}, parsedMail);\n         delete parsedMailWithoutAttachments.attachments;\n         parsedMailWithoutAttachments.clientHostname = session.clientHostname;\n         if (!parsedMailWithoutAttachments.html) {\n            logger.info({ sessionInfo: session, parsedMailInfo: parsedMail }, &quot;SMTPServer.onData.simpleParser: replace html with textAsHtml&quot;);\n            parsedMailWithoutAttachments.html = parsedMailWithoutAttachments.textAsHtml;\n         }\n \n         // Create directory for writing mail\n         logger.info({ sessionInfo: session, parsedMailInfo: parsedMail }, &quot;SMTPServer.onData.simpleParser: make directory for session.id&quot; + session.id);\n         const dateNowUtc = new Date().toISOString();\n         const mailDirectory = dateNowUtc + &quot;_&quot; + session.id;\n         fs.mkdirSync(mailDirectory);\n \n         // Write email as JSON\n         logger.info({ sessionInfo: session, parsedMailInfo: parsedMail }, &quot;SMTPServer.onData.simpleParser: write JSON file&quot;);\n         fs.writeFileSync(mailDirectory + &quot;/&quot; + session.id + &quot;.json&quot;, JSON.stringify(parsedMailWithoutAttachments));\n \n         // Write eamil as HTML\n         logger.info({ sessionInfo: session, parsedMailInfo: parsedMail }, &quot;SMTPServer.onData.simpleParser: write HTML file&quot;);\n         fs.writeFileSync(mailDirectory + &quot;/&quot; + session.id + &quot;.html&quot;, parsedMailWithoutAttachments.html);\n \n         // Write Attachments\n         parsedMail.attachments.forEach((attachment) =&gt; {\n            logger.info({ sessionInfo: session, parsedMailInfo: parsedMail }, &quot;SMTPServer.onData.simpleParser: write attachment: &quot; + attachment.filename);\n            fs.writeFileSync(mailDirectory + &quot;/&quot; + attachment.filename, attachment.content);\n         });\n \n         // Publish sessionId via MQTT\n         if (mqttClientConnected) {\n            const mqttPrefix = &quot;smtp2mqtt&quot;;\n            let mqttDevice = parsedMail.from.value[&quot;0&quot;].name;\n            if (!mqttDevice) {\n               mqttDevice = session.clientHostname.replace(&quot;.fritz.box&quot;, &quot;&quot;);\n            }\n \n            let mqttSubDevice = mqttDevice;\n            if (parsedMailWithoutAttachments.subject.indexOf(&quot;OpenMediaVault&quot;) != -1) {\n               mqttSubDevice = &quot;OpenMediaVault&quot;;\n            } else if (parsedMailWithoutAttachments.subject.indexOf(&quot;Buero&quot;) != -1) {\n               mqttSubDevice = &quot;Buero&quot;;\n            } else if (parsedMailWithoutAttachments.subject.indexOf(&quot;Gefriertruhe&quot;) != -1) {\n               mqttSubDevice = &quot;Gefriertruhe&quot;;\n            } else if (parsedMailWithoutAttachments.subject.indexOf(&quot;Kueche&quot;) != -1) {\n               mqttSubDevice = &quot;Kueche&quot;;\n            } else if (parsedMailWithoutAttachments.subject.indexOf(&quot;Wintergarten&quot;) != -1) {\n               mqttSubDevice = &quot;Wintergarten&quot;;\n            }\n \n            const topic = &quot;/&quot; + mqttPrefix + &quot;/&quot; + mqttDevice + &quot;/&quot; + mqttSubDevice;\n            const mqttPayload = JSON.stringify({\n               topic: topic,\n               mqttPrefix: mqttPrefix,\n               mqttDevice: mqttDevice,\n               mqttSubDevice: mqttSubDevice,\n               mailDirectory: mailDirectory,\n               sessionId: session.id,\n               clientHostname: session.clientHostname,\n            });\n            logger.info({ topic: topic, mqttPayload: mqttPayload }, &quot;Publish to MQTT with topic &quot; + topic);\n            mqttClient.publish(topic, mqttPayload);\n         }\n \n         logger.info({ sessionInfo: session, parsedMailInfo: parsedMail }, &quot;SMTPServer.onData.simpleParser: done&quot;);\n         callback();\n      });\n \n      logger.info({ sessionInfo: session }, &quot;SMTPServer.onData: done&quot;);\n   },\n   onAuth(auth, session, callback) {\n      logger.info({ sessionInfo: session }, &quot;SMTPServer.onAuth&quot;);\n      if (auth.username !== smtpUserName &amp;&amp; auth.password !== smtpPassword) {\n         return callback(new Error(&quot;Invalid username/password:&quot; + auth.username + &quot;/&quot; + auth.password));\n      }\n      callback(null, { user: 123456 }); // where 123 is the user id or similar property\n   },\n});\n \nsmtpServer.on(&quot;error&quot;, (err) =&gt; {\n   logger.error(&quot;Error %s&quot;, err.message);\n});\n \nprocess.chdir(smtpStorage);\n \nmqttClient.on(&quot;connect&quot;, () =&gt; {\n   mqttClientConnected = true;\n   logger.info(&quot;Connected to MQTT Broker&quot;);\n});\n \nmqttClient.on(&quot;error&quot;, function (error) {\n   logger.error(&quot;Error connecting to MQTT Broker, Error:&quot; + error);\n   process.exit(1);\n});\n \nlogger.info({}, &quot;Startup SMTP Server&quot;);\nsmtpServer.listen(smtpServerPort, &quot;0.0.0.0&quot;);\nCreate new user smtp2mqtt\nsudo useradd -m smtp2mqtt\nAdd user smtp2mqtt to docker group\nsudo usermod -aG docker smtp2mqtt\nLogin as user smtp2mqtt\nsudo -u smtp2mqtt -i\nGet uid and gid for user smtp2mqtt\nsmtp2mqtt@docker:~ $ id\nuid=1014(smtp2mqtt) gid=1014(smtp2mqtt) groups=1014(smtp2mqtt),995(docker)\nCreate directories for smtp2mqtt\nmkdir storage\nFile docker-compose.yaml\nversion: &quot;3.5&quot;\n \nservices:\n  smtp2mqtt:\n    image: smtp2mqtt:latest\n    container_name: smtp2mqtt\n    user: 1009:995\n    volumes:\n      - /home/smtp2mqtt/storage:/storage:rw\n    ports:\n      - &quot;2525:2525&quot;\n    restart: always\n    networks:\n        - influxdb2\n \nnetworks:\n    influxdb2:\n        external: true\n        name: &quot;influxdb2&quot;"},"SmartHome/Docker/WebHook2MQTT":{"title":"WebHook2MQTT","links":[],"tags":["Docker","NodeJS","ExpressJS","MQTT","Typescript","Dockerfile"],"content":"References\nbetterstack.com/community/guides/logging/how-to-install-setup-and-use-winston-and-morgan-to-log-node-js-applications/\nwww.steves-internet-guide.com/using-node-mqtt-client/\nnodejs.org/en/docs/guides/nodejs-docker-webapp/\nFile tsconfig.json\n{\n  &quot;compilerOptions&quot;: {\n    &quot;module&quot;: &quot;commonjs&quot;,\n    &quot;esModuleInterop&quot;: true,\n    &quot;target&quot;: &quot;es6&quot;,\n    &quot;moduleResolution&quot;: &quot;node&quot;,\n    &quot;sourceMap&quot;: true,\n    &quot;outDir&quot;: &quot;dist&quot;\n  },\n  &quot;lib&quot;: [&quot;es2015&quot;]\n}\nFile package.json\n{\n  &quot;name&quot;: &quot;webhook2mqtt&quot;,\n  &quot;version&quot;: &quot;1.0.0&quot;,\n  &quot;description&quot;: &quot;&quot;,\n  &quot;main&quot;: &quot;./dist/webhook2mqtt.js&quot;,\n  &quot;scripts&quot;: {\n    &quot;buildImage&quot;: &quot;docker build . -t webhook2mqtt&quot;,\n    &quot;build&quot;: &quot;npx tsc&quot;,\n    &quot;webhook2mqtt&quot;: &quot;node ./dist/webhook2mqtt.js&quot;\n  },\n  &quot;author&quot;: &quot;&quot;,\n  &quot;license&quot;: &quot;ISC&quot;,\n  &quot;devDependencies&quot;: {\n    &quot;@types/express&quot;: &quot;^4.17.15&quot;,\n    &quot;@types/node&quot;: &quot;^18.14.0&quot;,\n    &quot;@typescript-eslint/eslint-plugin&quot;: &quot;^5.53.0&quot;,\n    &quot;@typescript-eslint/parser&quot;: &quot;^5.53.0&quot;,\n    &quot;eslint&quot;: &quot;^8.34.0&quot;,\n    &quot;typescript&quot;: &quot;^4.9.4&quot;\n  },\n  &quot;dependencies&quot;: {\n    &quot;body-parser&quot;: &quot;^1.20.1&quot;,\n    &quot;express&quot;: &quot;^4.18.2&quot;,\n    &quot;express-winston&quot;: &quot;^4.2.0&quot;,\n    &quot;mqtt&quot;: &quot;^4.3.7&quot;,\n    &quot;uuid&quot;: &quot;^9.0.0&quot;,\n    &quot;winston&quot;: &quot;^3.8.2&quot;\n  }\n}\nFile .eslintrc.jrc\nmodule.exports = {\n   extends: [&quot;eslint:recommended&quot;, &quot;plugin:@typescript-eslint/recommended&quot;],\n   parser: &quot;@typescript-eslint/parser&quot;,\n   plugins: [&quot;@typescript-eslint&quot;],\n   root: true,\n};\nDockerfile\nFROM node:19.3.0-alpine3.16\n \n# Create app directory\nWORKDIR /webhook2mqtt\n \n# Install app dependencies\n# A wildcard is used to ensure both package.json AND package-lock.json are copied\n# where available (npm@5+)\nCOPY package*.json ./\n \nRUN npm install\n# If you are building your code for production\n# RUN npm ci --only=production\n \n# Bundle app source\nCOPY ./dist/webhook2mqtt.js .\n \nEXPOSE 3001\nCMD [ &quot;node&quot;, &quot;/webhook2mqtt/webhook2mqtt.js&quot; ]\nGenerate Javascript from Typescript\nnpm run build\nRun webhook2mqtt.js\nnpm run webhook2mqtt\nCreate Docker image\nnpm run buildImage\nSource code webhook2mqtt.ts\nimport express from &quot;express&quot;;\nimport bodyParser from &quot;body-parser&quot;;\nimport winston from &quot;winston&quot;;\nimport expressWinston from &quot;express-winston&quot;;\nimport { v4 as uuidv4 } from &quot;uuid&quot;;\nimport mqtt from &quot;mqtt&quot;;\n \nconst expressApp = express();\nconst expressPort = 3001;\nconst grafanaRoute = &quot;/Grafana/*&quot;;\nconst mqttBroker = &quot;mqtt://docker.fritz.box&quot;;\nconst mqttClientId = &quot;webhook2mqtt&quot;;\n \nlet loggerUuid = null;\n \n// Create Logger\nconst logger = winston.createLogger({\n   level: &quot;info&quot;,\n   format: winston.format.combine(winston.format.timestamp(), winston.format.json(), winston.format.errors({ stack: true })),\n   transports: [new winston.transports.Console()],\n   exceptionHandlers: [new winston.transports.Console()],\n   rejectionHandlers: [new winston.transports.Console()],\n   defaultMeta: {\n      get uuid() {\n         return loggerUuid;\n      },\n   },\n});\n \nconst expressLogger = expressWinston.logger({ winstonInstance: logger });\nconst expressErrorLogger = expressWinston.errorLogger({ winstonInstance: logger });\n \n// Add POST request helper\nexpressApp.use(expressLogger);\nexpressApp.use(bodyParser.urlencoded({ extended: false }));\nexpressApp.use(bodyParser.json());\nexpressApp.use(expressErrorLogger);\n// eslint-disable-next-line @typescript-eslint/no-unused-vars\nexpressApp.use(function (err, req, res, next) {\n   res.status(500).send(&quot;Internal Error&quot;);\n});\n \n// Connect to MQTT Broker\nlet mqttClientConnected = false;\nconst mqttClient = mqtt.connect(mqttBroker, { clientId: mqttClientId });\n \n// Handle POST request for /Grafana/*\nexpressApp.post(grafanaRoute, (request, response) =&gt; {\n   // generate new uuid\n   loggerUuid = uuidv4();\n   logger.info(`Received POST requst for url &quot;${request.path}&quot;`, {\n      request_path: request.path,\n      request_body: request.body,\n   });\n   const grafanaAlert = request.body;\n \n   // forward POST request to MQTT\n   if (mqttClientConnected) {\n      const mqttPrefix = &quot;webhook2mqtt&quot;;\n      const mqttWebHook = &quot;Grafana&quot;;\n      const mqttAlertName = grafanaAlert.commonLabels.alertname;\n      const topic = &quot;/&quot; + mqttPrefix + &quot;/&quot; + mqttWebHook + &quot;/&quot; + mqttAlertName;\n      const mqttPayload = JSON.stringify({\n         topic: topic,\n         mqttPrefix: mqttPrefix,\n         mqttWebHook: mqttWebHook,\n         mqttAlertName: mqttAlertName,\n         path: request.path,\n         payload: request.body,\n      });\n \n      logger.info(&quot;Publish to MQTT with topic &quot; + topic, { topic: topic, payload: mqttPayload });\n      mqttClient.publish(topic, mqttPayload);\n   }\n   response.end();\n   loggerUuid = null;\n});\n \nmqttClient.on(&quot;connect&quot;, () =&gt; {\n   mqttClientConnected = true;\n   logger.info(&quot;Connected to MQTT Broker&quot;);\n});\n \nmqttClient.on(&quot;error&quot;, function (error) {\n   logger.error(&quot;Error connecting to MQTT Broker, Error:&quot; + error);\n   process.exit(1);\n});\n \n// Start HTTP Server\nexpressApp.listen(expressPort, () =&gt; {\n   logger.info(`Express is listening at http://localhost:${expressPort}`);\n});\nCreate new user webhook2mqtt\nsudo useradd -m webhook2mqtt\nAdd user webhook2mqtt to docker group\nsudo usermod -aG docker webhook2mqtt\nLogin as user webhook2mqtt\nsudo -u webhook2mqtt -i\nGet uid and gid for user webhook2mqtt\nwebhook2mqtt@docker:~ $ id\nuid=1014(webhook2mqtt) gid=1014(webhook2mqtt) groups=1014(webhook2mqtt),995(docker)\nFile docker-compose.yaml\nversion: &quot;3.5&quot;\n \nservices:\n  webhook2mqtt:\n    image: webhook2mqtt:latest\n    container_name: webhook2mqtt\n    user: 1014:995\n    restart: always\n    networks:\n        - influxdb2\n    ports:\n      - &quot;3001:3001&quot;\nnetworks:\n    influxdb2:\n        external: true\n        name: &quot;influxdb2&quot;"},"SmartHome/HomeAssistant/Forward-HomeAssistant-logs-to-Grafana-Loki-with-Promtail":{"title":"Forward-HomeAssistant-logs-to-Grafana-Loki-with-Promtail","links":[],"tags":["SmartHome","Loki","HomeAssistant"],"content":"Add bluemaex to Addon-on Repositories\n\nInstall Grafana Loki addon\n\nYAML configuration\nlog_level: info\nclient:\n  url: docker.fritz.box:3100/loki/api/v1/push\nadditional_scrape_configs: /config/local-journal-scrape-config.yaml\nskip_local_journal_scrape_config: true\n/config/local-journal-scrape-config.yaml\n- job_name: journal\n  journal:\n    json: false\n    max_age: 12h\n    labels:\n      job: systemd-journal\n      host: homeassistanterdgeschoss.fritz.box\n    path: &quot;${JOURNAL_PATH}&quot;\n  relabel_configs:\n    - source_labels:\n        - __journal__systemd_unit\n      target_label: unit\n    - source_labels:\n        - __journal_syslog_identifier\n      target_label: syslog_identifier\n    - source_labels:\n        - __journal_container_name\n      target_label: container_name\n  pipeline_stages:\n    - match:\n        selector: &#039;{container_name=~&quot;homeassistant|hassio_supervisor&quot;}&#039;\n        stages:\n          - multiline:\n              firstline: &#039;^\\x{001b}&#039;\nLogs in Grafana Loki\n"},"SmartHome/Homematic/Install-NodeRed-on-RaspberryPi":{"title":"Install NodeRed on RaspberryPi (homematic.fritz.box)","links":[],"tags":["NodeRed","SmartHome"],"content":"References\nwww.golinuxcloud.com/install-nodejs-and-npm-on-raspberry-pi/\nnodered.org/docs/getting-started/raspberrypi\nEnable NodeSource Repository\nsudo su\ncurl -fsSL deb.nodesource.com/setup_17.x | bash -\nInstall NodeJS\nsudo apt install nodejs\nCheck NodeJS installation\npi@HomeMatic:/etc/promtail $ node --version\nv17.3.0\npi@HomeMatic:/etc/promtail $ npm --version\n8.3.0\nInstall Node-RED\nbash &lt;(curl -sL raw.githubusercontent.com/node-red/linux-installers/master/deb/update-nodejs-and-nodered)\nDisable credentialSecret encryption\nsetting.json\n   ...\n   credentialSecret: false,\n   ..."},"SmartHome/Homematic/Install-Promtail-on-RaspberryPi":{"title":"Install Promtail on RaspberryPi (homematic.fritz.box)","links":[],"tags":["Promtail","SmartHome"],"content":"References\nsbcode.net/grafana/install-promtail-service/\ngrafana.com/docs/loki/latest/clients/promtail/troubleshooting/\nregex101.com/\nDownload and install binary\ncd /usr/local/bin\nsudo curl -O -L &quot;github.com/grafana/loki/releases/download/v2.4.1/promtail-linux-arm.zip&quot;\nsudo unzip &quot;promtail-linux-arm.zip&quot;\nsudo chmod a+x &quot;promtail-linux-arm\nrm &quot;promtail-linux-arm.zip&quot;\n/etc/promtail/promtail.yml\nserver:\n  http_listen_address: 0.0.0.0\n  http_listen_port: 9080\n \npositions:\n  filename: /home/pi/promtail/positions.yaml\n \nclients:\n  - url: docker.fritz.box:3100/loki/api/v1/push\n \nscrape_configs:\n   - job_name: system\n     static_configs:\n        - targets:\n           - localhost\n          labels:\n             host: homematic.fritz.box\n             job: varlogs\n             __path__: /var/log/*.log\n     pipeline_stages:\n        - regex:\n           expression: (?P&lt;month&gt;\\S+)\\s+(?P&lt;date&gt;[0-9]{1,2})\\s+(?P&lt;time&gt;[0-9]+:[0-9]+:[0-9]+)\\s+(?P&lt;hostname&gt;\\S+)\\s+(?P&lt;daemon&gt;\\S+)(?P&lt;pid&gt;\\[[0-9]+\\]):\\s+\n        - labels:\n           daemon:\nRegEx for parsing the syslogs\n“P” is required\n(?P&lt;month&gt;\\S+)\\s+(?P&lt;date&gt;[0-9]{1,2})\\s+(?P&lt;time&gt;[0-9]+:[0-9]+:[0-9]+)\\s+(?P&lt;hostname&gt;\\S+)\\s+(?P&lt;daemon&gt;\\S+)(?P&lt;pid&gt;\\[[0-9]+\\]):\\s+\n\nTest promtail configuration\necho &quot;Jan  5 13:16:17 HomeMatic systemd[1]: Started Session 4858 of user pi.&quot; | promtail-linux-arm --stdin --dry-run --inspect --client.url  docker.fritz.box:3100/loki/api/v1/push --config.file /etc/promtail/promtail.yml\nCreate promtail service user\nsudo useradd --system promtail\nusermod -a -G adm promtail\n/etc/systemd/system/promtail.service\n[Unit]\nDescription=Promtail service\nAfter=network.target\n \n[Service]\nType=simple\nUser=promtail\nExecStart=/usr/local/bin/promtail-linux-arm -config.file /etc/promtail/promtail.yml\n \n[Install]\nWantedBy=multi-user.target\nStart promtail service\nsudo systemctl start promtail.service\nsudo systemctl enable promtail.service\nsudo systemctl status promtail.service"},"SmartHome/Homematic/Install-Telegraf-on-RaspberryPi":{"title":"Install-Telegraf-on-RaspberryPi","links":[],"tags":["Raspberry-Pi","SmartHome"],"content":"References\ndocs.influxdata.com/telegraf/v1.21/introduction/installation/\nAdd InfluxDB repository\nwget -qO- &lt;repos.influxdata.com/influxdb.key&gt; | sudo tee /etc/apt/trusted.gpg.d/influxdb.asc &gt;/dev/null\nsource /etc/os-release\necho &quot;deb &lt;repos.influxdata.com/${ID&gt;} ${VERSION_CODENAME} stable&quot; | sudo tee /etc/apt/sources.list.d/influxdb.list\nInstall Telegraf\nsudo apt-get update &amp;&amp; sudo apt-get install telegraf\nAdd runtimeDirectory to Telegraf service file\n[Unit]\nDescription=The plugin-driven server agent for reporting metrics into InfluxDB\nDocumentation=&lt;github.com/influxdata/telegraf&gt;\nAfter=network.target\n \n[Service]\nEnvironmentFile=-/etc/default/telegraf\nUser=telegraf\nExecStart=/usr/bin/telegraf -config /etc/telegraf/telegraf.conf -config-directory /etc/telegraf/telegraf.d $TELEGRAF_OPTS\nExecReload=/bin/kill -HUP $MAINPID\nRestart=on-failure\nRestartForceExitStatus=SIGPIPE\nKillMode=control-group\nRuntimeDirectory=telegraf\n \n[Install]\nWantedBy=multi-user.target\nTelegraf config file\n[global_tags]\n[agent]\n  interval = &quot;60s&quot;\n  round_interval = true\n  metric_batch_size = 1000\n  metric_buffer_limit = 10000\n  collection_jitter = &quot;0s&quot;\n  flush_interval = &quot;60s&quot;\n  flush_jitter = &quot;0s&quot;\n  precision = &quot;&quot;\n  hostname = &quot;homematic.fritz.box&quot;\n  omit_hostname = false\n \n###############################################################################\n#                            OUTPUT PLUGINS                                   #\n###############################################################################\n[[outputs.influxdb_v2]]\nurls = [&quot;docker.fritz.box:8086&quot;]\ntoken = &quot;***&quot;\norganization = &quot;fleishor&quot;\nbucket = &quot;fleishor&quot;\n \n###############################################################################\n#                            INPUT PLUGINS                                    #\n###############################################################################\n[[inputs.cpu]]\n  percpu = true\n  totalcpu = true\n  collect_cpu_time = false\n  report_active = false\n \n[[inputs.disk]]\n  ignore_fs = [&quot;tmpfs&quot;, &quot;devtmpfs&quot;, &quot;devfs&quot;, &quot;iso9660&quot;, &quot;overlay&quot;, &quot;aufs&quot;, &quot;squashfs&quot;]\n \n[[inputs.diskio]]\n \n[[inputs.kernel]]\n \n[[inputs.mem]]\n \n[[inputs.processes]]\n \n[[inputs.swap]]\n \n[[inputs.system]]\n \n[[inputs.socket_listener]]\n  service_address = &quot;unix:////var/run/telegraf/unixsock&quot;\n  socket_mode = &quot;777&quot;"},"SmartHome/Homematic/Integrate-Voltcraft-Sem6000-on-RaspberryPi":{"title":"Integrate Voltcraft SEM6000 (homematic.fritz.box)","links":[],"tags":["Raspberry-Pi","SEM6000","SmartHome"],"content":"References\ngithub.com/Heckie75/voltcraft-sem-6000\ngit.geekify.de/sqozz/sem6000\nOverview\n\nWohnzimmer.py\n#!/usr/bin/env python3\n \nimport sys\nimport time\nfrom sem6000 import SEMSocket\n \nimport bluepy\n \nsocket = None\n \ni = 0\nwhile i &lt; 5:\n    try:\n        if socket == None:\n            socket = SEMSocket(&#039;F0:C7:7F:0A:56:99&#039;)\n            socket.login(&quot;0000&quot;)\n \n        socket.getStatus()\n        print(&quot;Sem6000,Socket=Wohnzimmer powered={}&quot;.format(&quot;1&quot; if socket.powered else &quot;0&quot;))\n        print(&quot;Sem6000,Socket=Wohnzimmer voltage={}&quot;.format(socket.voltage))\n        print(&quot;Sem6000,Socket=Wohnzimmer current={}&quot;.format(socket.current))\n        print(&quot;Sem6000,Socket=Wohnzimmer power={}&quot;.format(socket.power))\n        break\n    except SEMSocket.NotConnectedException as ex:\n        print(&quot;SEMSocket.NotConnectedException: &quot;, ex, file =sys.stderr)\n        i += 1\n        print(&quot;Restarting after 5s...&quot;, file = sys.stderr)\n        time.sleep(5)\n    except bluepy.btle.BTLEDisconnectError as ex:\n        print(&quot;bluepy.btle.BTLEDisconnectError: &quot;, ex, file =sys.stderr)\n        i += 1\n        print(&quot;Restarting after 5s...&quot;, file = sys.stderr)\n        time.sleep(5)\n    except BrokenPipeError as ex:\n        print(&quot;BrokenPipeError:&quot;, ex, file =sys.stderr)\n        i += 1\n        print(&quot;Restarting after 5s...&quot;, file = sys.stderr)\n        time.sleep(5)\n    finally:\n        if socket != None:\n            socket.disconnect()\n            socket = None\nWohnzimmer.sh\n#!/bin/bash\n{ python3 Wohnzimmer.py 2&gt;&amp;3 | socat - UNIX-CLIENT:/var/run/telegraf/unixsock; } 3&gt;&amp;1 | logger -i --priority cron.error --tag sem6000-Wohnzimmer\nLogger write the error messages to /var/log/cron.log:\nJan 30 19:30:09 HomeMatic sem6000-Wohnzimmer[28979]: bluepy.btle.BTLEDisconnectError:  Failed to connect to peripheral $\nJan 30 19:30:09 HomeMatic sem6000-Wohnzimmer[28979]: Restarting after 5s...\n\nPromtail extracts “sem6000-Wohnzimmer” and sent it as label daemon to Loki\ncrontab\n*/10 * * * * cd /home/pi/sem6000 &amp;&amp; ./Wohnzimmer.sh\ntelegraf.conf\n[[inputs.socket_listener]]\n  service_address = &quot;unix:////var/run/telegraf/unixsock&quot;\n  socket_mode = &quot;777&quot;\n"},"SmartHome/Influxdb2-Maintenance":{"title":"InfluxDB2 Maintenance","links":[],"tags":["Influxdb"],"content":"API Token setzen\nexport INFLUX_TOKEN=&quot;***&quot;\necho $INFLUX_TOKEN\n\nMeasurement löschen\nEin Measurement kann nicht direkt gelöscht werden, sondern nur die Daten. Der Vorteil ist, dass ein leeres Measurement nicht angezeigt wird.\ninflux delete --bucket Fritzbox --start &#039;1970-01-01T00:00:00Z&#039; --stop $(date +&quot;%Y-%m-%dT%H:%M:%SZ&quot;) --predicate &#039;_measurement=&quot;Fritzbox&quot;&#039;\n\nAlle Measurements in einem Bucket anzeigen lassen\nLeider wird die Anzeige abgeschnitten\ninflux query &#039;import &quot;influxdata/influxdb/schema&quot; schema.measurements(bucket: &quot;HomeAssistant&quot;)&#039;\n"},"SmartHome/Install-Docker":{"title":"Installation Docker","links":[],"tags":["Docker","Raspberry-Pi"],"content":"Installation Guides\ndocs.docker.com/engine/install/debian/\nwww.antary.de/2021/09/20/raspberry-pi-docker-und-portainer/\nUpdate Raspi to latest versions\nsudo apt update\nsudo apt upgrade\nsudo rpi-update\nsudo reboot\nInstall Docker\nAnd here we have to possible ways\n\nuse the installation script (I will do it next time)\ninstall packages manually\n\nUse Installation script\ncurl -sSL get.docker.com | sh\nInstall necessary packages\nsudo apt-get install \\\n    apt-transport-https \\\n    ca-certificates \\\n    curl \\\n    gnupg \\\n    lsb-release\n \ncurl -fsSL download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n \necho \\\n  &quot;deb [arch=arm64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] download.docker.com/linux/debian \\\n  $(lsb_release -cs) stable&quot; | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nInstall docker packages\nsudo apt update\n \nsudo apt install docker-ce docker-ce-cli containerd.io\nCreate group and add current user to group\nsudo groupadd docker\n \nsudo usermod -aG docker $USER"},"SmartHome/Install-Docker/Install-Docker":{"title":"Installation Docker","links":[],"tags":["Docker","Raspberry-Pi"],"content":"Installation Guides\ndocs.docker.com/engine/install/debian/\nwww.antary.de/2021/09/20/raspberry-pi-docker-und-portainer/\nUpdate Raspi to latest versions\nsudo apt update\nsudo apt upgrade\nsudo rpi-update\nsudo reboot\nInstall Docker\nAnd here we have to possible ways\n\nuse the installation script (I will do it next time)\ninstall packages manually\n\nUse Installation script\ncurl -sSL get.docker.com | sh\nInstall necessary packages\nsudo apt-get install \\\n    apt-transport-https \\\n    ca-certificates \\\n    curl \\\n    gnupg \\\n    lsb-release\n \ncurl -fsSL download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n \necho \\\n  &quot;deb [arch=arm64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] download.docker.com/linux/debian \\\n  $(lsb_release -cs) stable&quot; | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nInstall docker packages\nsudo apt update\n \nsudo apt install docker-ce docker-ce-cli containerd.io\nCreate group and add current user to group\nsudo groupadd docker\n \nsudo usermod -aG docker $USER"},"SmartHome/NodeRed/Forward-MQTT-To-Telegram":{"title":"Forward MQTT message to telegram","links":[],"tags":["Docker","Raspberry-Pi","NodeRed","Telegram"],"content":"Workflow for sending MQTT to Telegram\n\nMQTT node\n\nMQTT configuration\n\nConvert MQTT payload from string to Javascript object\n\nSwitch depending on topic\n\nCreate Telegram message for SMTP\n\nCreate Telegram message for WebHook\n\nSend Telegram message to SmartHomeNotification channel\n\nWrite log message to NodeRed console and therefore to Loki\n\n"},"Softwarearchitecture/SOLID-Principles":{"title":"S.O.L.I.D Principles","links":[],"tags":["Softwarearchitecture","Python"],"content":"S.O.L.I.D\nDie SOLID-Prinzipien sind fünf grundlegende Prinzipien des objektorientierten Designs, die darauf abzielen, Software verständlicher, flexibler und wartbarer zu machen. Diese Prinzipien wurden von Robert C. Martin (auch bekannt als Uncle Bob) eingeführt und sind ein wichtiger Bestandteil der Softwareentwicklung. Hier sind die fünf Prinzipien im Detail:\n\nSingle Responsibility Principle (SRP): Eine Klasse sollte nur eine einzige Verantwortlichkeit haben. Das bedeutet, dass sie nur einen Grund haben sollte, sich zu ändern.\nOpen-Closed Principle (OCP): Softwaremodule sollten offen für Erweiterungen, aber geschlossen für Änderungen sein. Das bedeutet, dass man neue Funktionen hinzufügen kann, ohne den bestehenden Code zu ändern.\nLiskov Substitution Principle (LSP): Objekte einer Basisklasse sollten durch Objekte ihrer abgeleiteten Klassen ersetzt werden können, ohne dass das Programmverhalten verändert wird.\nInterface Segregation Principle (ISP): Eine Klasse sollte nicht gezwungen sein, Schnittstellen zu implementieren, die sie nicht benötigt. Es ist besser, viele spezifische Schnittstellen zu haben, als eine allgemeine.\nDependency Inversion Principle (DIP): Abhängigkeiten sollten von Abstraktionen abhängen, nicht von konkreten Implementierungen. Das bedeutet, dass hochrangige Module nicht von niederrangigen Modulen abhängen sollten, sondern beide von Abstraktionen.\n\nDiese Prinzipien helfen Entwicklern, sauberen, verständlichen und wartbaren Code zu schreiben.\nSingle Responsibility Principle\nDas Single Responsibility Principle (SRP) besagt, dass eine Klasse nur eine einzige Verantwortlichkeit haben sollte. Das bedeutet, dass eine Klasse nur einen Grund haben sollte, sich zu ändern. Hier ist ein praktisches Beispiel, wie man das SRP anwenden kann:\nStellen wir uns vor, wir haben eine Klasse, die sowohl für das Speichern von Benutzerdaten als auch für das Senden von E-Mails verantwortlich ist:\nclass UserService:\n    def save_user(self, user):\n        # Code zum Speichern des Benutzers\n        pass\n \n    def send_welcome_email(self, user):\n        # Code zum Senden einer Willkommens-E-Mail\n        pass\nDiese Klasse hat zwei Verantwortlichkeiten: das Speichern von Benutzerdaten und das Senden von E-Mails. Um das SRP zu befolgen, sollten wir diese Verantwortlichkeiten in separate Klassen aufteilen:\nclass UserRepository:\n    def save_user(self, user):\n        # Code zum Speichern des Benutzers\n        pass\n \nclass EmailService:\n    def send_welcome_email(self, user):\n        # Code zum Senden einer Willkommens-E-Mail\n        pass\nJetzt hat jede Klasse nur eine Verantwortlichkeit: UserRepository ist für das Speichern von Benutzerdaten zuständig, und EmailService ist für das Senden von E-Mails verantwortlich.\nOpen-Closed Principle\nDas Open-Closed Principle (OCP) besagt, dass Softwaremodule offen für Erweiterungen, aber geschlossen für Änderungen sein sollten. Das bedeutet, dass man neue Funktionen hinzufügen kann, ohne den bestehenden Code zu ändern. Hier ist ein praktisches Beispiel, wie man das OCP anwenden kann:\nStellen wir uns vor, wir haben eine Anwendung, die verschiedene Arten von Berichten generiert. Zunächst könnte der Code so aussehen:\nclass ReportGenerator:\n    def generate_report(self, report_type):\n        if report_type == &quot;PDF&quot;:\n            return self.generate_pdf_report()\n        elif report_type == &quot;Excel&quot;:\n            return self.generate_excel_report()\n    \n    def generate_pdf_report(self):\n        # PDF-Berichtserstellungscode\n        pass\n    \n    def generate_excel_report(self):\n        # Excel-Berichtserstellungscode\n        pass\nDieser Code verstößt gegen das OCP, da jede neue Berichtart eine Änderung der generate_report-Methode erfordert. Um das OCP zu befolgen, können wir eine abstrakte Basisklasse und spezifische Unterklassen für jede Berichtart erstellen:\nfrom abc import ABC, abstractmethod\n \nclass Report(ABC):\n    @abstractmethod\n    def generate(self):\n        pass\n \nclass PDFReport(Report):\n    def generate(self):\n        # PDF-Berichtserstellungscode\n        pass\n \nclass ExcelReport(Report):\n    def generate(self):\n        # Excel-Berichtserstellungscode\n        pass\n \nclass ReportGenerator:\n    def generate_report(self, report: Report):\n        return report.generate()\nJetzt können wir neue Berichtarten hinzufügen, indem wir einfach neue Unterklassen von Report erstellen, ohne den ReportGenerator zu ändern\nLiskov Substitution Principle (LSP)\nDas Liskov Substitution Principle (LSP) besagt, dass Objekte einer Basisklasse durch Objekte ihrer abgeleiteten Klassen ersetzt werden können sollten, ohne dass das Programmverhalten verändert wird. Hier ist ein praktisches Beispiel, wie man das LSP anwenden kann:\nStellen wir uns vor, wir haben eine Basisklasse Bird und zwei abgeleitete Klassen Eagle und Penguin. Ohne das LSP könnte der Code so aussehen:\nclass Bird:\n    def fly(self):\n        pass\n \nclass Eagle(Bird):\n    def fly(self):\n        print(&quot;Eagle is flying&quot;)\n \nclass Penguin(Bird):\n    def fly(self):\n        raise Exception(&quot;Penguins can&#039;t fly&quot;)\nHier verstößt die Klasse Penguin gegen das LSP, da sie die fly-Methode der Basisklasse Bird nicht sinnvoll implementiert. Um das LSP zu befolgen, sollten wir die Klassenhierarchie so gestalten, dass alle abgeleiteten Klassen die Methoden der Basisklasse sinnvoll nutzen können. Eine Möglichkeit wäre, die Bird-Klasse in zwei separate Basisklassen zu unterteilen:\nclass Bird(ABC):\n    @abstractmethod\n    def move(self):\n        pass\n \nclass FlyingBird(Bird):\n    def move(self):\n        self.fly()\n \n    @abstractmethod\n    def fly(self):\n        pass\n \nclass Eagle(FlyingBird):\n    def fly(self):\n        print(&quot;Eagle is flying&quot;)\n \nclass Penguin(Bird):\n    def move(self):\n        print(&quot;Penguin is walking&quot;)\nIn diesem Beispiel haben wir die Bird-Klasse in Bird und FlyingBird unterteilt. Jetzt können Eagle und Penguin die move-Methode sinnvoll implementieren, ohne das LSP zu verletzen.\nInterface Segregation Principle (ISP)\nDas Interface Segregation Principle (ISP) besagt, dass eine Klasse nicht gezwungen sein sollte, Schnittstellen zu implementieren, die sie nicht benötigt. Stattdessen sollten Schnittstellen klein und spezifisch sein. Hier ist ein praktisches Beispiel, wie man das ISP anwenden kann:\nStellen wir uns vor, wir haben eine Schnittstelle Worker, die sowohl Arbeits- als auch Essensmethoden enthält:\nfrom abc import ABC, abstractmethod\n \nclass Worker(ABC):\n    @abstractmethod\n    def work(self):\n        pass\n \n    @abstractmethod\n    def eat(self):\n        pass\n \nclass Developer(Worker):\n    def work(self):\n        print(&quot;Developer is coding.&quot;)\n \n    def eat(self):\n        print(&quot;Developer is eating.&quot;)\n \nclass Robot(Worker):\n    def work(self):\n        print(&quot;Robot is working.&quot;)\n \n    def eat(self):\n        raise NotImplementedError(&quot;Robot does not eat.&quot;)\nIn diesem Beispiel verstößt die Klasse Robot gegen das ISP, da sie die Methode eat implementieren muss, obwohl sie diese nicht benötigt. Um das ISP zu befolgen, können wir die Schnittstelle Worker in spezifischere Schnittstellen aufteilen:\nfrom abc import ABC, abstractmethod\n \nclass Workable(ABC):\n    @abstractmethod\n    def work(self):\n        pass\n \nclass Eatable(ABC):\n    @abstractmethod\n    def eat(self):\n        pass\n \nclass Developer(Workable, Eatable):\n    def work(self):\n        print(&quot;Developer is coding.&quot;)\n \n    def eat(self):\n        print(&quot;Developer is eating.&quot;)\n \nclass Robot(Workable):\n    def work(self):\n        print(&quot;Robot is working.&quot;)\n}\nJetzt implementiert Robot nur die Workable-Schnittstelle und muss keine unnötigen Methoden mehr implementieren.\nDependency Inversion Principle (DIP)\nDas Dependency Inversion Principle (DIP) besagt, dass hochrangige Module nicht von niederrangigen Modulen abhängen sollten, sondern beide von Abstraktionen. Dies fördert lose Kopplung und erleichtert die Wartung und Erweiterung des Codes. Hier ist ein praktisches Beispiel, wie man das DIP in Python anwenden kann:\nIm negativen Beispiel haben wir eine Klasse LightSwitch, die direkt von der Klasse LightBulb abhängt:\nclass LightBulb:\n    def turn_on(self):\n        print(&quot;LightBulb is on&quot;)\n \n    def turn_off(self):\n        print(&quot;LightBulb is off&quot;)\n \nclass LightSwitch:\n    def __init__(self, bulb: LightBulb):\n        self.bulb = bulb\n \n    def operate(self, on: bool):\n        if on:\n            self.bulb.turn_on()\n        else:\n            self.bulb.turn_off()\nIn diesem Beispiel hängt die Klasse LightSwitch direkt von der konkreten Implementierung LightBulb ab. Dies verstößt gegen das DIP, da Änderungen an LightBulb auch Änderungen an LightSwitch erfordern könnten.\nUm das DIP zu befolgen, sollten wir Abstraktionen einführen, sodass LightSwitch nicht direkt von LightBulb abhängt:\nfrom abc import ABC, abstractmethod\n \nclass Switchable(ABC):\n    @abstractmethod\n    def turn_on(self):\n        pass\n \n    @abstractmethod\n    def turn_off(self):\n        pass\n \nclass LightBulb(Switchable):\n    def turn_on(self):\n        print(&quot;LightBulb is on&quot;)\n \n    def turn_off(self):\n        print(&quot;LightBulb is off&quot;)\n \nclass Fan(Switchable):\n    def turn_on(self):\n        print(&quot;Fan is on&quot;)\n \n    def turn_off(self):\n        print(&quot;Fan is off&quot;)\n \nclass LightSwitch:\n    def __init__(self, device: Switchable):\n        self.device = device\n \n    def operate(self, on: bool):\n        if on:\n            self.device.turn_on()\n        else:\n            self.device.turn_off()\nIn diesem positiven Beispiel haben wir eine Abstraktion Switchable eingeführt, die von LightBulb und Fan implementiert wird. Die Klasse LightSwitch hängt nun von der Abstraktion Switchable ab, nicht von der konkreten Implementierung. Dadurch können wir leicht neue Geräte hinzufügen, ohne den LightSwitch ändern zu müssen."},"Windows/Hyper-V-und-DefaultSwitch":{"title":"Create and configure a new NAT switch for Hyper-V","links":[],"tags":["Linux"],"content":"Commands\nPS C:\\Users\\fleishor&gt; Get-VMSwitch\n\nName           SwitchType NetAdapterInterfaceDescription\n----           ---------- ------------------------------\nIntern         Internal\nExtern         External   DisplayLink Network Adapter NCM\nDefault Switch Internal\n\nPS C:\\Users\\fleishor&gt; Get-NetAdapter -includeHidden\n\nName                      InterfaceDescription                    ifIndex Status       MacAddress             LinkSpeed\n----                      --------------------                    ------- ------       ----------             ---------\nvEthernet (Default Switc… Hyper-V Virtual Ethernet Adapter             46 Up           00-15-5D-85-07-EF        10 Gbps\nEthernet (Docking)        Dell Giga Ethernet                           42 Up           9C-EB-E8-62-55-E8         1 Gbps\nLAN-Verbindung* 11        WAN Miniport (Network Monitor)               40 Up                                      0 bps\nvSwitch (VPN)             Hyper-V Virtual Switch Extension A...#3      38 Up                                    10 Gbps\nLAN-Verbindung* 4         WAN Miniport (SSTP)                          35 Disconnected                            0 bps\nEthernet (Kerneldebugger) Microsoft Kernel Debug Network Adapter       33 Not Present                             0 bps\nvEthernet (Intern)        Hyper-V Virtual Ethernet Adapter #3          32 Up           00-15-5D-B2-37-02        10 Gbps\nLAN-Verbindung* 6         WAN Miniport (L2TP)                          31 Disconnected                            0 bps\nLAN-Verbindung* 10        WAN Miniport (IPv6)                          30 Up                                      0 bps\nWLAN                      Intel(R) Wi-Fi 6 AX201 160MHz                26 Disconnected 4C-79-6E-89-FD-E6          0 bps\nBluetooth-Netzwerkverbin… Bluetooth Device (Personal Area Networ…      25 Disconnected 4C-79-6E-89-FD-EA         3 Mbps\nLAN-Verbindung* 9         WAN Miniport (IP)                            24 Up                                      0 bps\nTeredo Tunneling Pseudo-…                                              23 Not Present                             0 bps\nvSwitch (Default Switch)  Hyper-V Virtual Switch Extension Adapt…       3 Up                                    10 Gbps\nvEthernet (Extern)        Hyper-V Virtual Ethernet Adapter #2          20 Up           9C-EB-E8-62-55-E8         1 Gbps\nEthernet 7                PANGP Virtual Ethernet Adapter Secure        19 Disabled     02-50-41-00-00-01         2 Gbps\nLAN-Verbindung* 12        Microsoft Wi-Fi Direct Virtual Ada...#4      18 Disconnected 4E-79-6E-89-FD-E6          0 bps\nLAN-Verbindung* 7         WAN Miniport (PPTP)                          16 Disconnected                            0 bps\nLAN-Verbindung* 3         Microsoft Wi-Fi Direct Virtual Ada...#3      15 Disconnected 4C-79-6E-89-FD-E7          0 bps\nLAN-Verbindung* 5         WAN Miniport (IKEv2)                         10 Disconnected                            0 bps\nvSwitch (Extern)          Hyper-V Virtual Switch Extension A...#2       7 Up                                    10 Gbps\nLAN-Verbindung* 8         WAN Miniport (PPPOE)                          6 Disconnected                            0 bps\nMicrosoft IP-HTTPS Platf…                                               5 Not Present                             0 bps\n6to4 Adapter                                                            2 Not Present                             0 bps\n\nPS C:\\Users\\fleishor&gt; get-netipAddress -interfaceindex 46\n\nIPAddress         : 192.168.96.1\nInterfaceIndex    : 46\nInterfaceAlias    : vEthernet (Default Switch)\nAddressFamily     : IPv4\nType              : Unicast\nPrefixLength      : 20\nPrefixOrigin      : Manual\nSuffixOrigin      : Manual\nAddressState      : Preferred\nValidLifetime     : Infinite ([TimeSpan]::MaxValue)\nPreferredLifetime : Infinite ([TimeSpan]::MaxValue)\nSkipAsSource      : False\nPolicyStore       : ActiveStore\n\nPS C:\\Users\\fleishor&gt; New-VMSwitch -SwitchName &quot;MyHyperV NAT&quot; -SwitchType Internal\n\nName         SwitchType NetAdapterInterfaceDescription\n----         ---------- ------------------------------\nMyHyperV NAT Internal\n\n\nPS C:\\Users\\fleishor&gt; Get-NetAdapter\n\nName                      InterfaceDescription                    ifIndex Status       MacAddress             LinkSpeed\n----                      --------------------                    ------- ------       ----------             ---------\nvEthernet (MyHyperV NAT)  Hyper-V Virtual Ethernet Adapter #4          79 Up           00-15-5D-B2-37-15        10 Gbps\nEthernet (Docking)        Dell Giga Ethernet                           42 Up           9C-EB-E8-62-55-E8         1 Gbps\nvEthernet (Intern)        Hyper-V Virtual Ethernet Adapter #3          32 Up           00-15-5D-B2-37-02        10 Gbps\nWLAN                      Intel(R) Wi-Fi 6 AX201 160MHz                26 Disconnected 4C-79-6E-89-FD-E6          0 bps\nBluetooth-Netzwerkverbin… Bluetooth Device (Personal Area Networ…      25 Disconnected 4C-79-6E-89-FD-EA         3 Mbps\nvEthernet (Extern)        Hyper-V Virtual Ethernet Adapter #2          20 Up           9C-EB-E8-62-55-E8         1 Gbps\nEthernet 7                PANGP Virtual Ethernet Adapter Secure        19 Disabled     02-50-41-00-00-01         2 Gbps\n\nPS C:\\Users\\fleishor&gt; New-NetIPAddress -IPAddress 192.168.0.1 -PrefixLength 24 -InterfaceIndex 79\n\nIPAddress         : 192.168.0.1\nInterfaceIndex    : 79\nInterfaceAlias    : vEthernet (MyHyperV NAT)\nAddressFamily     : IPv4\nType              : Unicast\nPrefixLength      : 24\nPrefixOrigin      : Manual\nSuffixOrigin      : Manual\nAddressState      : Tentative\nValidLifetime     : Infinite ([TimeSpan]::MaxValue)\nPreferredLifetime : Infinite ([TimeSpan]::MaxValue)\nSkipAsSource      : False\nPolicyStore       : ActiveStore\n\nIPAddress         : 192.168.0.1\nInterfaceIndex    : 79\nInterfaceAlias    : vEthernet (MyHyperV NAT)\nAddressFamily     : IPv4\nType              : Unicast\nPrefixLength      : 24\nPrefixOrigin      : Manual\nSuffixOrigin      : Manual\nAddressState      : Invalid\nValidLifetime     : Infinite ([TimeSpan]::MaxValue)\nPreferredLifetime : Infinite ([TimeSpan]::MaxValue)\nSkipAsSource      : False\nPolicyStore       : PersistentStore\n\nPS C:\\Users\\fleishor&gt; New-NetNat -Name &quot;MyHyperV NAT&quot; -InternalIPInterfaceAddressPrefix 192.168.0.0/24\n\nName                             : MyHyperV NAT\nExternalIPInterfaceAddressPrefix :\nInternalIPInterfaceAddressPrefix : 192.168.0.0/24\nIcmpQueryTimeout                 : 30\nTcpEstablishedConnectionTimeout  : 1800\nTcpTransientConnectionTimeout    : 120\nTcpFilteringBehavior             : AddressDependentFiltering\nUdpFilteringBehavior             : AddressDependentFiltering\nUdpIdleSessionTimeout            : 120\nUdpInboundRefresh                : False\nStore                            : Local\nActive                           : True\n"},"Windows/Windows-Terminal-Panes":{"title":"Windows Terminal Panes","links":[],"tags":["Windows-Terminal","Shortcuts"],"content":"Creating a new pane\nHorizontal pane\nalt + shift + +\nVertical pane\nalt + shift + -\nSwitching between panes\nalt + arrow keys\nResizing a pane\nalt + shift + arrow keys\nClosing a pane\nalt + shift + w"},"index":{"title":"MyDigitalGarden","links":["Dotnet/Keycloak/Node-Express","Dotnet/Keycloak/Glossar","Softwarearchitecture/SOLID-Principles","SmartHome/Docker/Add-SNMP-to-Telegraf","Elastic8/Overview","Linux/iPerf3","Dotnet/Authentication/JWT-based-Authentication","SmartHome/Docker/Installations/Keycloak-with-Docker","Dotnet/Authentication/Cookie-based-Authentication","Dotnet/Mediatr/Serilog-for-CQRS-with-MediatR"],"tags":[],"content":"                    2025-03-28                  Keycloak Node.js adapter                  Authenticate with Node to Keycloak                             2025-03-17                  Keycloak - Glossar                  null                             2025-01-15                  S.O.L.I.D Principles                  Die SOLID-Prinzipien sind fünf grundlegende Prinzipien des objektorientierten Designs, die darauf abzielen, Software verständlicher, flexibler und wartbarer zu machen.                             2024-11-24                  Send SNMP statistics via Telegraf to InfluxDB                  Take statistics from Netgear devices with SNMP and Telegraf and forward them to InfluxDB. The statistics are also written to different buckets                             2024-11-13                  Elastic8 - Übersicht                  Eine Übersicht über Elastic8                             2024-11-08                  iPerf(3)                  Verwendung von iPerf3 und iPerf; Server/Client                             2024-10-24                  JWT based authentication                  A short example for JWT-based authentication; for authorization we use policy-based.                             2024-10-17                  Keycloak with Docker                  Installation steps for Keycloak with Docker                             2024-10-14                  Cookie based Authentication                  A short example for cookie-based authentication; for authorization we use role-based and policy-based.                             2024-09-29                  Add Serilog to &quot;CQRS with MediatR&quot;                  Add Serilog to &quot;CQRS with MediatR&quot; project; the logs are written as plain text to console and as ([Compact Log Event Format (CLEF)](clef-json.org/)) to a file. Additionally we can set a correlation id in the http header.         "}}